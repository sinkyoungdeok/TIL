{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a505b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2e0072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/31 22:42:28 WARN Utils: Your hostname, singyeongdeog-ui-Macmini.local resolves to a loopback address: 127.0.0.1; using 222.98.22.103 instead (on interface en0)\n",
      "22/07/31 22:42:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/31 22:42:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/31 22:42:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY = \"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediction\")\\\n",
    "                    .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "                    .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7606ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_files = \"/Users/singyeongdeog/Documents/github_code/data-engineering/01-spark/data/yellow/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500119be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df = spark.read.csv(f\"file:///{trip_files}\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fa8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6e13a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8919b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') as day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 4\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_dropoff_datetime) < '2021-08-01'\n",
    "\"\"\"\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32caa158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|            1.0|               142|                 43|          2.1|          0|     Friday|        11.8|\n",
      "|            1.0|               238|                151|          0.2|          0|     Friday|         4.3|\n",
      "|            1.0|               132|                165|         14.7|          0|     Friday|       51.95|\n",
      "|            0.0|               138|                132|         10.6|          0|     Friday|       36.35|\n",
      "|            1.0|                68|                 33|         4.94|          0|     Friday|       24.36|\n",
      "|            1.0|               224|                 68|          1.6|          0|     Friday|       14.15|\n",
      "|            1.0|                95|                157|          4.1|          0|     Friday|        17.3|\n",
      "|            1.0|                90|                 40|          5.7|          0|     Friday|        21.8|\n",
      "|            1.0|                97|                129|          9.1|          0|     Friday|        28.8|\n",
      "|            2.0|               263|                142|          2.7|          0|     Friday|       18.95|\n",
      "|            3.0|               164|                255|         6.11|          0|     Friday|        24.3|\n",
      "|            2.0|               255|                 80|         1.21|          0|     Friday|       10.79|\n",
      "|            2.0|               138|                166|          7.4|          0|     Friday|       33.92|\n",
      "|            1.0|               236|                237|         1.01|          0|     Friday|        10.3|\n",
      "|            1.0|               142|                239|         0.73|          0|     Friday|       12.09|\n",
      "|            1.0|               238|                166|         1.17|          0|     Friday|       12.36|\n",
      "|            1.0|               239|                238|         0.78|          0|     Friday|        9.96|\n",
      "|            2.0|               151|                142|         1.66|          0|     Friday|        12.3|\n",
      "|            3.0|               239|                142|         0.93|          0|     Friday|         9.3|\n",
      "|            2.0|               238|                142|         1.16|          0|     Friday|       11.84|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4d4076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70c1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.8,0.2],seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbeb9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/singyeongdeog/Documents/github_code/data-engineering/01-spark/data/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3334c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.write.format(\"parquet\").save(f\"{data_dir}/train/\")\n",
    "test_df.write.format(\"parquet\").save(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f69daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffcacc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16d4b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday -> 3 -> [0,0,0,1,0,0] one-hot-encoding\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "cat_feats = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "stages = []\n",
    "\n",
    "for c in cat_feats:\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol= c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c + \"_onehot\"])\n",
    "    stages += [cat_indexer, onehot_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0191a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_bec00fe4b4d4,\n",
       " OneHotEncoder_0ea42f81beb4,\n",
       " StringIndexer_6acb76e33543,\n",
       " OneHotEncoder_6b772b0e9170,\n",
       " StringIndexer_8f133e623a4c,\n",
       " OneHotEncoder_ec83ab4fee65]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a156aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_feats = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_feats:\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol= n + \"_vector\")\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol= n + \"_scaled\")\n",
    "    stages += [num_assembler, num_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b010c842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_bec00fe4b4d4,\n",
       " OneHotEncoder_0ea42f81beb4,\n",
       " StringIndexer_6acb76e33543,\n",
       " OneHotEncoder_6b772b0e9170,\n",
       " StringIndexer_8f133e623a4c,\n",
       " OneHotEncoder_ec83ab4fee65,\n",
       " VectorAssembler_2c3d71333502,\n",
       " StandardScaler_59c60a5b8b59,\n",
       " VectorAssembler_de553c584d35,\n",
       " StandardScaler_9495c34dda86,\n",
       " VectorAssembler_4661affdfa2a,\n",
       " StandardScaler_28c509e3fe39]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02edc9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pickup_location_id_onehot', 'dropoff_location_id_onehot', 'day_of_week_onehot', 'passenger_count_scaled', 'trip_distance_scaled', 'pickup_time_scaled']\n"
     ]
    }
   ],
   "source": [
    "assembler_inputs = [c + \"_onehot\" for c in cat_feats] + [n + \"_scaled\" for n in num_feats]\n",
    "print(assembler_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ab54221",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adbb06ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_bec00fe4b4d4, OneHotEncoder_0ea42f81beb4, StringIndexer_6acb76e33543, OneHotEncoder_6b772b0e9170, StringIndexer_8f133e623a4c, OneHotEncoder_ec83ab4fee65, VectorAssembler_2c3d71333502, StandardScaler_59c60a5b8b59, VectorAssembler_de553c584d35, StandardScaler_9495c34dda86, VectorAssembler_4661affdfa2a, StandardScaler_28c509e3fe39, VectorAssembler_1f80931a6d4c, VectorAssembler_1f80931a6d4c]\n"
     ]
    }
   ],
   "source": [
    "stages += [assembler]\n",
    "print(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e09f3e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transform_stages = stages\n",
    "pipeline = Pipeline(stages = transform_stages)\n",
    "fitted_transformer = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0973878",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25b95a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    maxIter = 5,\n",
    "    solver = \"normal\",\n",
    "    labelCol = \"total_amount\",\n",
    "    featuresCol = \"feature_vector\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f2e908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/31 23:04:31 WARN Instrumentation: [25f1fcb6] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 43:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/31 23:04:44 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/07/31 23:04:44 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/31 23:04:54 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = lr.fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df072327",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtest_df = fitted_transformer.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "383dce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(vtest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11dcca76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: double, pickup_location_id: int, dropoff_location_id: int, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, passenger_count_vector: vector, passenger_count_scaled: vector, trip_distance_vector: vector, trip_distance_scaled: vector, pickup_time_vector: vector, pickup_time_scaled: vector, feature_vector: vector, prediction: double]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d873b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+\n",
      "|trip_distance|day_of_week|total_amount|        prediction|\n",
      "+-------------+-----------+------------+------------------+\n",
      "|          0.7|   Saturday|       12.35|12.689527507405728|\n",
      "|          1.5|     Friday|        11.8| 14.50426259211963|\n",
      "|          2.9|     Sunday|        15.8| 16.30302554462105|\n",
      "|          2.1|   Saturday|       15.35|  16.9425655126106|\n",
      "|          1.7|   Saturday|        13.3|14.493378049111925|\n",
      "|          0.4|   Thursday|         4.8|  9.53814848314505|\n",
      "|          1.4|     Friday|         8.3|11.998519125627354|\n",
      "|          2.2|    Tuesday|        13.3|13.397050101542845|\n",
      "|          3.8|    Tuesday|       27.25|17.712843503279146|\n",
      "|          1.7|    Tuesday|        11.8|12.840609883249272|\n",
      "|          4.5|  Wednesday|       27.65|19.465887664817274|\n",
      "|         13.4|     Monday|       66.35|62.293303116515744|\n",
      "|         16.2|     Monday|       82.37| 68.89146993128816|\n",
      "|          7.2|  Wednesday|       32.75| 29.80928746034624|\n",
      "|          4.1|     Friday|        20.8|22.247206928383093|\n",
      "|          0.4|     Friday|         7.3|13.417207597463857|\n",
      "|          5.1|  Wednesday|       24.95|27.844980355817825|\n",
      "|          9.1|   Saturday|       45.85|34.216009113471955|\n",
      "|          9.3|     Friday|        36.3|33.755516146251466|\n",
      "|          6.6|   Saturday|        25.3| 28.81965249212226|\n",
      "+-------------+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select([\"trip_distance\",\"day_of_week\",\"total_amount\",\"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1962ea9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.676293357333079"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34a13162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8064767086817035"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2 # 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74404cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
