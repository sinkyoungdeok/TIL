- [1장 사용자 수에 따른 규모 확장성](#1장-사용자-수에-따른-규모-확장성)
  - [단일 서버](#단일-서버)
  - [데이터베이스](#데이터베이스)
  - [수직적 규모 확장 vs 수평적 규모 확장](#수직적-규모-확장-vs-수평적-규모-확장)
  - [캐시](#캐시)
  - [CDN](#cdn)
  - [무상태(stateless) 웹 계층](#무상태stateless-웹-계층)
  - [데이터 센터](#데이터-센터)
  - [메시지 큐](#메시지-큐)
  - [로그, 메트릭, 그리고 자동화](#로그-메트릭-그리고-자동화)
  - [데이터베이스의 규모 확장](#데이터베이스의-규모-확장)
  - [백만 사용자, 그리고 그 이상](#백만-사용자-그리고-그-이상)
- [2장 개략적인 규모 추정](#2장-개략적인-규모-추정)
  - [2의 제곱수](#2의-제곱수)
  - [모든 프로그래머가 알아야 하는 응답지연 값](#모든-프로그래머가-알아야-하는-응답지연-값)
  - [가용성에 관계된 수치들](#가용성에-관계된-수치들)
  - [예제: 트위터 QPS와 저장소 요구량 측정](#예제-트위터-qps와-저장소-요구량-측정)
  - [팁](#팁)
- [3. 시스템 설계 면접 공략법](#3-시스템-설계-면접-공략법)
  - [효과적 면접을 위한 4단계 접근법](#효과적-면접을-위한-4단계-접근법)
- [4. 처리율 제한 장치의 설계](#4-처리율-제한-장치의-설계)
  - [1단계: 문제 이해 및 설계 범위 확정](#1단계-문제-이해-및-설계-범위-확정)
  - [2단계: 개략적 설계안 제시 및 동의 구하기](#2단계-개략적-설계안-제시-및-동의-구하기)
  - [3단계: 상세 설계](#3단계-상세-설계)
  - [4단계: 마무리](#4단계-마무리)
- [5. 안정 해시 설계](#5-안정-해시-설계)
  - [해시 키 재배치 문제](#해시-키-재배치-문제)
  - [안정 해시](#안정-해시)
  - [마치며](#마치며)
- [6. 키-값 저장소 설계](#6-키-값-저장소-설계)
  - [문제 이해 및 설계 범위 확정](#문제-이해-및-설계-범위-확정)
  - [단일 서버 키-값 저장소](#단일-서버-키-값-저장소)
  - [분산 키-값 저장소](#분산-키-값-저장소)
  - [요약](#요약)
- [7. 분산 시스템을 위한 유일 ID 생성기 설계](#7-분산-시스템을-위한-유일-id-생성기-설계)
  - [1단계: 문제 이해 및 설계 범위 확정](#1단계-문제-이해-및-설계-범위-확정-1)
  - [2단계: 개략적 설계안 제시 및 동의 구하기](#2단계-개략적-설계안-제시-및-동의-구하기-1)
  - [3단계: 상세 설계](#3단계-상세-설계-1)
  - [4단계: 마무리](#4단계-마무리-1)
- [9. 웹 크롤러 설계](#9-웹-크롤러-설계)
  - [1단계: 문제 이해 및 설계 범위 확정](#1단계-문제-이해-및-설계-범위-확정-2)
  - [2단계: 개략적 설계안 제시 및 동의 구하기](#2단계-개략적-설계안-제시-및-동의-구하기-2)
  - [3단계: 상세 설계](#3단계-상세-설계-2)
  - [4단계: 마무리](#4단계-마무리-2)
  - [번외: 동적 크롤링 예시](#번외-동적-크롤링-예시)
- [10. 알림 시스템 설계](#10-알림-시스템-설계)
  - [1단계: 문제 이해 및 설계 범위 확정](#1단계-문제-이해-및-설계-범위-확정-3)
  - [2단계: 개략적 설계안 제시 및 동의 구하기](#2단계-개략적-설계안-제시-및-동의-구하기-3)
  - [3단계: 상세 설계](#3단계-상세-설계-3)
  - [4단계: 마무리](#4단계-마무리-3)
- [11. 뉴스 피드 시스템 설계](#11-뉴스-피드-시스템-설계)
  - [1단계: 문제 이해 및 설계 범위 확정](#1단계-문제-이해-및-설계-범위-확정-4)
  - [2단계: 개략적 설계안 제시 및 동의 구하기](#2단계-개략적-설계안-제시-및-동의-구하기-4)
  - [3단계: 상세 설계](#3단계-상세-설계-4)
  - [4단계: 마무리](#4단계-마무리-4)
- [12. 채팅 시스템 설계](#12-채팅-시스템-설계)
  - [1단계: 문제 이해 및 설계 범위 확정](#1단계-문제-이해-및-설계-범위-확정-5)
  - [2단계: 개략적 설계안 제시 및 동의 구하기](#2단계-개략적-설계안-제시-및-동의-구하기-5)
  - [3단계: 상세 설계](#3단계-상세-설계-5)
  - [4단계: 마무리](#4단계-마무리-5)
- [13. 검색어 자동완성 시스템](#13-검색어-자동완성-시스템)
  - [1단계: 문제 이해 및 설계 범위 확정](#1단계-문제-이해-및-설계-범위-확정-6)
  - [2단계: 개략적 설계안 제시 및 동의 구하기](#2단계-개략적-설계안-제시-및-동의-구하기-6)

## 1장 사용자 수에 따른 규모 확장성 
한 명의 사용자를 지원하는 시스템에서 시작하여, 최종적으로는 몇백만 사용자를 지원하는 시스템을 설계해 볼 것이다.


### 단일 서버

<img width="768" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/9b648507-0bfa-4a01-9e48-14fd42e7fcc1">  

웹, 앱, 데이터베이스, 캐시 등이 전부 서버 한대에서 실행된다.

사용자 요청 처리 흐름은 다음과 같다. 

1. 도메인이름을 이용하여 웹사이트에 접속. 
2. 도메인 이름을 DNS(Domain Name Service)에 질의하여 IP 주소로 변환 
3. 해당 IP주소로 HTTP 요청이 전달
4. 웹서버에서 HTML or JSON 응답 반환 



### 데이터베이스

<img width="768" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/262181b3-a2e9-4c0e-8529-a68fce7bdd9a">  

- 사용자가 늘면서 하나는 웹/모바일 트래픽 처리 용도의 서버, 다른 하나는 데이터베이스용 서버로 분리했다.
- 분리함으로써 각각을 독립적으로 확장해 나갈 수 있게 됐다.

**어떤 데이터베이스를 사용할 것인가?** 
- RDB vs NoSql 에서 고른다.
- RDB: Mysql, Oracle, PostgreSQL 등 
- NoSQL: CouchDB, Neo4j, Cassandra, HBase, Amazon DynamoDB 등
- NoSQL은 네 부류로 나눌 수 있다.
  - key-value store
  - graph store
  - column store
  - document store
- RDB는 조인 연산을 지원하는 반면, NoSQL은 조인 연산을 지원하지 않는다. 
- 아래 케이스에선 NoSQL를 고려해보자.
  - 아주 낮은 응답 지연시간 필요
  - 데이터가 비정형임
  - 데이터를 serialize, deserialize 할 수 있기만 하면 됨
  - 

### 수직적 규모 확장 vs 수평적 규모 확장
- 스케일업(수직 = vertical scaling): 서버에 고사양 자원을 추가하는 행위
- 스케일 아웃(수평 = scale out): 더 많은 서버를 추가
- 트래픽 양이 적을 때는 스케일업이 좋은 선택이고 단순함이 큰 장점인 반면에 심각한 단점이 있다.
  - 한 대의 서버에 CPU나 메모리를 무한대로 증설 할 수 없으므로 한계가 있다.
  - 장애에 대한 자동복구 방안이나 다중화 방안을 제시할 수 없다.
- 대규모 애플리케이션에서는 scale out이 더 적절하다.

트래픽이 몰릴 때에는 load balancer를 도입해보자.

**로드밸런서**  

<img width="728" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/08338d86-af45-481b-8319-a1e5da13dd38">    

- 사용자는 로드밸런서의 공개 IP 주소로 접속한다. 
- 부하 분산 집합에 웹 서버를 하나 더 추가하고 나면 장애를 자동복구하지 못하는 문제(no failover)는 해소되며, 웹 계층의 가용성(availability)은 향상된다.
  - 서버한대가 다운되면(offline) 다른 서버로 트래픽이 전송된다.
  - 트래픽이 급증하면 서버 1대이상을 추가하기만 하면 된다. 

웹 계층은 이제 괜찮은데, DB는 하나 뿐이고 장애의 자동복구나 다중화를 지원하지 않는다.  
DB 다중화는 이런 문제를 해결하는 보편화된 기술이다.


**데이터베이스 다중화**  

- 보통 DB는 서버 사이에 master-slave 관계를 설정하고 데이터 원본은 주 서버에, 사본은 부 서버에 저장한다.
- 쓰기 연산은 마스터에서만 지원한다.
- slave DB는 master DB 로부터 사본을 전달 받으며, 읽기 연산만을 지원한다. 
- 대부분의 애플리케이션은 읽기 연산 비중이 쓰기 연산보다 훨씬 높기 때문에, slave DB 수를 더 많이 구성한다.


<img width="816" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/6c43457a-2c25-4da9-960a-5939cd149818">  

- 데이터베이스를 다중화하면 다음과 같은 이득이 있다. 
  - 더 나은 성능: 병렬로 처리될 수 있는 query 수가 늘어나서 성능이 올라간다. 
  - 안정성: DB 일부가 죽더라도 데이터는 보존된다. 데이터를 지역적으로 떨어진 여러 장소에 다중화 시킬수도 있다.
  - 가용성: 데이터를 여러 지역에 복제해 둠으로써, 하나의 DB에 장애가 발생해도 다른 서버에 있는 데이터를 가져와서 서비스할 수 있다. 
- 데이터베이스 한대가 다운되면 어떤일이 벌어질까?
  - slave DB가 한대인데 다운된경우
    - 읽기 연산이 일시적으로 master DB로 감.
    - 새로운 slave DB가 장애서버를 대체 
  - slave DB가 여러대인데 다운된경우
    - 읽기 연산이 나머지 slave DB로 간다.
    - 새로운 slave DB가 장애서버를 대체 
  - master DB가 한대인데 다운된경우
    - slave DB가 master DB로 승격된다.
    - 모든 연산은 새로운 master DB로 수행된다.
    - 그리고 새로운 slave DB가 추가된다. 
    - 부 서버에 보관된 데이터가 최신 상태가 아닐 수 있는데, 이 때에는 없는 데이터에 대해서 복구 스크립트를 돌려서 추가해야 한다. 
    - 다중 마스터(multi-masters)나 원형 다중화(circular replication) 방식을 도입하면 이런 상황을 대처하는데 도움이 되지만 구성이 훨씬 복잡해진다. 

<img width="868" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/652e7778-0ae2-490f-84d8-94224e4cb10d">  
 
- 로드밸런서 + DB 다중화를 고려한 설계 


다음은 응답시간을 개선해보자.  
응답시간은 캐시를 붙이고 정적 컨텐츠를 CDN으로 옮기면 개선할 수 있다.


### 캐시
- 값비싼 연산 결과 또는 자주 참조되는 데이터를 메모리 안에 두고, 뒤이은 요청이 빨리 처리될 수 있도록 하는 저장소다.

**캐시 계층**  
- 캐시 계층은 데이터가 잠시 보관되는 곳으로 DB보다 훨씬 빠르다.
- 성능 개선 뿐 아니라 DB 부하를 줄일 수 있고, 캐시 계층을 독립적으로 확장시키는 것도 가능하다. 

<img width="802" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/cc0b0344-f7fb-44ac-83a2-fd3573a03173">  

- 이러한 캐시 전략을 읽기 주도형 캐시 전략이라고 부른다.
- 이것 이외에도 다양한 캐시 전략이 있는데, 캐시할 데이터 종류, 크기, 액세스 패턴에 맞는 캐시 전략을 선택하면 된다.

**캐시 사용 시 유의할 점**  
- 캐시는 어떤 상황에 바람직한가? 데이터 갱신은 자주 일어나지 않지만 참조는 빈번하게 일어난다면 고려해보자.
- 어떤 데이터를 캐시에 두어야 하는가? 캐시 데이터는 휘발성 메모리에 두므로, 영속적으로 보관할 데이터를 캐시에 두는 것은 바람직하지 않다. 
- 캐시에 보관된 데이터는 어떻게 만료 되는가? 이에 대한 정책을 마련해 두는 것이 좋다. 만료 기한은 너무 짧거나 길면 곤란하다. DB를 너무 자주 읽거나, 원본과 차이날 수 있기 때문이다.
- 일관성은 어떻게 유지되는가? 저장소의 원본을 갱신하는 연산과 캐시를 갱신하는 연산이 단일 트랜잭션으로 처리되지 않는 경우 일관성은 깨질 수 있다. 
- 장애에는 어떻게 대처할 것인가? 캐시 서버를 한 대만 두는 경우 SPOF가 될 수 있다. 특정 지점의 장애가 전체시스템의 동작을 중단시킬 수 있는 경우가 SPOF인데, SPOF를 피하려면 여러 지역에 걸쳐 캐시 서버를 분산시켜야 한다.

<img width="781" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/5044d3ad-e628-4ac6-9072-5cef4f2de623">  

- 캐시 메모리는 얼마나 크게 잡을 것인가? 캐시 메모리가 너무 작으면 데이터가 너무 자주 캐시에서 밀려나버려 캐시 성능이 저하될 수 있다.
- 데이터 방출(eviction) 정책은 무엇인가? 가장 널리 쓰이는 것은 LRU(마지막으로 사용된 데이터를 내보냄) 이다. LFU(사용 빈도가 가장 낮은 데이터를 내보내는 정책)이나 FIFO같은 것도 있는데 경우에 맞게 적용하자.


### CDN

- 정적 콘텐츠를 전송하는 데 쓰이는, 지리적으로 분산된 서버의 네트워크이다. 
- 이미지, 비디오, CSS, js 파일 등을 캐시할 수 있다. 
- request path, query string, cookie, request header 등의 정보에 기반하여 HTML 페이지를 캐시하는 것이다.

<img width="781" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/62d15673-538f-4f8c-8b6b-7a9ee6fb3589">  

- 어떤 사용자가 웹사이트를 방문하면, 그 사용자에게 가장 가까운 CDN 서버가 정적 콘텐츠를 전달한다.


<img width="781" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/d81941b6-ee99-4060-a3c4-adab36acc354">  


**CDN 사용 시 고려해야 할 사항**  
- 비용: CDN으로 들어가고 나가는 데이터 전송 양에 따라 요금을 내게 된다. 자주 사용하지 않은 컨텐츠를 캐싱하는 것은 이득이 크지 않으므로 CDN에서 빼는 것을 고려하자.
- 적절한 만료 시한 설정: 시의성이 중요한(time-sensitive) 컨텐츠의 경우 만료 시점을 잘 정해야 한다.
- CDN 장애에 대한 대처 방안: CDN이 응답 하지 않을 경우, 문제를 감지하여 원본 서버로부터 직접 컨텐츠를 가져오도록 클라이언트를 구성하는것이 필요할 수 있다.
- 컨텐츠 무효화 방법: 아직 만료되지 않은 컨텐츠라 하더라도 아래 방법들 중 하나를 쓰면 CDN에서 제거할 수 있다.
  - CDN 서비스 사업자가 제공하는 API 사용 
  - 컨텐츠의 다른 버전을 서비스하도록 오브젝트 버저닝이용. 컨텐츠의 새로운 버전을 지정하기 위해서 URL 마지막에 버전 번호를 인자로 주면 된다. ex) image.png?v=2 

<img width="781" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/5d1f04cf-70b2-46f2-873e-2aac12bb7ceb">  

- CDN과 캐시가 추가된 설계
- 변화된 부분
  - 정적컨텐츠는 CDN을 통해 제공하여 더 나은 성능을 보장한다.
  - 캐시가 데이터베이스 부하를 줄여준다.

### 무상태(stateless) 웹 계층 
- 웹 계층을 수평적으로 확장하는 방법을 고민해보자. 
- 이를 위해서는 상태 정보(사용자 세션 데이터와 같은)를 웹 계층에서 제거해야 한다.
- 상태 정보는 RDB, NoSQL 같은 지속성 저장소에 보관하고, 필요할 때 가져오도록 하자.
- 이렇게 구성된 웹 계층을 무상태 웹 계층이라 부른다.

**상태 정보 의존적인 아키텍처**  
- 상태 정보를 보관하는 서버와 그렇지 않은 서버 사이에는 몇가지 중요한 차이가 있다.
- 상태 정보를 보관하는 서버는 상태 정보를 요청들 사이에 공유되도록 한다. (무상태 서버에는 이런 장치가 없다)

<img width="792" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/833392fb-a63b-44fb-aa75-39b4b2b76660">  

- 같은 클라이언트로부터의 요청은 항상 같은 서버로 전송되어야 한다.
- 대부분의 로드밸런서가 이를 지원하기 위해 고정 세션이라는 기능을 제공하는데, 이는 로드밸런서에 부담을 준다.
- 게다가 로드밸런서 뒷단에 서버를 추가하거나 제거하기도 까다로워지고, 서버의 장애를 처리하기도 복잡해진다.

**무상태 아키텍처**  

<img width="624" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/fbd1e93a-3dc9-44bf-8f4e-1b1304645a5a">  

- 이 구조에서는 HTTP 요청이 어떤 웹 서버로도 전달될 수 있다. 
- 웹 서버는 상태 정보가 필요할 경우 공유 저장소로부터 데이터를 가져온다. 
- 따라서 상태 정보는 웹 서버로부터 물리적으로 분리되어 있고, 구조가 단순하고 안정적이며 규모 확장이 쉬워진다.

<img width="854" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/28e3d617-a065-4051-8dc0-5d260f515a23">

- 세션 데이터를 웹 계층에서 분리하고 지속성 데이터 보관소에 저장하도록 만들었다.
- 이 공유 저장소는 RDB가 될수도 있고, Memcached/Redis 같은 캐시 시스템일 수도 있고 NoSQL일 수도 있다.
- 여기서는 NoSQL을 사용하였으며, 규모 확장이 간편해서다.


가용성을 높이고 전 세계 어디서도 쾌적하게 사용할 수 있도록 하려면  
여러 데이터 센터를 지원하는 것이 필수다.

### 데이터 센터

<img width="854" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/d87c323d-99d8-48be-862e-314b314af0cf">  

- 두 개의 데이터 센터를 이용하는 사례다.
- 장애가 없는 상황에서 사용자는 가장 가까운 데이터 센터로 안내되는데, 이것을 지리적 라우팅이라고(geoDNS-routing or geo-routing) 부른다.
- 지리적 라우팅에서의 geoDNS는 사용자의 위치에 따라 도메인 이름을 어떤 IP 주소로 변환할지 결정할 수 있도록 해주는 DNS 서비스다.

<img width="854" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/0345acc1-a6c7-444b-8f52-a3f313d2265a">  

- 데이터 센터 중 하나에 심각한 장애가 발생하면 모든 트래픽이 장애가 없는 데이터 센터로 전송된다.
- 위 그림은 US-West에 장애가 발생하여, US-East로 전송되는 상황이다.

이 사례와 같은 다중 데이터센터 아키텍처를 만들려면 몇가지 기술적 난제를 해결해야 한다.
- 트래픽 우회: 올바른 데이터 센터로 트래픽을 보내는 효과적인 방법을 찾아야한다. ex) GeoDNS
- 데이터 동기화: 데이터 센터마다 별도의 DB를 사용하면, 장애가 자동으로 복구되어 트래픽이 다른 DB로 우회되도, 해당 데이터센터에는 찾는 데이터가 없을 수 있다. 이런 상황을 막는 보편적 전략은 데이터를 여러 데이터센터에 걸쳐 다중화하는 것이다.
- 테스트와 배포: 웹 사이트 또는 애플리케이션을 여러 위치에서 테스트해보는 것이 중요하다. 또, 자동화된 배포 도구는 모든 데이터 센터에 동일한 서비스가 설치되도록 하는 데 중요한 역할을 한다.

시스템을 더 큰 규모로 확장하기 위해서는 시스템의 컴포넌트를 분리하여, 각기 독립적으로 확장될 수 있도록 하여야 한다.  
메시지 큐는 많은 실제 분산 시스템이 이 문제를 풀기 위해 채용하고 있는 핵심적 전략 가운데 하나다. 

### 메시지 큐 
- 메시지의 무손실을 보장하는, 비동기 통신을 지원하는 컴포넌트다. 
- 메시지의 버퍼 역할을 하며, 비동기적으로 전송한다. 
- 메시지 큐를 이용하면 서비스 또는 서버 간 결합이 느슨해져서, 규모 확장성이 보장되어야 하는 안정적 애플리케이션을 구성하기 좋다. 


<img width="759" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/737f8c57-6d92-4747-92f6-b7f087553fbb">  

- 이미지의 크로핑, 샤프닝, 블러핑 등을 지원하는 사진 보정 애플리케이션을 만든다고 해 보자.
- 보정은 시간이 오래 걸릴 수 있는 프로세스이므로 비동기적으로 처리하면 편하다.
- 웹 서버는 사진 보정 작업을 메시지 큐에 넣는다. 
- 사진 보정 작업(worker) 프로세스들은 이 작업을 메시지 큐에서 꺼내어 비동기적으로 완료한다.
- 생산자와 소비자 서비스의 규모는 각기 독립적으로 확장될 수 있다. 
- 큐의 크기가  커지면 작업 프로세스를 늘리고, 큐가 항상 비어 있는 상태라면 작업 프로세스의 수를 줄일 수 있다. 

### 로그, 메트릭, 그리고 자동화 
- 소규모 웹 사이트에선 필요 없지만, 웹사이트와 함께 사업 규모가 커지고나면 로그나 메트릭, 자동화 같은 도구에 필수적으로 투자해야 한다.
- 로그: 에러 로그를 서버 단위로 모니터링 할 수도 있지만, 로그를 단일 서비스로 모아주는 도구를 활용하면 더 편리하게 검색하고 조회할 수 있다. 
- 메트릭
  - 호스트 단위 메트릭: CPU, 메모리, 디스크 I/O에 관한 메트릭
  - 종합 메트릭: DB 계층의 성능, 캐시 계층의 성능 
  - 핵심 비즈니스 메트릭: 일별 능동 사용자, 수익, 재방문 등
- 자동화: 시스템이 크고 복잡해지면 생산성을 높이기 위해 자동화 도구를 활용해야 한다. 
  - 지속적 통합을 도와주는 도구를 활용하면 개발자가 만드는 코드가 어떤 검증 절차를 자동으로 거치도록 할 수 있어서 문제를 쉽게 감지할 수 있다.


**메시지 큐, 로그, 메트릭, 자동화 등을 반영하여 수정한 설계안**  

<img width="831" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/0ebc156a-6ab2-4ccb-a255-52b55b38f090">  

1. 메시지큐는 각 컴포넌트가 느슨히 결합될 수 있도록 하고, 결함에 대한 내성을 높인다.
2. 로그, 모니터링 메트릭, 자동화 등을 지원하기 위한 장치를 추가했다.


### 데이터베이스의 규모 확장  
- 저장할 데이터가 많아지면 DB에 대한 부하가 증가하고, 그때엔 DB를 증설할 방법을 찾아야 한다.
- DB 규모를 확장하는데는 수직적 규모 확장법, 수평적 규모 확장법이 있다.

**수직적 확장**  
- 고성능의 자원(CPu, RAM, 디스크 등)을 증설하는 방법
- 수직적 접근법에는 단점이 있다.
  - 무한 증설이 불가능 하다.
  - SPOF 위험성
  - 고성능 서버로 갈수록 가격이 올라가서, 비용이 많이 든다.

**수평적 확장**  
- 샤딩이라고도 부름, 더 많은 서버를 추가해서 성능 향상 
- 대규모 DB를 샤드라고 부르는 작은 단위로 분할하는 기술을 일컫는다. 
- 모든 샤드는 같은 스키마를 쓰지만 샤드에 보관되는 데이터 사이에는 중복이 없다.

<img width="834" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/71ac5974-e3c0-4203-8d4c-5f3cc64a27ed">  

- 샤드로 분할된 데이터베이스의 예다.
- 이 케이스에서는 사용자 데이터를 어느 샤드에 넣을지는 사용자 ID에 따라 정한다.
- 샤딩 전략을 구현할 때 고려해야 할 가장 중요한 것은 샤딩 키를 어떻게 정하느냐다.
- 샤딩 키는 파티션 키라고도 부르는데, 데이터가 어떻게 분산될지 정하는 하나 이상의 칼럼으로 구성된다. 
- 위 케이스에서는 샤딩키가 user_id 이다. 
- 샤딩 키를 정할 때는 데이터를 고르게 분할 할 수 있도록 하는게 가장 중요하다. 


샤딩은 DB 규모 확장을 실현하는 훌륭한 기술이지만 완벽하진 않고, 도입하면 시스템이 복잡해지고 풀어야할 새로운 문제도 생긴다. 
- 데이터의 재 샤딩(resharding)
  - 1) 데이터가 너무 많아져서 하나의 샤드로는 더 이상 감당하기 어려울 때
  - 2) 샤드 간 데이터 분포가 균등하지 못하여 어떤 샤드에 할당된 공간 소모가 다른 샤드에 비해 빨리 진행될 때 
  - 샤드소진 이라고도 부르는 이런 현상이 발생하면 샤드 키를 계싼하는 함수를 변경하고 데이터를 재배치해야 한다. 안정해시 기법을 활용하면 이 문제를 해결할 수 있다.
- 유명인사(celebrity) 문제
   - 핫스팟 키 문제라고도 부르는데, 특정 샤드에 질의가 집중되어 서버에 과부하가 걸리는 문제다.
   - 이 문제를 풀려면 샤드를 잘 쪼개는게 중요하다.
- 조인과 비정규화
  - 하나의 DB를 여러 샤드 서버로 쪼개고 나면, 여러 샤드에 걸친 데이터를 조인하기가 힘들어진다. 
  - 이를 해결하는 한 가지 방법은 DB를 비정규화하여 하나의 테이블에서 질의가 수행할 수 있도록 하는 것이다. 

<img width="834" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/d259088b-afa8-4afc-9849-cfb544856613">  

- 샤딩을 적용 하고 DB에 대한 부하를 줄이기 위해 굳이 RDB가 요구되지 않는 기능들은 NoSQL로 이전했다.


### 백만 사용자, 그리고 그 이상
- 시스템의 규모를 확장하는 것은 지속적이고 반복적인 과정이다. 
- 수백만 사용자 이상을 지원하려면 새로운 전략을 도입해야 하고 지속적으로 시스템을 가다듬어야 할 것이다. ex) 시스템을 최적화하고 더 작은 단위의 서비스로 분할해야 할 수도 있다. 
- 이번 장에서 시스템 규모 확장을 위해 살펴본 기법들을 정리해보면
  - 웹 계층은 무상태 계층으로
  - 모든 계층에 다중화 도입
  - 가능한 한 많은 데이터를 캐시
  - 여러 데이터 센터를 지원
  - 정적 콘텐츠는 CDN 사용 
  - 데이터 계층은 샤딩을 통해 확장
  - 각 계층은 독립적 서비스로 분할
  - 시스템을 지속적으로 모니터링하고, 자동화 도구들을 활용할 것

## 2장 개략적인 규모 추정

### 2의 제곱수

- 분산 시스템에서 다루는 데이터 양은 엄청 커질 수 있으나 계산법은 기본을 크게 벗어나지 않는다.
- 제대로 된 계산 결과를 얻으려면 데이터 볼륨의 단위를 2의 제곱수로 표현하면 어떻게 되는지 알아야 한다.

| 2의 x 제곱 | 근사치 | 이름 | 축약형 |
| --- | --- | --- | --- |
| 10 | 1천 | 1킬로바이트 | 1KB |
| 20 | 1백만 | 1메가바이트 | 1MB |
| 30 | 10억 | 1기가바이트 | 1GB |
| 40 | 1조 | 1테라바이트 | 1TB |
| 50 | 1000조 | 1페타바이트 | 1PB |

### 모든 프로그래머가 알아야 하는 응답지연 값 

- 구글의 제프 딘에서 2010년에 통상적인 컴퓨터에서 구현된 연산들의 응답지연 값을 공개했었다.
- 이들 중 몇몇은 컴퓨터가 더 빨라지면서 유효하지 않지만, 아직 어느정도 짐작할 수 있도록 해준다. 

| 연산명 | 시간 |
| --- | --- |
| L1 캐시 참조 | 0.5ns |
| 분기 예측 오류 | 5ns |
| L2 캐시 참조 | 7ns |
| 뮤텍스 락/언락 | 100ns |
| 주 메모리 참조 | 100ns |
| Zippy로 1KB 압축 | 10,000ns = 10us |
| 1 Gbps 네트워크로 2KB 전송 | 20,000ns = 20us |
| 메모리에서 1MB 순차적으로 read | 250,000ns = 250us |
| 같은 데이터 내에서의 메시지 오아복 지연시간 | 500,000ns = 500us |
| 디스크 검색 | 10,000,000ns = 10ms |
| 네트워크에서 1MB 순차적으로 read | 10,000,000ns = 10ms |
| 디스크에서 1MB 순차적으로 read | 30,000,000ns = 30ms |
| 한 패킷의 CA로부터 네덜란드까지의 왕복 지연시간 | 150,000,000ns = 150ms |

정리하자면
- 메모리는 빠르지만 디스크는 아직도 느리다
- 디스크 검색은 가능한 피하라
- 단순한 압축 알고리즘은 빠르다
- 데이터를 인터넷으로 전송하기 전에 가능하면 압축하자.
- 데이터 센터는 보통 여러 지역에 분산되어 있고, 센터들 간에 데이터를 주고받는 데는 시간이 걸린다.

### 가용성에 관계된 수치들 
- 고가용성(HA = high availability)은 시스템이 오랜 시간 동안 지속적으로 중단 없이 운영될 수 있는 능력의 용어다. 
- 100%는 시스템이 중단 없다는 의미 이고, 대부분의 서비스는 99%  ~ 100% 사이의 값을 찾는다.
- SLA(Service Level Agreement)는 서비스 사업가(service provider)가 보션적으로 사용하는 용어로, 서비스 사업자와 고객 사이에 맺어진 합의이다.
- 이 합의에는 서비스 사업자가 제공하는 서비스의 가용시간(uptime)이 공식적으로 기술되어 있다. 

| 가용률 | 하루당 장애시간 | 주당 장애시간 | 개월당 장애시간 | 연간 장애시간 |
| --- | --- | --- | ---| --- |
| 99% | 14.40분 | 1.68시간 | 7.31시간 | 3.65일 |
| 99.9% | 1.44분 | 10.08분 | 43.83분 | 8.77시간 |
| 99.99% | 8.64초 | 1.01분 | 4.38분 | 52.60분 |
| 99.999% | 864.00밀리초 | 6.05초 | 26.30초 | 5.26분 |
| 99.9999% | 86.40밀리초 | 604.80밀리초 | 2.63초 | 31.56초 |

### 예제: 트위터 QPS와 저장소 요구량 측정 

제시된 수치들은 예시일뿐, 실제 성능이나 요구사항과는 관계 없다.

가정
- 월간 능동 사용자(MAU = montly active user)는 3억명이다.
- 50%의 사용자가 트위터를 매일 사용
- 평균적으로 각 사용자는 매일 2건의 트윗을 올린다
- 미디어를 포함하는 트윗은 10% 정도다
- 데이터는 5년간 보관된다

추정
- QPS 추정치 
- 일간 능동 사용자(DAU = daily active user) = 3억 % 50% = 1.5억
- QPS = 1.5 억 X 2 트윗 / 24시간 / 3600초 = 약 3500
- 최대 QPS = 2 X QPS = 약 7000

미디어 저장을 위한 저장소 요구량 
- 평균 트윗 크기 
  - tweet_id에 64바이트
  - 텍스트에 140바이트 
  - 미디어에 1MB
- 미디어 저장소 요구량: 1.5억 X 2 X 10% X 1MB = 30TB/일
- 5년간 미디어를 보관하기 위한 저장소 요구량: 30TB X 365 X 5 = 약 55PB

### 팁

개략적 규모 추정 면접에서 가장 중요한 것은 문제를 풀어 나가는 절차고, 문제 해결 능력이다.  
- 근사치를 활용한 계산: 99987/9.1 대신 100,000 / 10으로 간소화하자.
- 가정들은 적어두자 
- 단위를 붙이자. 스스로 헷갈리지 않기 이ㅜ해
- 많이 출제되는 개략적 규모 추정 문제는 QPS, 최대 QPS, 저장소 요구량, 캐시 요구량, 서버 수 등을 추정하는 것이다. 


## 3. 시스템 설계 면접 공략법 

- 시스템 설계 면접은 두 명의 동료가 모호한 문제를 풀기 위해 협력하여 그 해결책을 찾아내는 과정에 대한 시뮬레이션이다. 
- 최종적으로 도출될 설계안은 노력에 비하면 그다지 중요하지 않다. 
- 해당 면접은 설계 기술을 시연하는 자리이고, 설계 과정에서 내린 결정들에 대한 방어 능력을 보이는 자리이며, 면접관의 피드백을 건설적인 방식으로 처리할 자질이 있음을 보이는 자리이다. 
- 많은 사람이 시스템 설계 면접은 지원자의 설계 능력의 기술적 측면을 평가하는 자리라고 생각하지만, 지원자가 협력에 적합한 사람인지, 압박이 심한 상황도 잘 헤쳐 나갈 자질이 있는지, 모호한 문제를 건설적으로 해결할 능력이 있는지, 좋은 질문을 던질 능력이 있는지 등도 평가한다. 
- 훌륭한 면접관은 부정적 신호도 놓치지 않는다.
  - 설계의 순수성에 집착한 나머지 타협적 결정(tradeoff)을 도외시하고 over-engineering을 하고 마는 엔지니어들이 현업에도 많다.
  - 그런 엔지니어들은 과도한 엔지니어링의 결과로 시스템 전반의 비용이 올라간다는 사실을 알아채지 못하는 일이 많은데, 그 결과로 상당후 회사들은 값비싼 대가를 치르고 있다. 
- 이번 장에서는 시스템 설계 면접에 관한 유용한 팁들을 살펴보고, 시스템 설계 문제를 공략하는 효과적 접근법을 소개한다.

### 효과적 면접을 위한 4단계 접근법

**1단계: 문제 이해 및 설계 범위 확정**  

- 시스템 설계 면접에서는 생각 없이 바로 답을 내서는 안된다.
- 요구사항을 완전히 이해하지 않고 답을 내놓는 행위는 엄청난 부정적 신호다.
- 속도를 늦추고, 깊이 생각하고 질문하여 요구사항과 가정들을 분명히 하자.
- 엔지니어가 가져야할 가장 중요한 기술 중 하나는 올바른 질문, 적절한 가정, 그리고 시스템 구축에 필요한 정보를 모으는 것이다.
- 요구사항을 정확히 이해하는데 필요한 질문의 예시는 
  - 구체적으로 어떤 기능들을 만들어야 하나?
  - 제품 사용자 수는 얼마나 되나?
  - 회사의 규모는 얼마나 빨리 커지리라 예상하나? 석 달, 여섯 달, 일년 뒤의 규모는 얼마가 되리라 예상하는가?
  - 회사가 주로 사용하는 기술스택은 무엇인가? 설계를 단순화하기 위해 활용할 수 있는 기존 서비스로는 어떤 것들이 있는가?


예시: 뉴스피드 시스템 설계에서 요구사항을 분명히 하기 위한 질문을 던져야 한다.

지원자: 모바일 앱과 웹 앱 가운데 어느 쪽을 지원해야 하나요? 아니면 둘 다일까요?  
면접관: 둘다 지원해야 합니다.  
지원자: 가장 중요한 기능은 무엇 인가요?  
면접관: 새로운 포스트(post)를 올리고, 다른 친구의 뉴스피드를 볼 수 있도록 하는 기능 입니다.  
지원자: 이 뉴스피드는 시간 역순으로 정렬되어야 하나요? 아니면 다른 특별한 정렬 기준이 있습니까?    
제가 특별한 정렬 기준이 있느냐고 묻는이유는, 피드에 올라갈 포스트마다 다른 가중치가 부여되어야 하는지 알고 싶어서 인데요.   
가령 가까운 친구의 포스트가 사용자그룹(user group) 에 올라가는 포스트 보다 더 중요하다거나.  
면접관: 문제를 단순하게 만들기 위해, 일단 시간 역순으로 정렬된다고 가정합시다.  
지원자: 한 사용자는 최대 몇명의 사용자와 친구를 맺을수있나요?  
면접관: 500명입니다.  
지원자: 사이트로 오는 트래픽규모는 어느정도입니까?  
면접관: 일간능동사용자(dailyactive user,DAU)는 천만명입니다.  
지원자: 피드에 이미지나 비디오도 올라올수있나요? 아니면 포스트는 그저 텍스트입니까?  
면접관: 이미지나 비디오 같은 미디어 파일도 포스트 할 수 있어야 합니다. 


**2단계: 개략적인 설계안 제시 및 동의 구하기**   
- 설계안에 대한 최초 청사진을 제시하고 의견을 구하라. 면접관을 마치 팀원인 것처럼 대하자.
- 화이트보드나 종이에 핵심 컴포넌트를 포함하는 다이어그램을 그려라. 클라이언트, API, 웹 서버, 데이터 저장소, 캐시, CDN, 메시지 큐 같은 것들이 포함될 수 있다.
- 최초 설계안이 시스템 규모에 관계된 제약사항들을 만족하는지를 개략적으로 계산해보자. 이런 개략적 추정이 필요한지는 면접관한태 미리 물어보자.

예시: 뉴스 피드 시스템을 설계에서 개략적 설계를 살펴보자   

개략적으로 보면 이 설계는 두가지 flow로 나눠 생각 할 수 있는데, 피드 발생과 피드 생성이다.
- 피드 발행: 사용자가 포스트를 올리면 관련된 데이터가 캐시/데이터베이스에 기록되고, 해당 사용자의 친구 뉴스 피드에 뜬다.
- 피드 생성: 어떤 사용자의 뉴스 피드는 해당 사용자 친구들의 포스트를 시간 역순으로 정렬하여 만든다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/07c15030-6ae3-4c32-ab69-46f207b40d52)  
- 3-1: 피드 발생
- 3-2: 피드 생성


**3단계: 상세 설계**     
- 이 단계로 왔다면 면접관과 다음 목표는 달성한 상태이다.  
  - 시스템에서 전반적으로 달성해야 할 목표와 기능 범위 확인
  - 전체 설계의 개략적 청사진 마련
  - 해당 청사진에 대한 면접관의 의견 청취
  - 상세 설계에서 집중해야 할 영역들 확인 
- 이제 면접관과 설계 대상 컴포넌트 사이의 우선순위를 정하는 것이다.   
- 대부분의 경우 면접관은 특정 시스템 컴포넌트들의 시부사항을 깊이 있게 설명하는 것을 보길 원한다.
- 단축 URL 생성기라면, 해시 함수의 설계를 / 채팅 시스템에 관한 문제라면, 지연시간을 줄이고 사용자의 온/오프라인 상태를 표시할 것인지를 듣고자 할 것이다.
- 반대로 불필요한 세부사항에 시간을 쓰지말자. 뉴스 피드의 순위를 매기는데 사용되는 EdgeRank 알고리즘에 대해 이야기하는 것은 바람직 하지 않다. 시간을 너무 많이 쓰고, 규묘확장 가능한 시스템 설계 능력을 보여주기에는 도움되지 않는다.


예시: 뉴스 피드 시스템의 개략적 설계를 마친 상황 (면접관도 그 설계에 만족)  

두가지 중요한 용례를 깊이 탐구하자 
1. 피드 발행 (3-3)
2. 뉴스 피드 가져오기 (3-4)

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/8313f2c3-771d-4c85-95af-8fdb153618f2)  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/d3f41763-487e-4956-9034-1e379e1edb27)  


**4단계: 마무리**   
- 면접관이 시스템 병목구간, 혹은 좀 더 개선 가능한 지점을 찾아내라 주문할 수 있다. 
  - 설계가 완벽하다거나 개선할 부분이 없다고 답하지 말자, 개선할 점은 언제든 있다.
- 설계를 한번 다시 요약해주는 것도 좋다.
- 오류가 발생하면 무슨일이 생기는지 따져보는것도 좋다.
- 운영 이슈도 논의할 가치가 충분하다. 
  - 메트릭은 어떻게 수집하고 모니터링?
  - 로그는?
  - 시스템은 어떻게 배포(roll-out)해나 갈 것인가?
- 미래에 닥칠 규모 확장 요구에 어떻게 대처할 것인지도 좋다.
- 시간이 좀 남으면, 필요하지만 다루지 못했던 세부적 개선사항들을 제안해보자.



면접 세션에서 해야 할 것  
- 질문을 통해 확인하자. 스스로 내린 가정이 옳다 믿고 진행하지 말자
- 문제의 요구사항을 이해해라
- 정답이나 최선의 답안 같은 것은 없다는 것을 명심하자
- 사고 흐름을 이해할 수 있도록 소통 하자
- 가능하다면 여러 해법을 함께 제시하자
- 개략적 설계에 면접관이 동의하면, 각 컴포넌트의 세부사항을 설명하기 시작하고 가장 중요한 컴포넌트부터 진행해라.
- 면접관의 아이디어를 이끌어 내자. 좋은 면접관은 팀원처럼 협력한다
- 포기하지 말자

면접 세션에서 하면 안되는 것  
- 전형적인 면접 문제들에도 대비하지 않은 상태에서 면접장에 가지 말자
- 요구사항이나 가정들을 분명히 하지 않은 상태에서 설계를 제시하지 말자
- 처음부터 특정 컴포넌트의 세부사항을 깊이 설명하지 말자. 개략적 부터 세부사항으로 가자.
- 진행 중에 막혔다면, 힌트를 청하자
- 침묵 속의 설계를 진행하지 말자.
- 설계안을 내놓는 순간 면접이 끝났다고 생각하지 말자. 의견을 일찍, 자주 구하자

시간 배분 - 예시 45분 
1. 문제 이해 및 설계 범위 확정: 3분 ~ 10분
2. 개략적 설계안 제시 및 동의 구하기: 10분 ~ 15분
3. 상세 설계: 10분 ~ 25분
4. 마무리: 3분 ~ 5분 


## 4. 처리율 제한 장치의 설계 

- 네트워크 시스템에서 rate limiter(처리율 제한 장치)는 클라이언트 또는 서비스가 보내는 트래픽의 처리율을 제어하기 위한 장치이다.
- HTTP에선 클라이언트의 요청 횟수를 제한한다.
- API 요청 횟수가 제한 장치에 정의된 threshold를 넘어서면 추가로 도달한 모든 호출은 처리가 중단된다. 
- 예시
  - 사용자는 초당 2회 이상 새 글을 올릴 수 없다.
  - 같은 IP 주소로는 하루에 10개 이상의 계정을 생성할 수 없다.
  - 같은 디바이르소는 주당 5외 이상 리워드를 요청할 수 없다.

API에 rate limiter를 적용하면 좋은점
- DOS 공격에 의한 자원 고갈(resource starvation) 방지 
  - 예) 트위터는 3시간동안 300개의 트윗만 올릴 수 있도록 제한하고 있다.
- 비용을 절감한다
  - 서버를 많이 두지 않아도 되고, 우선순위가 높은 API에 더 많은 자원을 할당할 수 있음.
  - 신용을 확인하거나, 신용카드 결제를 하거나, 건강 상태를 확인하거나 하기 위해 호출하는 API에 대한 과금이 횟수에 따라 이뤄져야 비용 절감이 가능하다.
- 서버 과부하를 막는다.
  - 봇에서 오는 트래픽이나 사용자의 잘못된 이용패턴으로 유발된 트래픽을 걸러낼 수 있다.



### 1단계: 문제 이해 및 설계 범위 확정

rate limiter를 구현할 때 여러 가지 알고리즘을 사용할 수 있는데, 각각은 고유한 장단점을 갖고 있다.

**요구사항**  
- 설정된 처리율을 초과하는 요청은 정확하게 제한한다.
- 낮은 응답시간: rate limiter는 HTTP 응답시간에 나쁜 영향을 주면 안된다.
- 가능한 한 적은 메모리를 써야 한다.
- distributed rate limiting: 하나의 rate limiter를 여러 서버나 프로세스에서 공유할 수 있어야 한다.
- 예외 처리: 요청이 제한되었을 때는 그 사실을 사용자에게 분명하게 보여주어야 한다.
- 높은 결함 감내성(fault tolerance): 제한 장치에 장애가 생기더라도 전체 시스템에 영향을 주면 안된다.

### 2단계: 개략적 설계안 제시 및 동의 구하기

일을 너무 복잡하게 만드는 것은 피하고, 기본적인 클라이언트-서버 통신 모델을 사용하자.

**rate limiter는 어디에 둘 것인가?**  
- 클라이언트에 둔다면: 일반적으로 클라이언트는 rate limiter를 안정적으로 걸 수 있지 않다. 클라이언트 요청은 쉽게 위변조가 가능하기 떄문이고, 모든 클라이언트의 구현을 통제하는 것도 어려울 수 있다.

**서버 1안**

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/1bd4da30-0543-4e55-84e4-8d8c357cefaf)



**서버 2안**   

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/bda941ce-5727-42e7-88fc-13bc69fcc154)  
- 처리율 제한 장치를 API 서버에 두는 대신, 처리율 제한 미들웨어를 만들어 미들웨어를 통해 API 서버로 가는 요청을 통제한다.
- 클라우드 마이크로서비스의 경우, rate limiter는 보통 API 게이트웨이라 불리는 컴포넌트에 구현된다.
- 클라우드 업체가 처리율 제한, SSL 종단, 사용자 인증, IP 허용목록 관리 등을 지원하는 완전 위탁관리형 서비스를 API게이트웨이에서 만들고 유지보수한다.
- rate limiter 기능을 설계할 때 중요하게 따져야 하는것은
  - 서버에 둘지, 게이트웨이에 둘지다.
  - 회사의 기술 스택, 엔지니어링 인력, 우선순위, 목표에 따라 달라질 수 있다.


rate limiter에 일반적으로 적용될 수 있는 몇가지 지침   
- 프로그래밍 언어, 캐시 서비스 등 현재 사용하고 있는 기술 스택을 점검. 현재 사용하는 프로그래밍 언어가 서버 측 구현을 지원하기 충분할 정도로 효율이 높은지 확인하자.
- 필요에 맞는 처리율 제한 알고리즘을 찾자. 서버측에서 구현하면, 알고리즘은 자유롭게 선택할 수 있지만, 제3 사업자가 제공하는 게이트웨이를 사용하면 선택지는 제한될 수 있다.
- 설계가 마이르코서비스에 기반하고 있고, 사용자 인증이나 IP 허용목록 관리 등을 처리하기 위해 API 게이트웨이를 이미 설계에 포함시켰다면 rate limiter도 포함시키는게 좋다.
- rate limiter를 만드는 데도 시간이 많이 든다. 구현하기에 충분한 인력이 없다면 상용 API 게이트웨이를 쓰는 것이 바람직하다.


**처리율 제한 알고리즘**  
널리 알려진 알고리즘  
- 토큰 버킷
- 누출 버킷
- 고정 윈도 카운터
- 이동 윈도 로그
- 이동 윈도 카운터 

**1. 토큰 버킷 알고리즘**  
- rate limiter에 폭넓게 이용되고 있다.
- 간단하고, 알고리즘에 대한 세간의 이해도도 높은 편이며 인터넷 기업들이 보편적으로 사용하고 있다.
- 아마존, 스트라라이프가 사용중이다.

토큿 버킷 알고리즘의 동작 원리    
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/ebd13289-58bd-4863-9dd9-988550decdb1)     
- 토큰 버킷은 지정된 용량을 갖는 컨테이너다.
- 이 버켓에 사전 설정된 양의 토큰이 주기적으로 채워진다.
- 토큰이 꽉 찬 버킷에는 더 이상의 토큰은 추가되지 않는다.
- 위 그림 예제는 용량이 4인 버킷.
- 토큰 공급기(refiller)는 이 버킷에 매초 2개의 토큰을 추가하고, 버킷이 가득 차면 추가로 공급된 토큰은 버려진다.
- 각 요청은 처리될 때마다 하나의 토큰을 사용하는데, 요청이 도착하면 버킷에 충분한 토큰이 있는지 검사한다.
  - 충분한 토큰이 있는 경우, 버킷에서 토큰 하나를 꺼낸 후 요청을 시스템에 전달한다.
  - 충분한 토큰이 없는 경우, 해당 요청은 버려진다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/6ed112c9-da5f-4f84-9c11-351ad187ba23)  
- 토큰 버킷 크기:4 , 토큰 공급률: 분당 4
- 토큰 버킷 알고리즘은 2개 인자를 받는다
  - 버킷 크기: 버킷에 담을 수 있는 토큰 최대 갯수
  - 토큰 공급률: 초당 공급되는 버킷 수 


버킷은 몇개나 사용해야 할까? 공급 제한 규칙에 따라 달라진다.  
- 통상적으로, API 엔드포인트마다 별도의 버킷을 둔다. 예) 사용자마다 하루에 한 번만 포스팅 할 수 있음
- IP 주소별로 처리율 제한을 적용해야 한다면, IP 주소마다 버킷을 하나씩 할당해야 한다.
- 시스템의 처리율을 초당 10,000개 요청으로 제한하고 싶다면, 모든 요청이 하나의 버킷을 공유하도록 해야 할 것이다.


토큰 버킷 알고리즘의 장단점  

장점
- 구현이 쉬움
- 메모리 사용 측면에서도 효율
- 짧은 시간에 집중되는 트래픽도 처리 가능. 
- 버킷에 남은 토큰이 있기만 하면 요청은 시스템에 전달된다.  

단점
- 파라미터 2개 (버킷 크기, 공급률) 값을 적절하게 튜닝하는 것이 까다롭다


**2. 누출 버킷 알고리즘**  

- 토큰 버킷 알고리즘과 비슷하지만 요청 처리율이 고정되어 있다는 점이 다르다.
- 보통 FIFO 큐로 구현한다.


동작 원리

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/bbd11ec1-d808-4384-bbbc-b06a781c0b22)  
- 요청이 도착하면 큐가 가득 차 있는지 보고, 빈자리가 있는 경우에는 큐에 요청을 추가한다
- 큐가 가득 차 있는 경우에는 새 요청은 버린다.
- 지정된 시간마다 큐에서 요청을 꺼내어 처리한다.
- 인자는 두개를 사용한다
  - 버킷 크기: 큐 사이즈.
  - 처리율: 지정된 시간당 몇 개의 항목을 처리할지 지정하는 값. 보통 초 단위로 표현
- 전자상 거래 기업인 쇼피파이가 사용중.

장점  
- 큐의 크기가 제한되어 있어 메모리 사용량 측면에서 효율적 
- 고정된 처리율을 갖고 있기 때문에 안정적 출력이 필요한 경우에 적합하다.

단점  
- 단시간에 많은 트래픽이 몰리는 경우 큐에는 오래된 요청들이 쌓이게 되고, 그 요청들을 제때 처리 못하면 최신 요청들은 버려진다.
- 두개의 파라미터를 올바르게 튜닝하기가 까다롭다.


**3. 고정 윈도 카운터 알고리즘**  

동작 원리  

- 타임라인을 고정된 간격의 윈도로 나누고, 각 윈도마다 카운터를 붙인다.
- 요청이 접수될 때마다 이 카운터의 값은 1씩 증가한다
- 이 카운터의 값이 사전에 설정된 threshold에 도달하면 새로운 요청은 새 윈도가 열릴 때까지 버려진다

장점  
- 메모리 효율 좋고, 이해하기 쉬움
- 윈도가 닫히는 시점에 카운터를 초기화하는 방식은 특정한 트래픽 패턴을 처리하기에 적합하다

단점  
- 윈도 경계 부근에서 일시적으로 많은 트래픽이 몰려드는 경우, 기대했던 시스템의 처리 한도보다 많은 양의 요청을 처리하게 된다.



**4. 이동 윈도 로깅 알고리즘**  
- 고정 윈도 카운터 알고리즘의 윈도 경계 부근에 트래픽이 집중되는 경우 시스템에 설정된 한도보다 많은 요청을 처리하는 현상을 해결할 수 있다.

동작 원리  
- 요청의 타임스탬프를 추적한다. 타임스탬프 데이터는 보통 redis의 정렬 집합(sorted set) 같은 캐시에 보관한다.
- 새 요청이 오면 만료된 타임스탬프는 제거한다. 만료된 타임스탬프는 그 값이 현재 윈도의 시작 시점보다 오래된 타임스탬프를 말한다.
- 새 요청의 타임스탬프를 로그에 추가한다.
- 로그의 크기가 허용치보다 같거나 작으면 요청을 시스템에 전달한다. 그렇지 않은 경우에는 처리를 거부한다.


예) 분당 2개 요청이 한도인 시스템  
- 요청이 1:00:01에 도착할 때, 로그가 비어있으므로 허용
- 요청이 1:00:30에 도착할 때, 허용 한도인 2보다 크지 않으므로 허용
- 요청이 1:00:50에 도착할 때, 추가 직후 로그 크기는 3이므로, 거부되지만 타임스탬프는 추가한다
- 요청이 1:01:40에 도착할 때, [1:00:40, 1:01:40) 범위 안에 있는 요청이 1분 윈도 안에 요청이고, 그 이전 타임스탬프는 만료된 값이다. 그래서 만료된 타임스탬프인 01, 30초 로그를 지우고 해당 요청은 허용한다.

장점  
- 알고리즘 메커니즘이 아주 정교하다. 
- 어느순간의 윈도를 보더라도, 허용되는 요청의 개수는 시스템의 처리율 한도를 넘지 않는다.

단점  
- 다량의 메모리를 사용한다.
- 거부된 요청의 타임스탬프도 보관하기 때문이다. 

**5. 이동 윈도 카운터 알고리즘**     
- 고정 윈도 카운터 알고리즘 + 이동 윈도 로깅 알고리즘
- 구현방법은 2가지인데, 여기서는 하나만 설명 


동작 원리  
- 처리율 제한 장치의 한도: 분당 7개, 이전 1분 동안 5개 요청, 현재 1분 동안 3개의 요청이 왔다고 해보자.
- 현재 1분의 30% 시점에 도착한 새 요청의 경우, 현재 윈도에 몇 개의 요청이 온것으로 보고 처리할까?
  - 현재 1분간의 요청 수 + 직전 1분간의 요청 수 X 이동 윈도와 직전 1분이 겹치는 비율 
  - 3 + 5 X 70% = 6.5개. 반올림, 내림 둘다 쓸 수 있지만, 여기에서는 내림하여 써서 6이다.
- 분당 한도가 7개 요청이므로 이번 요청은 성공하지만, 그 직후에는 요청을 받을 수 없게 된다.

장점  
- 이전 시간대의 평균 처리율에 따라 현재 윈도의 상태를 계산하므로 짧은 시간에 몰리는 트래픽에도 잘 대응한다.
- 메모리 효율이 좋다.

단점  
- 직접 시간대에 도착한 요청이 균등하게 분포되어 있다고 가정한 상태에서 추정치를 계산하기 때문에 다소 느슨하다.
- 하지만 이 문제는 생각만큼 심각하진 않지만, 클라우드 플레어가 실시했던 실험에 따르면 40억 개의 요청 가운데 시스템의 실제 상태와 맞지 않게 허용되거나 버려진 요청은 0.003%에 불과했다. 


**개략적인 아키텍처**  
- rate limiter 알고리즘의 기본 아이디어는 단순하다.
- 얼마나 많은 요청이 접수되었는지를 추적할 수 있는 카운터를 추적 대상 별로 두고 (사용자, IP, API 엔드포인트, 서비스 단위 등), 이 카운터의 값이 한도를 넘으면 이후 도착한 요청은 거부한다.
- 그럼 카운터는 어디 보관 할까?
  - DB는 느려서 안되고
  - 메모리상에서 동작하는 캐시가 바람직한데, 빠른데다 시간에 기반한 만료 정책을 지원하기 때문이다. 
  - 레디스는 rate limiter를 구현할 때 자주 사용되는 메모리 기반 저장장치로서, INCR와 EXPIRE 두가지 명령어를 지원한다.
    - INCR: 메모리에 저장된 카운터의 값을 1만큼 증가
    - EXPIRE: 카운터에 타임아웃 값을 설정하고, 설정된 시간이 지나면 카운터는 자동으로 삭제된다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/3e6b32ef-ecc3-4d2f-b600-7a6225284e39)  
- 클라이언트가 처리율 제한 미들웨엉에게 요청을 보낸다.  
- 처리율 제한 미들웨어는 레시드의 지정 버킷에서 카운터를 가져와서 한도에 도달했는지 아닌지를 검사한다 
  - 한도에 도달하면 요청은 거부
  - 도달하지 않았다면 요청은 API 서버로 전달하고 미들웨어는 카운터의 값을 증가시킨 후 다시 레디스에 저장한다.


### 3단계: 상세 설계   
4-12 그림에서는 다음과 같은 사항은 알 수 없었다.  
- 처리율 제한 규칙은 어떻게 만들어지고 어디에 저장하나?
- 처리가 제한된 요청들은 어떻게 처리되는가?  

**처리율 제한 규칙**  
```yaml
domain: messaging
descriptors:
  - key: message_type
    Value: marketing
    rate_limit:
      unit: day
      requests_per_unit: 5
```
- 마케팅 메시지의 최대치를 하루 5개로 제한한 예제

```yaml
domain: auth
descriptors:
  - key: auth_type
    Value: login
    rate_limit:
      unit: minute
      requests_per_unit: 5
```
- 분당 5회 이상 로그인 할 수 없도록 제한한 예제 
- 이런 규칙들은 보통 설정파일 형태로 디스크에 저장된다.

**처리율 한도 초과 트래픽의 처리**  
- 어떤 요청이 한도 제한에 걸리면 HTTP 429 응답을 클라이언트에 보낸다.  
- 경우에 따라서는 한도 제한에 걸린 메시지를 나중에 처리하기 위해 큐에 보관할 수도 있다.
- 예를 들어, 어떤 주문이 시스템 과부하 때문에 한도 제한에 걸렸다고 하면 해당 주문건들은 보관했다가 나중에 처리할 수도 있을 것 이다.

처리율 제한 장치가 사용하는 HTTP 헤더  
- 클라이언트는 자기 요청이 처리율 제한에 걸리고 있는지를(throttle) 어떻게 감지할 수 있나?  
- 자기 요청이 처리율 제한에 걸리기까지 얼마나 많은 요청을 보낼 수 있는지 어떻게 알 수 있나?
- 답은 HTTP 응답 헤더에 있다.
- 이번 장에서 설계하는 rate limiter 헤더
  - X-Ratelimit-Remaining: 윈도 내에 남은 처리 가능 요청의 수
  - X-Ratelimit-Limit: 매 윈도마다 클라이언트가 전송할 수 있는 요청의 수
  - X-Ratelimit-Retry-After: 한도 제한에 걸리지 않으려면 몇 초 뒤에 요청을 다시 보내야 하는지 알림 
- 사용자가 너무 많은 요청을 보내면 429 오류를 X-Ratelimit-Retry-After 헤더와 함께 반환한다.


**상세 설계**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/210383ad-d361-4925-b39c-db6c11c806a1)   
- 처리율 제한 규칙은 디스크에 보관하고, 작업 프로세스(workers)는 수시로 규칙을 디스크에서 읽어 캐시에 저장한다
- 클라이언트가 요청을 서버에 보내면 처리율 제한 미들웨어에 도달한다.
- 처리율 제한 미들웨어는 제한 규칙을 캐시에서 가져온다. 아울러 카운터 및 마지막 요청의 타임스탬프를 레디스 캐시에서 가져온다. 
  - 요청이 제한에 걸리지 않으면 API 서버로 보낸다
  - 제한에 걸리면 429 에러를 클라이언트에 보낸다. 한편 해당 요청은 그대로 버릴 수도 있고 메시지 큐에 보관할 수도 있다.


**분산 환경에서의 처리율 제한 장치의 구현**  
- 단일 서버에서의 rate limiter는 어렵지 않다.
- 하지만 여러 대의 서버와 병렬 스레드를 지원하도록 시스템을 확장하는 것은 또 다른 문제다. 다음 두가지 어려운 문제를 풀어야 한다
  - 경쟁 조건 
  - 동기화


경쟁 조건  
rate limiter는 다음으로 동작한다  
- 레디스에서 카운터의 값을 읽는다
- counter + 1 값이 임계치를 넘는지 본다
- 넘지 않는다면 레디스에 보관된 카운터 값을 1만큼 증가시킨다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/147afc24-89af-4e40-9f1a-2dceef0c40b9)  
- 병행성이 심한 환경에서는 위와 같은 경쟁 조건 이슈가 발생할 수 있다.  
- 레디스에 저장된 변수 counter의 값이 3이라고 하자.
- 그리고 두 개 요청을 처리하는 스레드가 각각 병렬로 counter 값을 읽었지만 변경된 값을 저장하지 않은 상태이다.
- 둘다 다른 요청의 처리 상태는 상관하지 않고 counter에 1을 더한 값을 레디스에 기록할 것이다.
- 4로 변경될거지만, 사실은 5가 되어야 한다.
- 경쟁 조건 문제를 해결하는 가장 널리 알려진 해결책은 락이다.
- 하지만 락은 시스템의 성능을 상당히 떨어뜨리는 문제가 있다.  
- 위 설계의 경우에는 락 대신 쓸 수 있는 해결책이 두 가지 있는데, 하나는 루아 스크립트이고 다른 하나는 정렬 집합이라 불리는 레디스 자료구조를 쓰는 것이다.


동기화 이슈  
- 동기화는 분산 환경에서 고려해야 할 또 다른 중요한 요소다.
- 수백만 사용자를 지원하려면 한 대의 rate limiter 서버로는 충분하지 않을 수 있다.
- 그래서 rate limiter 서버를 여러 대 두게 되면 동기화가 필요해진다.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/e4b7a8bb-2c41-4a73-b604-4b7480e2646f)  
- 왼쪽 그림: 클라이언트 1은 rate limit1, 클라이언트 2는 rate limit
- 웹 계층은 stateless이므로 다음 요청은 우측 그림처럼 보낼 수 있다.
- 이때 동기화를 하지 않는다면 제한 장치 1은 클라이언트 2에 대해서는 아무것도 몰라서 처리율 제한을 올바르게 수행할 수 없을 것이다. 
- 고정 세션을 활용해서 해결할 수도 있는데, 확장 가능하지도 않고 유연하지도 않아서 좋지 않다.  


![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/f1d0625e-8add-4fcb-a8af-0b1c4cc9ab1a)  
- 더 나은 해결책은 레디스와 같은 중앙 집중형 데이터 저장소를 쓰는 것이다.   


성능 최적화  
- 성능 최적화는 시스템 설계 면접의 단골 주제다.   
- 지금까지 살펴본 설계는 두가지 지점에서 개선 가능하다.  
- 우선, 여러 데이터센터를 지원하는 문제는 처리율 제한 장치에 매우 중요한 문제다. 데이터센터에서 멀리 떨어진 사용자를 지원하려다보면 latency가 증가할수 밖에 없기 때문이다.
  - 대부분의 클라우드 사업자는 세계 곳곳에 edge server를 심어놓고, 사용자 트래픽을 가장 가까운 edge server로 전달하여 지연시간을 줄인다.
- 두번째로 고려할 것은 rate limit 장치 간에 데이터를 동기화 할 때 최종 일관성 모델을 사용하는 것이다.


모니터링  
처리율 제한 장치를 설치한 이후에는 효과적으로 동작하고 있는지 보기 위해 데이터를 모아야한다. 기본적으로 모니터링을 통해 확인하는것은 다음과 같다
- 채택된 처리율 제한 알고리즘이 효과적이다.
- 정의한 처리율 제한 규칙이 효과적이다.

예를 들어 처리율 제한 규칙이 너무 빡빡하면 많은 유효 요청이 처리되지 못하고 버려질 것이다. 그런 일이 벌어진다면 규칙을 완화할 필요가 있다.  
깜짝 세일 같은 이벤트 때문에 트래픽이 급증할 때 처리율 제한 장치가 비효율적으로 동작하면, 그런 트래픽 패턴을 잘 처리할 수 있도록 알고리즘을 바꾸는것을 생각해보자.  
그런 상황에서는 토큰 버킷이 적합할 것이다.  

### 4단계: 마무리  
시스템 설계 문제와 마찬가지로, 시간이 허락한다면 다음과 같은 부분을 언급해보면 도움 될 것이다.  
- 경성(hard) 또는 연성(soft) 처리율 제한  
  - 경성 처리율 제한: 요청의 개수는 임계치를 절대 넘어설 수 없다.
  - 연성 처리율 제한: 요청 개수는 잠시 동안은 임계치를 넘어설 수 있다.
- 다양한 계층에서의 처리율 제한
  - 이번 장에서는 애플리케이션 계층에서의 처리율 제한에 대해서만 알아보았다.
  - 하지만, 다른 계층에서도 처리율 제한이 가능하다.
  - 예를 들어, Iptables를 사용하면 IP 주소(IP는 OSI 기준으로 3번 계층인 네트워크 계층)에 처리율 제한을 적용하는 것이 가능하다.
- 처리율 제한을 회피하는 방법. 클라이언트를 어떻게 설계하는 것이 최선인가?  
  - 클라이언트 측 캐시를 사용하여 API 호출 횟수를 줄인다.
  - 처리율 제한의 임계치를 이해하고, 짧은 시간 동안 너무 많은 메시지를 보내지 않도록 한다.
  - 예외나 에러를 처리하는 코드를 도입하여 클라이언트가 예외적 상황으로부터 gracefully(우아하게) 복구될 수 있도록 한다.
  - 재시도(retry) 로직을 구현할 때는 충분한 백오프(back-off) 시간을 둔다.

## 5. 안정 해시 설계 

- 슈평적 규모 확장성을 달성하기 위해서는 요청 또는 데이터를 서버에 균등하게 나누는 것이 중요하다.
- 안정 해시는 이 목표를 달성하기 위해 보편적으로 사용하는 기술이다.

### 해시 키 재배치 문제  

- N개의 캐시 서버들에 부하를 균등하게 나누는 보편적 방법은 아래의 해시 함수를 사용하는 것이다. 
  - `serverIndex=hash(key) % N 
- server pool의 크기가 고정되어 있을 때, 그리고 데이터 분포가 균등할 때는 잘 동작한다.
- 하지만 서버가 추가되거나 기존 서버가 삭제되면 문제가 생긴다. (서버가 중단되어서 서버풀의 크기를 1개를 줄이면 서버 인덱스가 변한다.)
- 장애가 발생한 서버에 보관되어 있는 키 뿐만 아니라 대부분의 키가 재분배되고 그 결과 대규모 캐시 미스가 발생하게 된다.
- 안정 해시는 이 문제를 효과적으로 해결하는 기술이다.

### 안정 해시  
- 안정해시: 해시 테이블 크기가 조정될 때 평균적으로 k/n개의 키만 재배치하는 해시 기술 (k = 키의 개수, n 슬롯 개수)
- 대부분의 전통 해시 테이블: 슬롯의 수가 바뀌면 거의 대부분 키를 재배치한다. 

**해시 공간과 해시 링**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/7b2badc9-d9ea-4b48-8505-d02a859a09c9)  
- 해시 함수 f로 SHA-1을 사용한다고 하고
- SHA-1 해시 공간 범위는 0 ~ 2^160 - 1 이므로 x0 ~ xn 값은 해당 범위이다.
- 해시 공간의 양쪽을 구부려 접으면 해시 링이 만들어진다. 


**해시 서버**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/2922f162-dabe-438d-88f9-80036f6c55a1)  
- 이 해시 함수 f를 사용하면 서ㅓㅂ IP나 이름을 이 링 위에 대응시킬 수 있다. 
- 위 그림은 4개의 서버를 해시 링 위에 배치한 결과다.

**해시 키**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/96d5cebc-bd30-4c1e-9bec-e952d7d34c98)  
- 아까 위의 "해시 키 재배치 문제" 에 언급된 함수와는 다른 함수다 (% 연산으로 하는게 아님)
- 위 그림은 캐시할 키 key0 ~ key3 도 해시 링에 배치한 결과이다.

**서버 조회**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/0711478e-97a2-4a07-9de0-1a3d93c79221)  
- 어떤 키가 저장되는 서버는, 해당 키의 위치로부터 시계 방향으로 링을 탐색 해나가면서 만나는 첫번째 서버다.

**서버 추가**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/91090716-34ca-4dd3-b521-a5572a4ed43c)  
- 서버를 추가하더라도 키 가운데 일부만 재배치하면 된다.  
- 위 그림은 서버 4가 추가된 뒤에 key0만 재배치됐다.

**서버 제거**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/cc84117d-4fe7-49aa-9cf6-ef3285db1395)  
- 하나의 서ㅓㅂ가 제거되면 키 가운데 일부만 재배치된다.
- 위 그림은 서버 1이 삭제되었을 때 key1만 서버2로 재배치된 것을 보여준다. 

**기본 구현법의 두 가지 문제**  
- 안정 해시 알고리즘의 기본 절차는 다음과 같다.
  - 서버와 키를 균등 분포 해시 함수를 사용해 해시 링에 배치한다.
  - 키의 위치에서 링을 시계빵향으로 탐색하다 만나는 최초의 서버가 키가 저장될 서버다. 

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/f8f85990-9d67-4e67-9ced-8706f5b9edb0)  
- 이 접근법에는 두가지 문제가 있다. 
- 1) 서버가 추가되거나 삭제되는 상황을 감안하면 파티션의 크기를 균등하게 유지하는 게 불가능하다. (파티션 == 인접한 서버 사이의 해시 공간) 어떤 서버는 굉장히 작은 해시 공간, 어떤 서버는 굉장히  큰 해시 공간을 할당 받는다. 
- 위 그림은 s1이 삭제되는 바람에 s2의 파티션이 다른 파티션 대비 거의 두 배로 커지는 상황을 보여준다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/0681d2f3-b365-4751-b566-f9dfd4f8fe6b)  
- 2) 키의 균등 분포를 달성하기가 어렵다. 
- 위 그림은 서버1과 서버3은 아무 데이터도 갖지 않는 반면, 대부분의 키는 서버2에 보관될 것이다. 
- 이 문제를 해결하기 위해 제안된 기법이 가상노드 또는 복제라 불리는 기법이다.


**가상 노드**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/a8e9991b-0397-425a-a174-49a08010fb10)  
- 가상 노드는 실제 노드 또는 서버를 가리키는 노드로서, 하나의 서버는 링 위에 여러개의 가상 노드를 가질 수 있다.
- 위 그림을 보면 서버0와 서버1은 3개의 가상 노드를 갖는다. (숫자 3은 임의의 숫자로 정한 것, 실제 시스템에서는 훨씬 큰값을 사용한다)
- 서버 0을 링에 배치하기 위해 s0 하나만 쓰는 대신, s0_0, s0_1, s0_2의 세 개 가상 노드를 사용한다. (서버 1도 마찬가지)
- 각 서버는 여러개의 파티션을 관리해야 한다.   

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/ddd4f3bc-2cb9-4fb9-9825-a7c8030d11da)  
- 키의 위치로부터 시계방향으로 링을 탐색하다 만나는 최초의 가상 노드가 해당 키가 저장될 서버가 된다.  
- k0가 저장되는 서버는 k0의 위치로부터 링을 시계방향으로 탐색하다 만나는 최초의 가상 노드 s1_1 (서버1)이다.
- 가상 노드의 개수를 늘릴수록 키의 분포는 점점 더 균등해진다.
- 표준 편차가 작아져서 데이터가 고르게 분포되기 떄문이다. (표준 편차 == 데이턱 어떻게 퍼져 나갔는지를 보이는 척도)
- 100~200개의 가상 노드 => 표준 편차 평균 10% ~ 5%
- 가상 노드를 늘릴수록 표준 편차의 값은 떨어지지만, 반대로 가상 노드 데이터를 저장할 공간은 더 많이 필요하다 (trade off)
- 시스템 요구사항에 맞도록 가상 노드 개수를 적절히 조정해야 한다.

**재배치할 키 결정**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/1dc97586-c8dd-42ef-bcdd-9153f93a568b)  
- 서버가 추가되거나 제거되면 데이터 일부는 재배치해야한다. 어느 범위의 키들이 재배치되어야 할까?
- 위의 그림은 서버4가 추가되고, s4부터 그 반시계 방향에 있는 첫번째 서버 s3까지 영향을 받았다.
- 즉 s3부터 s4사이에 있는 키들을 s4로 재배치해야한다. 

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/d2a44618-a1b0-43ac-9e00-4ecbcd4bb35a)  
- 서버 s1이 삭제되면 s1부터 그 반시계 방향에 있는 최초 서버 s0 사이에 있는 키들이 s2로 재배치되어야한다.  

### 마치며  
안정 해시의 이점
- 서버가 추가되거나 삭제될 대 재배치되는 키의 수가 최소화 
- 데이터가 균등하게 분포하게 되므로 수평적 규모 확장성을 달성하기 쉽다
- 핫스팟 키 문제를 줄인다. 
  - 특정한 샤드에 대한 접근이 지나치게 빈번하면 서버 과부하 문제가 생길 수 있다.
  - 케이티 페리, 저스틴 비버, 레이디 가가 같은 유명인의 데이터가 전부 같은 샤드에 몰리는 상황이 이러한 것이다.
  - 안정 해시는 데이터를 좀 더 균등하게 분배하므로 위의 문제가 생길 가능성을 줄인다.
- 안정 해시 사용 예시
  - 아마존 다이나모 DB의 파티셔닝 관련 컴포넌트
  - 아파치 카산드라 클러스터에서의 데이터 파티셔닝
  - 디스코드 채팅 어플리케이션
  - 아카마이 CDN
  - 매그레프 네트워크 부하 분산기 


## 6. 키-값 저장소 설계
key-value store는 key-value db라고도 불리는 non-relational db이다.  
저장소에 저장되는 값은 고유 식별자를 키로 가져야 한다.  
키와 값 사이의 이런 연결 관계를 "키-값" 쌍이라고 지칭한다.

키  
- 값은 키를 통해서만 접근할 수 있다.  
- 키는 일반 텍스트일 수도 있고, 해시 값일 수도 있다.  
- 성능상 키는 짧을수록 좋다.

값  
- 값은 문자열, 리스트, 객체 등이 될 수 있고 무엇이든 될 수 있다.

key-value DB예시로는 ex) 아마존 다이나모, memcached, 레디스 


### 문제 이해 및 설계 범위 확정  
- 완벽한 설계란 없다.  
- 읽기, 쓰기 그리고 메모리 사용량 사이에 어떤 균형을 찾고, 데이터의 일관성과 가용성 사이에서 타협적 결정을 내린 설계를 만드는게 좋다.  
- 이번장에서는 다음 특성을 갖는 key-value store를 설계해보자. 
  - 키-값 쌍의 크기는 10KB 이하
  - 큰 데이터를 저장할 수 있어야 한다.
  - 높은 가용성을 제공해야 함. 따라서 시스템은 장애가 있더라도 빨리 응답해야 한다.
  - 높은 규모 확장성을 제공해야 함. 따라서 트래픽 양에 따라 자동으로 서버 증설/삭제가 이루어져야 한다
  - 데이터 일관성 수준은 조정이 가능해야 한다
  - 응답 지연시간(latency)이 짧아야 한다.

### 단일 서버 키-값 저장소   
- 한 대 서버만 사용하는 키-값 저장소를 설계하는 것은 쉽다.  
- 가장 직관적인 방법은 키-값 쌍 전부를 메모리에 해시 테이블로 저장하는 것
- 그러나 빠른속도를 보장하지만, 모든 데이터를 메모리 안에 두는 것이 불가능 할 수 있다는 약점이 있다. 
- 이 문제를 해결하기 위한 개선책은 
  - 데이터 압축
  - 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장 
- 그러나 이렇게 개선한다고 해도, 한 대 서버로 부족한 때가 찾아온다. 
- 많은 데이터를 저장하려면 분산 키-값 저장소를 만들 필요가 있다.

### 분산 키-값 저장소  
- 분산 해시 테이블이라고도 불린다.
- 키-값 쌍을 여러 서버에 분산시킨다.  
- 분산 시스템을 설계할 때는 CAP 정리를 이해하고 있어야 한다. 

**CAP 정리**  
- 데이터 일관성(consistency), 가용성(availability), 파티션 감내(partition tolerance) 라는 세 가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능하다는 정리
- 각 요구사항의 의미는
  - 데이터 일관성: 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속했느냐에 관계없이 언제나 같은 데이터를 보아야 함.
  - 가용성: 분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생해도 항상 응답을 받아야 한다.
  - 파티션 감내: 파티션은 두 노드 사이에 통신 장애가 발생하였음을 의미하고, 파티션 감내는 네트워크에 파티션이 생기더라도 시스템은 계속 동작해야 한다는 뜻
- CAP 정리는 3가지중에 2가지를 충족하려면 나머지 하나는 반드시 희생되어야 한다는 의미이다.
- 키-값 저장소는 어떤 2가지를 만족하느냐에 따라 다음과 같이 분류 된다.
  - CP 시스템: 가용성을 희생
  - AP 시스템: 데이터 일관성을 희생
  - CA 시스템: 파티션 감내를 희생. 그러나 보통 네트워크 장애는 피할 수 없는 일로 여겨져서, 실세계에 CA 시스템은 존재하지 않는다.
  

구체적인 사례를 살펴보자.  
분산 시스템에서 데이터는 보통 여러 노드에 복제되어 보관된다.  


![image](https://github.com/meokgu-skku/be/assets/28394879/ab2bb678-55fd-4f08-ab14-0fba438cdaae)  
- 위의 그림처럼 세 대의 복제(replica) 노드 n1,n2,n3에 데이터를 복제하여 보관하는 상황을 가정해보자. 

**이상적 상태**  
- 이상적 환경이면 네트워큭가 파티션되는 상황은 절대로 일어나지 않는다.
- n1에 기록된 데이터는 자동으로 n2, n3에 복제되므로 일관성과 가용성도 만족한다.  

**실세계의 분산 시스템**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/602fd61e-3548-47c8-b185-a2f09236d76d)
- 분산 시스템은 파티션 문제를 피할 수 없고, 파티션 문제가 발생하면 일관성과 가용성 사이에서 하나를 선택해야 한다.
- 위 그림은 n3에 장애가 발생하여 n1 및 n2와 통신할 수 없는 상황을 보여준 것이다.
- 클라이언트가 n1 또는 n2에 기록한 데이터는 n3에 전달되지 않는다.
- n3에 기록되었으나 아직 n1 및 n2로 전달되지 않은 데이터가 있다면 n1, n2는 오래된 사본을 갖고 있을 것이다.
- 가용성 대신 일관성을 선택하면, 불일치 문제를 피하기 위해 n1, n2에 대해 쓰기 연산을 중단시켜야한다. 즉, 가용성이 깨진다.
  - 은행권 시스템은 보통 데이터 일관성을 양보하지 않는다. ex) 온라인 뱅킹 시스템이 계좌 최신 정보를 출력하지 못하면 큰 문제다.
  - 파티션때문에 일관성이 깨질 수 있는 상황이 발생하면 해결되기전 까지 오류를 반환해야 한다.
- 일관성 대신 가용성을 선택하면, 낡은 데이터를 반환할 위험이 있더라도 계속 읽기 연산을 허용해야 한다.
  - n1, n2는 계속 쓰기 연산을 허용할 것이고, 파티션 문제가 해결된 뒤에 새 데이터를 n3에 전송한다.
- 분산 키-값 저장소를 만들 때는 그 요구사항에 맞도록 CAP 정리를 적용해야 한다.
- 면접을 볼 떄에는 면접관과 상의하고, 그 결론에 따라 시스템을 설계 하자.

**시스템 컴포넌트**  
키-값 저장소 구현에 사용될 핵심 컴포넌트들 및 기술들을 살펴보자.  
- 데이터 파티션
- 데이터 다중화(replication)
- 일관성(consistency)
- 일관성 불일치 해소(inconsistency resolution)
- 장애 처리
- 시스템 아키텍처 다이어그램
- 쓰기 경로
- 읽기 경로 

**데이터 파티션**  
대규모 애플리케이션의 경우 전체 데이터를 한 대 서버에 욱여넣는 것은 불가능하다.  
가장 단순한 해결책은 데이터를 작은 파티션들로 분할해서 여러 대 서버에 저장하는 것이다.  
데이터를 파티션 단위로 나눌 때는 다음 두가지 문제를 중요하게 따져야 한다.  
- 데이터를 여러 서버에 고르게 분산할 수 있는가
- 노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가 
5장에서 다룬 안정해시로 이런문제를 풀기에 적합하다.  
안정 해시를 사용하여 데이터를 파티션하면 좋은점은 다음과 같다.  
- 규모 확장 자동화(automatic scaling): 시스템 부하에 따라 서버가 자동으로 추가되거나 삭제되도록 만들 수 있다.  (궁금한 것: 서버 추가되면 데이터는 어떻게 넣을까?)
- 다양성(heterogeneity): 각 서버의 용량에 맞게 가상 노드의 수를 조정할 수 있다. 다시 말해, 고성능 서버는 더 많은 가상 노드를 갖도록 설정할 수 있다.  

**데이터 다중화**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/ecc11ff6-0b97-4b1e-955c-80f8c175461e)  
- 높은 가용성과 안정성을 확보하기 위해서는 데이터를 N개 서버에 비동기적으로 다중화할 필요가 있다.  
- 여기서 N은 튜닝 가능한 값이다.
- N개를 선정하는 방법은
  - 어떤 키를 해시 링 위에 배치한 후, 그 지점으로부터 시계 방향으로 링을 순회하면서 만나는 첫 N개 서버에 데이터 사본을 보관한다.
  - 위 그림은 N=3으로 설정하여 key0는 s1,s2,s3에 저장된다.
- 그런데 가상 노드를 사용한다면 위와 같이 선택한 N개의 노드가 대응될 실제 물리 서버의 개수가 N보다 작아질 수 있다.  
  - 이 문제를 피하려면 노드를 선택할 때 같은 물리 서버를 중복 선택하지 않도록 해야 한다.
- 같은 데이터 센터에 속한 노드는 정전, 네트워크 이슈, 자연재해 등의 문제를 동시에 겪을 가능성이 있다.  
  - 따라서 안정성을 담보하기 위해 데이터의 사본은 다른 센터의 서버에 보관하고, 센터들은 고속 네트워크로 연결한다.

**데이터 일관성**  
여러 노드에 다중화된 데이터는 적절히 동기화가 되어야 한다.  
정족수 합의(Quorum Consensus) 프로토콜을 사용하면 읽기/쓰기 연산 모두에 일관성을 보장할 수 있다.  
관계된 정의부터 몇 가지 살펴보자.  
- N=사본 개수  
- W=쓰기 연산에 대한 정족수. 쓰기 연산이 성공한 것으로 간주되려면 적어도 W개의 서버로 부터 쓰기 연산이 성공했다는 응답을 받아야 함.
- R=읽기 연산에 대한 정족수. 읽기 연산이 성공한 것으로 간주되려면 적어도 R개의 서버로부터 응답을 받아야 함.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/cef4026f-cab3-4213-8471-ff3d482d0b37)  
- 위의 그림은 N=3인 경우에 대한 예제.  
- W=1은 데이터가 한 대 서버에만 기록된다는 것은 아니고, 중재자가 최소 한 대 서버로부터 쓰기 성공 응답을 받아야 한다는 의미이다.  
  - 따라서 s3로부터 성공 응답을 받았다면, s0, s2로부터의 응답을기다릴 필요가 없다. 
  - 중재자는 클라이언트와 노드 사이에서 proxy 역할을 한다.
- W, R, N의 값을 정하는 것은 응답 지연과 데이터 일관성 사이의 타협점을 찾는 전형적인 과정이다. 
- W = 1 또는 R =1 인 구성의 경우 응답속도는 빠를 것이다.
- W 또는 R 값이 1보다 큰 경우에는 일관성은 증가하지만, 가장 느린 서버로부터의 응답을 기다려야 하므로 느려진다.  
- W+R > N 인 경우에는 강한 일관성이 보장된다. 
  - 일관성성을 보증할 최신 데이터를 가진 노드가 최소 하나는 겹칠 것이기 때문이다.  
- 면접 시에는 N,W,R 값을 어떻게 정해야 할까? 다음에 가능한 몇가지 구성을 제시했다.
  - R = 1, W = N: 빠른 읽기 연산에 최적화된 시스템
  - W = 1, R = N: 빠른 쓰기 연산에 최적화된 시스템
  - W + R > N: 강한 일관성이 보장된 (보통 N = 3, W=R=2)
  - W + R <= N: 강한 일관성이 보장되지 않음 

**일관성 모델**  
- 일관성 모델은 키-값 저장소를 설계할 때 고려해야 할 또 하나의 중요한 요소다.  
- 일관성 모델은 데이터 일관성의 수준을 결정하는데, 종류가 다양하다.  
  - 강한 일관성(strong consistency): 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환한다. 즉, 클라이언트는 절대로 낡은 데이터를 보지 못한다.
  - 약한 일관성(weak consistency): 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 수 있다.  
  - 최종 일관성(eventual consistency): 약한 일관성의 한 형태로, 갱신 결과가 결국에는 모든 사본에 반영(즉, 동기화)되는 모델이다
- 강한 일관성을 달성하는 일반적인 방법은, 모든 사본에 현재 쓰기 연산의 결과가 반영될 떄까지 해당 데이터에 대한 읽기 쓰기를 금지하는 것이다.
  - 이 방법은 고가용성 시스템에는 적합하지 않다. 
- 다이나모, 카산드라 같은 저장소는 최종 일관성 모델을 택하고 있는데, 이번 장에서도 그 모델에 맞게 키-값 저장소를 설계할 것이다.
- 최종 일관성 모델을 따를 경우 쓰기 연산이 병렬적으로 발생하면 시스템에 저장된 값의 일관성이 깨어질 수 있는데, 이 문제는 클라이언트가 해결해야 한다.  
  - 클라이언트측에서 데이터의 버전 정보를 활용해 일관성이 깨진 데이터를 읽지 않도록 하는기법에 대해서 아래에서 살펴보자. 

**비 일관성 해소 기법:데이터 버저닝**  
- 데이터를 다중화하면 가용성은 높아지지만 사본 간 일관성이 깨질 가능성이 높아진다.  
- 버저닝(versioning)과 벡터 시계(vector clock)는 그 문제를 해소하기 위해 등장한 기술이다.  
- 버저닝은 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 만드는것을 의미한다.  
- 따라서 각 버전의 데이터는 변경 불가능하다.
- 버저닝에 대해 알아보기 전에 우선 데이터 일관성이 깨지는지 예제를 통해 알아보자.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/78bc4f8a-5f4f-4ef6-bc6e-8fa8c2226d5f)  
- 위 그림과 같이 어떤 데이터의 사본이 노드 n1과 n2에 보관되어 있다고 하자.  
- 데이터를 가져오려는 서버1과 서버2는 get("name") 연산의 결과로 같은 값을 얻는다.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/16760a67-1c6c-4195-8421-419cea4a2e45)  
- 서버 1은 johnSanFranciso로 바꾸고, 서버 2는 jhonNewYork로 바꾸는데 이 두 연산은 동시에 이뤄진다고 해보자.
- 이제 우리는 충돌(conflict)하는 두 값을 갖게 되었다. 그리고 각각 버전을 v1, v2라고 하자.  
- 변경이 이뤄진 후에, 원래 값은 무시할 수 있어도 v1, v2 사이의 충돌은 해소하기 어려워 보인다.  
- 이 문제를 해결하려면, 충돌을 발견하고 자동으로 핵려해 낼 버저닝 시스템이 필요하다.  
- 벡터 시계(vector clock)는 이런 문제를 푸는데 보편적으로 사용되는 기술이다. 

지금부터 그 동작 원리를 살펴보자.  
- 벡터 시계는 [서버, 버전]의 순서쌍을 데이터에 매단 것이다.  
- 어떤 버전이 선행 버전인지, 후행 버전인지, 아니면 다른 버전과 충돌이 있는지 판별하는데 쓰인다.  
- 벡터 시계는 D([S1,v1], [S2,v2], ..., [Sn, vn])와 같이 표현한다고 가정하자. 
- D는 데이터이고, vi는 버전 카운터, Si는 서버 번호이다.  
- 만일 데이터 D를 서버 Si에 기록하면, 시스템은 아래 작업 가운데 하나를 수행해야 한다.  
  - [Si, vi] 가 있으면 vi를 증가시킨다
  - 그렇지 않으면 새 항목 [Si, 1]를 만든다.


![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/f4b42f95-43a5-4d62-b595-b130afc8dc7b)  
1. 클라이언트가 데이터 D1을 시스템에 기록한다. 이 쓰기 연산을 처리한 서버는 Sx이다. 따라서 벡터 시계는 D1[(Sx, 1)]으로 변한다.
2. 다른 클라이언트가 데이터 D1을 읽고 D2로 업데이트한 다음 기록한다. D2는 D1에 대한 변경이므로 D1을 덮어쓴다. 이떄 쓰기 연산은 같은 서버 Sx가 처리한다고 가정하자. 벡터 시계는 D2([Sx, 2])로 바뀔 것이다. 
3. 다른 클라이언트가 D2를 읽어 D3로 갱신한 다음 기록한다. 이 쓰기 연산은 Sy가 처리한다고 가정하자. 벡터 시계 상태는 D3([Sx,2],[Sy,1])로 바뀐다.
4. 또 다른 클라이언트가 D2를 읽고 D4로 갱신한 다음 기록한다. 이때 쓰기 연산은 서버 Sz가 처리한다고 가정하자. 벡터 시계는 D4([Sx,2],[Sz,1])일 것이다.
5. 어떤 클라이언트가 D3, D4를 읽으면 데이터 간 충돌이 있다는것을 알게 된다. 이 충돌은 클라이언트가 해소한 후에 서버에 기록한다. 이 쓰기 연산을 처리한 서버는 Sx였다고 하자. 벡터 시계는 D5([Sx,3],[Sy,1],[Sz,1])로 바뀐다. 충돌이 일어났다는 것을 어떻게 감지하는지는 잠시 후에 자세히 살펴보자.  


- 벡터 시계를 사용하면 어떤 버전 X가 버전 Y의 이전 버전인지(따라서 충돌이 없는지) 쉽게 판단할 수 있다.  
- 버전 Y에 포함된 모든 구성요소의 값이 X에 포함된 모든 구성요소 값보다 같거나 큰지만 보면 된다.
  - ex) 벡터 시계 D([s0,1],[s1,1]) 은 D([s0,1],[s1,2])의 이전 버전이고, 충돌은 없다.
- 어떤 버전 X와 Y사이에 충돌이 있는지 보려면 Y의 벡터 시계 구성요소 가운데 X의 벡터 시계 동일 서버 구성요소보다 작은 값을 갖는 것이 있는지 보면된다.
  - ex) D([s0,1],[s1,2]) 와 D([s0,2],[s1,1])는 서로 충돌한다. 
- 그러나 벡터 시계를 사용해 충돌을 감지하고 해소하는 방법에는 두 가지 단점이 있다. 
  - 1) 충돌 감지 및 해소 로직이 클라이언트에 들어가므로, 클라 구현이 복잡해진다.
  - 2) [서버:버전]의 순서쌍 개수가 굉장히 빨리 늘어난다.  
    - 이 문제를 해결하려면 그 길이에 어떤 임계치를 설정하고, 임계치 이상으로 길어지면 오래된 순서쌍을 제거하도록 해야 하는데 
    - 이렇게하면 버전 간 선후 관계가 정확하게 결정될 수 없기 때문에 충돌 해소 과정의 효율성이 낮아진다.
    - 하지만 다이나모 데이터베이스에 관계된 문헌에 따르면 아마존은 실제 서비스에서 그런 문제가 벌어지는 것을 발견한 적이 없다고 한다.
    - 그러니 대부분의 기업에서 벡터 시계는 적용해도 괜찮은 솔루션일 것이다. 

**장애 처리**  
- 대다수 대규모 시스템에서 장애는 아주 흔한 사건이다.
- 따라서 장애를 어떻게 처리할 것인지는 굉장히 중요한 문제다.
- 이번 절에서 장애 감지 기법들을 먼저 살펴보고, 그다음으로 장애 해소 전략들을 짚어보자.

**장애 감지**  
- 분산 시스템에서는 한 대 서버가 A서버가 죽어도 바로 A서버를 장애처리 하지 않는다. 
- 보통 두 대 이상의 서버가 똑같이 서버 A의 장애를 보고해야 장애가 발생했다고 간주한다.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/7290d843-0921-4e78-9ab3-b31fb100ca5f)  
- 위 그림과 같이 모든 노드 사이에 멀티캐스팅 채널을 구축하는 것이 서버 장애를 감지하는 가장 손쉬운 방법이다.
- 하지만 이 방법은 서버가 많을 때 비효율적이다.



가십 프로토콜(gossip protocol) 같은 분산형 장애 감지 (decentralized failure detection) 솔루션을 새택하는 편이 보다 효율적이다.  
가십 프로토콜의 동작 원리는 다음과 같다.  
- 각 노드는 멤버십 목록(membership list)를 유지한다. 멤버십 목록은 각 멤버 ID와 그 박동 카운터(heartbeat counter) 쌍의 목록이다.  
- 각 노드는 주기적으로 자신의 박동 카운터를 증가시킨다.
- 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보낸다.
- 박동 카운터 목록을 받은 노드는 멤버십 목록을 최신 값으로 갱신한다.
- 어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 해당 멤버는 장애(offline) 상태인 것으로 간주한다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/9451ad34-86f5-4307-96a6-c23f4c9c46fa)  
- 위의 그림은 가십 프로토콜의 예시이다.  
- 노드 s0은 그림 좌측의 테이블과 같은 멤버십 목록을 가진 상태다.
- 노드 s0은 노드 s2의 박동 카운터가 오랫동안 증가되지 않았다는 것을 발견한다.
- 노드 s0은 노드 s2를 포함하는 박동 카운터 목록을 무작위로 선택된 다른 노드에게 전달한다 
- 노드 s3의 박동 카운터가 오랫동안 증가되지 않았음을 발견한 모든 노드는 해당 노드를 장애 노드로 표시한다.


**일시적 장애 처리**  
- 가십 프로토콜로 장애를 감지한 시스템은 가용성을 보장하기 위해 필요한 조치를 해야 한다. 
  - 엄격한 정족수(strict quorum) 접근법을 쓴다면, 읽기와 쓰기 연산을 금지해야 한다.
- 느슨한 정족수(sloppy quorum) 접근법은 이 조건을 완화하여 가용성을 높인다.
  - 정족수 요구사항을 강제하는 대신, 쓰기 연산을 수행할 W개의 건강한 서버와 읽기 연산을 수행할 R개의 건강한 서버를 해시 링에서 고른다. (장애 상태 서버는 무시)
- 네트워크나 서버 문제로 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리한다. 
- 그동안 발생한 변경사항은 해당 서버가 복구되었을 때 일괄 반영하여 데이터 일관성을 보존한다. 
- 이를 위해 임시로 쓰기 연산을 처리한 서버에는 그에 관한 단서(hint)를 남겨둔다.  
- 따라서 이런 장애 처리 방안을 단서 후 임시 위탁(hinted handoff) 기법이라 부른다.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/45cb6391-e3db-4252-b320-3e5b287165b5)  
- 그림 예제를 보자.  
- 장애 상태인 노드 s3에 대한 읽기 및 쓰기 연산은 일시적으로 노드 s3가 처리한다.
- s2가 복구되면, s3는 갱신된 데이터를 s2로 인계할 것이다.


**영구 장애 처리**  
- 단서 후 임시 위탁 기법은 일시적 장애를 처리하기 위한 것이다. 영구적인 노드의 장애 상태는 어떻게 처리해야 할까? 
- 반-엔트로피(anti-entropy) 프로토콜을 구현하여 사본들을 동기화해보자.
- 반-엔트로피 프로토콜은 사본들을 비교하여 최신 버전으로 갱신하는 과정을 포함한다.
- 사본 간의 일관성이 망가진 상태를 탐지하고 전송 데이터의 양을 줄이기 위해서는 머클(Merkle) 트리를 사용할 것이다.
  - 해시트리라고도 불리는 머클트리는 각 노드에 그 자식 노드들에 보관된 값의 해시, 또는 자식 노드들의 레이블로부터 계산된 해시값을 레이블로 붙여두는 트리다. 
  - 해시 트리를 사용하면 대규모 자료 구조의 내용을 효과적이면서도 보안상 안전한 방법으로 검증할 수 있다.
- 키 공간(key space)가 1부터 12까지일 때 머클 트리를 만드는 예제를 한번 살펴보자. 일관성이 망가진 데이터가 위치한 상자는 다른 색으로 표시해두었다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/2041a0ab-3ae3-4e4c-b3cb-9bc6dda05ffe)  
- 1단계: 키 공간을 위와 같이 버킷으로 나눈다 (예제에선 4개의 버킷)

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/59433646-d793-45b3-a48f-793677623349)  
- 2단계: 버킷에 포함된 각각의 키에 균등 분포 해시(uniform hash) 함수를 적용하여 해시 값을 계산한다. 

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/5ac84796-4604-43e2-94ca-148a81facae1)  
- 3단계: 버킷별로 해시값을 계싼한 후, 해당 해시 값을 레이블로 갖는 노드를 만든다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/cf202c9e-f5ce-4ed3-aa5b-bf7db42ae7a5)  
- 4단계: 자식 노드의 레이블로부터 새로운 해시 값을 계싼하여, 이진 트리를 상향식으로 구성해 나간다.


- 이 두 머클 트리의 비교는 루트(root) 노드의 해시값을 비교하는 것으로 시작한다. 
- 루트 노드의 해시 값일 일치한다면 두 서버는 같은 데이터를 갖는 것이다.
- 그 값이 다른 경우에는 왼쪽 자식 노드의 해시 값을 비교하고, 그다음으로 오른쪽 자식 노드의 해시 값을 비교한다.
- 이렇게 하면서 아래쪽으로 탐색해 나가다 보면 다른 데이터를 갖는 버킷을 찾을 수 있으므로, 그 버킷들만 동기화하면 된다.  
- 머클 트리를 사용하면 동기화해야 하는 데이터의 양은 실제로 존재하는 차이의 크기에 비례할 뿐, 두 서버에 보관된 데이터의 총량과는 무관해진다.  
- 하지만 실제로 쓰이는 시스템의 경우 버킷 하나의 크기가 꽤 크다는 것은 알아 두어야 한다.  
- 가능한 구성 가운데 하나를 예로 들면 10(1B)개의 키를 백만 (1M)개의 버킷으로 관리하는 것인데, 그 경우 하나의 버킷은 1,000개 키를 관리하게 된다.

궁금한점: 데이터가 추가만 된다면 유효한데, 삭제나 업데이트는 어떻게 감지할까? -> 타임스탬프를 도입하거나 상태 플래그와 같은 추가적인 메커니즘을 사용하면 해결 가능


데이터 센터 장애 처리  
- 데이터 센터 장애는 정전, 네트어워크 장애, 자연재해 등 다양한 이유로 발생할 수 있다.  
- 데이터 센터 장애에 대응할 수 있는 시스템을 만들려면 데이터를 여러 데이터 센터에 다중화하는 것이 중요하다.
- 한 데이터센터가 완전히 망가져도 사용자는 다른 데이터 센터에 보관된 데이터를 이용할 수 있다.


**시스템 아키텍처 다이어그램**  
이 아키텍처의 주된 기능은 다음과 같다.  
- 클라이언트는 키-값 저장소가 제공하는 두 가지 단순한 API, 즉 get(key) 및 put(key, value)와 통신한다. 
- 중재자(coordinator)는 클라이언트에게 키-값 저장소에 대한 프락시(proxy) 역할을 하는 노드다.
- 노드는 안정 해시(consistent hash)의 해시 링(hash ring) 위에 분포한다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/200e98a8-f16d-4bb3-af90-5bcc874ee7b4)  
- 노드를 자동으로 추가 또는 삭제할 수 있도록, 시스템은 완전히 분산된다(decentralized)
- 데이터는 여러 노드에 다중화된다.
- 모든 노드가 같은 책임을 지므로, SPOF는 존재하지 않는다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/92129f87-0acf-469f-8f07-ce05b86ef030)  
- 완전히 분산된 설계를 채택했으므로, 모든 노드는 위 그림에 제시된 기능 전부를 지원해야 한다.

**쓰기 경로**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/5be2da84-75cd-49ae-99ac-b422db041f86)  
- 위 그림은 쓰기 요청이 특정 노드에 전달되면 무슨일이 벌어지는지를 보여준다.  
- 이 그림에서 보인 구조는 기본적으로 카산드라의 사례를 참고한 것이다.  
  
1. 쓰기 요청이 커밋 로그(commit log) 파일에 기록된다.
2. 데이터가 메모리 캐시에 기록된다.
3. 메모리 캐시가 가득차거나 사전에 정의된 어떤 임계치에 도달하면 데이터는 디스크에 있는 SSTable에 기록된다. 
   - SSTable은 Sorted-String Table이고 
   - <키,값>의 순서쌍을 정렬된 리스트 형태로 관리하는 테이블이다. 

**읽기 경로**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/c58248ab-038a-4aaf-8b0c-adeb92f77e99)  
- 읽기 요청을 받은 노드는 데이터가 메모리 캐시에 있는지부터 살핀다.  
- 있는 경우에는 데이터를 클라이언트에게 반환한다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/5e472010-2f8a-431a-851e-86c9cc6e2c87)  
- 데이터가 메모리에 없는 경우에는 디스크에서 가져온다.
- 어느 SSTable에 찾는 키가 있는지 알아낼 효율적인 방법이 필요할 것이다. 
- 이런 문제를 푸는데는 블룸 필터(Bloom filter)가 흔히 사용된다.

1. 데이터가 메모리에 있는지 검사한다. 없으면 2로 
2. 데이터가 메모리에 없으므로 블룸 필터를 검사한다.
3. 블룸 필터를 통해 어떤 SSTable에 키가 보관되어 있는지 알아낸다.
4. SSTable에서 데이터를 가져온다
5. 해당 데이터를 클라이언트에 반환한다.


### 요약  
- 기억을 되살리는 차원에서 분산 키-값 저장소가 가져야 하는 기능과 그 기능 구현에 이용되는 기술을 정리해보자.

| 목표/문제 | 기술 |
| --- | --- |
| 대규모 데이터 저장 | 안정 해시를 사용해 서버들에 부하 분산 |
| 읽기 연산에 대해 높은 가용성 보장 | 데이터를 여러 데이터센터에 다중화 |
| 쓰기 연산에 대한 높은 가용성 보장 | 버저닝 및 벡터 시계를 사용한 충돌 해소 |
| 데이터 파티션 | 안정 해시 | 
| 점진적 규모 확장성 | 안정 해시 |
| 다양성(heterogeneity) | 안정 해시 |
| 조절 가능한 데이터 일관성 | 정족수 합의(quorum consensus) | 
| 일시적 장애 처리 | 느슨한 정족수 프로토콜(sloppy quorum)과 단서 후 임시 위탁(hinted handoff) |
| 영구적 장애 처리 | 머클 트리(Merkle tree) |
| 데이터 센터 장애 대응 | 여러 데이터 센터에 걸친 데이터 다중화 |


## 7. 분산 시스템을 위한 유일 ID 생성기 설계 

- auto_increment 속성이 설정된 관계형 DB의 기본키를 쓰면 되지 않을까? 라고 생각할수도 있지만
- 분산 환경에서 이 접근법은 통하지 않는다. 
  - 데이터베이스 서버 한대로는 그 요구를 감당할 수 없고
  - 여러 데이터베이스 서버를 쓰는 경우에는 지연 시간(delay)을 낮추기가 무척 힘들다.

유일성이 보장되는 ID의 몇 가지 예를 보자


### 1단계: 문제 이해 및 설계 범위 확정 
- 시스템 설계 면접 문제를 푸는 첫 단계는 적절한 질문을 통해 모호함을 없애고 설계 방향을 정하는 것이다.  
- 다음은 면접관과 지원자 사이에 오갈 수 있는 질문과 답변의 예시다. 

지원자: ID는 어떤 특성을 갖나요?  
면접관: ID는 유일해야 하고, 정렬 가능해야 합니다.  
지원자: 새로운 레코드에 붙일 ID는 항상 1만큼 큰 값이어야 하나요?  
면접관: ID의 값은 시간이 흐름에 따라 커질 테지만 언제나 1씩 증가한다고 할 수는 없습니다. 다만 확실한 것은, 아침에 만든 ID보다는 저녁에 만든 ID가 큰 값을 갖는다는 점입니다.  
지원자: ID는 숫자로만 구성되나요?  
면접관: 그렇습니다.  
지원자: 시스템 규모는 어느 정도입니까?  
면접관: 초당 10,000 ID를 생성할 수 있어야 합니다.  

질문을 할 때는 요구사항을 이해하고 모호함을 해소하는 데 초점을 맞추어야 한다.  
이번 문제에 대한 답안이 만족해야 할 요구사항은 아래와 같다.  
- ID는 유일해야 한다.
- ID는 숫자로만 구성되어야 한다.
- ID는 64비트로 표현될 수 있는 값이어야 한다.  
- ID는 발급 날짜에 따라 정렬 가능해야 한다 
- 초당 10,000개의 ID를 만들 수 있어야 한다. 

### 2단계: 개략적 설계안 제시 및 동의 구하기   
분산 시스템에서 유일성이 보장되는 ID를 만드는 방법은 여러 가지다.  
- 다중 마스터 복제 (multi-master replication)
- UUID(Universally Unique Identifier)
- 티켓 서버 (ticket server)
- 트위터 스노플레이크 (twitter snowflacke) 접근법

이들 각각의 동작 원리와 장단점을 살펴보자  


**다중 마스터 복제**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/2d7f1464-10b4-408b-b5bf-1ffa0bc997f6)  
- 다중 마스터 복제는 대략 위의 그림과 같은 구성을 갖는다.  
- 이 접근법은 데이터베이스의 auto_increment 기능을 활용하는 것이다.  
- 다만 다음 ID의 값을 구할 때 1만큼 증가시켜 얻는 것이 아니라, k만큼 증가시킨다. (k = DB 수)
- 어떤 서버가 만들어 낼 다음 아이디는, 해당 서버가 생성한 이전 ID 값에 전체 서버의 수 2를 더한 값이다.  
- 이렇게하면 DB수를 늘리면 초당 생산 가능 ID 수도 늘릴 수 있기 때문에, 규모 확장성 문제를 어느정도 해결할 수 있다.
- 하지만 다음과 같은 중대한 단점이 있다.
  - 여러 데이터 센터에 걸쳐 규모를 늘리기 어렵다.
  - ID의 유일성이 보장되겠지만 그 값이 시간 흐름에 맞추어 커지도록 보장할 수는 없다.
  - 서버를 추가하거나 삭제할 때도 잘 동작하도록 만들기 어렵다

**UUID**  
- 유일성이 보장되는 ID를 만드는 또 하나의 간단한 방법.
- 컴퓨터 시스템에 저장되는 정보를 유일하게 식별하기 위한 128비트짜리 수  
- UUID 값은 충돌 가능성이 지극히 낮다.  
- UUID는 서버 간 조율 없이 독립적으로 생성 가능하다.

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/d54116c8-a95c-4a34-b68f-31c5997272ef)  
- 이 구조에서 각 웹 서버는 별도의 ID 생성기를 사용해 독립적으로 ID를 만들어낸다.

장점
- UUID를 만드는 것은 단순하다. 서버 사이의 조율이 필요 없으므로 동기화 이슈도 없다.
- 각 서버가 자기가 쓸 ID를 알아서 만드는 구조이므로 규모 확장도 쉽다. 

단점  
- ID가 128비트로 길다. 여기에서 요구사항은 64비트다.  
- ID를 시간순으로 정렬할 수 없다.  
- ID에 숫자 아닌 값이 포함될 수 있다.  

**티켓 서버**  
- 플리커(Flicker)는 분산 기본 키(distributed primary key)를 만들어 내기 위해 이 기술을 이용하였다. 

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/65483e6b-3be7-46bf-9c24-2625e3b2b0b6)  
- 이 아이디어의 핵심은 auto_increment 기능을 갖춘 데이터베이스 서버, 즉 티켓 서버를 중앙 집중형으로 하나만 사용하는 것이다.  

장점  
- 유일성이 보장되는 오직 숫자로만 구성된 ID를 쉽게 만들 수 있다.
- 구현하기 쉽고 ,중소 규모 애플리케이션에 적합하다.  

단점
- 티켓 서버가 SPOF가 된다. 
  - 이 서버에 장애가 발생하면, 해당 서버를 이용하는 모든 시스템이 영향을 받는다.  
  - 이 이슈를 피하려면 티켓 서버를 여러 대 준비해야 한다.  
  - 하지만 그렇게 하면 데이터 동기화 같은 새로운 문제가 발생할 것이다.  


**트위터 스노플레이크 접근법**  
- 지금까지 여러 ID 생성기 구현 방법을 살펴보았지만, 이번 장에서 풀어야할 문제의 요구사항을 만족시키는 것은 없었다.  
- 트위터는 스노플레이크라고 부르는 독창적인 ID생성 기법을 사용한다.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/cde07968-6224-45c0-b33a-55730d15ac3f)  
- ID를 바로 생성하는 대신, 각개 격파 전략(divide and conquery)을 먼저 적용해보자.  
- 생성해야 하는 ID의 구조를 여러 절(section)로 분할하는 것이다.  
- 각 절의 쓰임새를 살펴보면 
  - 사인(sign) 비트: 1비트를 할당. 지금으로서는 쓰임새가 없지만 나중을 위해 유보해둔다. 음수와 양수를 구별하는데 사용할 수 있다.
  - 타임 스탬프: 41비트를 할당한다. 기원 시각(epoch) 이후로 몇 밀리초가 경과했는지를 나타내는 값이다. 본 설계안의 경우는 기원 시각으로 트위터 스노플레이크 구현에서 사용하는 값인 1288834974657(Nov 04, 2010, 01:42:54 UTC에 해당)을 이용할 것이다. 
  - 데이터 센터 ID: 5비트를 할당한다. 따라서 2^5 = 32 개 데이터센터를 지원할 수 있다.
  - 서버 ID: 5비트 할당. 데이터센터당 32개 서버를 사용할 수 있다.
  - 일련번호: 12비트를 할당. 각 서버에서는 ID를 생성할 때마다 이 일련번호를 1만큼 증가시킨다. 이 값은 1밀리초가 경과할 때마다 0으로 초기화(reset)된다.


### 3단계: 상세 설계  

트위터 스노플레이크 접근법을 다시 보자.  
- 데이터 센터 ID, 서버 ID는 시스템이 시작할 때 결정되며, 일반적으로 시스템 운영 중에는 바뀌지 않는다.  
- 타임 스탬프나 일련번호는 ID 생성기가 돌고 있는 만들어지는 값이다.  

**타임스탬프**  
- ID 구조에서 가장 중요한 41비트를 차지하고 있다.  
- 타임스탬프는 시간이 흐름에 따라 점점 큰 값을 갖게 되므로, 결국 ID는 시간순으로 정렬 가능하게 될 것이다.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/c7200c1d-1172-4b9b-bea1-0ec70a4121f6)  
- 그림은 앞서 살펴본 ID 구조를 따르는 값의 이진 표현 형태로부터 UTC시각을 추출하는 예이다.  
- 이 방법을 역으로 적용하면 어떤 UTC 시각도 상술한 타임스탬프 값으로 변환할 수 있다.  
- 41비트로 표현할 수 있는 타임스탬프의 최댓값은 대략 69년에 해당한다.  
- 따라서 69년동안만 정상 동작하는데, 기원 시각을 현재에 가깝게 맞춰서 오버플로가 발생하는 시점을 늦춴호은 것이다.  
- 69년이 지나면 기원 시각을 바꾸거나 ID 체계를 다른 것으로 이전해야(migration) 한다.

**일련번호**  
- 12비트이므로, 2^12 = 4096개의 값을 가질 수 있다.  
- 어떤 서버가 같은 밀리초 동안 하나 이상의 ID를 만들어 낸 경우에만 0보다 큰 값을 갖게 된다.

궁금증) 

### 4단계: 마무리  
- 유일성이 보장되는 ID생성기 구현에 쓰일 수 있는 다양한 전략, 즉 다중 마스터 복제, UUID, 티켓 서버, 트위터 스노플레이크의 네 가지 방법을 살펴보았다.  
- 우리가 선택한 방식은 스노플레이크인데, 모든 요구사항을 만족하고 분산 환경에서 규모 확장이 가능했기 때문이다.  

설계를 진행하고 시간이 조금 남았다면 면접관과 다음을 추가로 논의할 수도 있을 것이다.  

- 시계 동기화(clock synchronization)
  - 이번 설계를 진행하면서 우리는 ID 생성 서버들이 전부 같은 시계를 사용한다고 가정하였다.
  - 하지만 이런 가정은 하나의 서버가 여러 코어에서 실행될 경우 유효하지 않을 수 있다. 
  - 여러 서버가 물리적으로 독립된 여러 장비에서 실행되는 경우에도 마찬가지다.
  - NTP(Network Time Protocol)은 이 문제를 해결하는 가장 보편적 수단이다. 
- 각 절(section)의 길이 최적화
  - 예를 들어 동시성(concurrency)이 낮고 수명이 긴 애플리케이션이라면 일련번호 절의 길이를 줄이고 타임스탬프 절의 길이를 늘리는 것이 효과적일 수 있다.
- 고가용성
  - ID 생성기는 필수 불가결(mission critical) 컴포넌트 이므로 아주 높은 가용성을 제공해야 한다.

 
## 9. 웹 크롤러 설계 

- 웹 크롤러는 로봇(robot) 또는 스파이더(spider)라고도 부른다.
- 검색 엔진에서 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 컨텐츠를 찾아내는 것이 주된 목적
  - 콘텐츠는 웹 페이지, 이미지나 비디오, 또는 PDF 파일이 될 수 있음 
- 웹 크롤러는 몇 개 웹 페이지에서 시작해서 링크를 따라 나가면서 새로운 컨텐츠를 수집한다. 

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/47409148-efe5-4e4b-a526-e3e8544c1d32)  
- 위 그림은 크롤러 과정을 시각적 예제로 정리한 내용

크롤러는 다양하게 이용된다.  

- 검색 엔진 인덱싱
  - 크롤러의 가장 보편적인 용례
  - 크롤러는 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만듬
  - 일례로 Googlebot은 구글 검색 엔진이 사용하는 웹 크롤러
- 웹 아카이빙
  - 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
  - 많은 국립 도서관이 크롤러를 돌려 웹사이트를 아카이빙하고 있음. 
- 웹 마이닝
  - 웹의 폭발적 성장세는 데이터 마이닝 업계에 전례 없는 기회다.
  - 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해 낼 수 있는 것
- 웹 모니터링
  - 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있음.


### 1단계: 문제 이해 및 설계 범위 확정

웹 크롤러의 기본 알고리즘은 간단하.  

1. URL 집합에서 URL들이 가리키는 모든 웹 페이지를 다운로드
2. 다운받은 웹 페이지에서 URL들을 추출
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복

웹 크롤러의 실제 동작은 복잡하다.  
엄청난 규모 확장성을 갖는 웹 크롤러를 설계하는 것은 어렵다.   
주어진 인터뷰 시간 동안 완성하기는 거의 불가능하다.  
그러니 설계를 진행하기 전에 질문을 던져서 요구사항을 알아내고 설계 범위를 좁히자.  


지원자: 이 크롤러의 주된 용도는 무엇인가요? 검색 엔진 인덱스 생성용인가요? 아니면 데이터 마이닝? 아니면 그 외의 다른 용도가 있나요?  
면접관: 검색 엔진 인덱싱에 쓰일 것입니다.  
지원자: 매달 얼마나 많은 웹 페이지를 수집해야 하나요?  
면접관: 10 억 개의 웹 페이지를 수집해야 합니다.  
지원자: 새로 만들어진 웹 페이지나 수정된 웹 페이지도 고려해야 하나요?  
면접관: 그렇습니다.  
지원자: 수집한 웹 페이지를 저장해야 합니까?  
면접관: 네. 5년간 저장해 두어야 합니다.  
지원자: 중복된 콘텐츠는 어떻게 해야 하나요?  
면접관: 중복된 콘텐츠를 갖는 페이지는 무시해도 됩니다.  

이 질문들은 몇 가지 사례일 뿐이다.  
웹 크롤러가 직관적으로 이해하기 쉬운 제품이긴 하지만 그렇다고 해도 지원자와 면접관은 마음속으로 다른 가정을 하고 있을 수 있다.  

다음은 좋은 웹 크롤러가 만족시켜야 할 속성이다.  
- 규모 확장성
  - 오늘날 웹은 수십억 개의 페이지가 존재한다.  
  - 따라서 병행성(parallelism)을 활용하면 보다 효과적으로 웹 크롤링을 할 수 있다.  
- 안정성
  - 웹은 함정이 많다.  
  - 잘못 작성된 HTML, 아무 반응 없는 서버, 장애, 악성 코드가 붙어 있는 링크 등
  - 크롤러는 이런 비정상적 입력이나 환경에 잘 대응해야 한다.
- 예절(politeness)  
  - 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안 된다. 
- 확장성(extensibility)
  - 새로운 형태의 콘텐츠를 지원하기 쉬어야 한다.
  - 이미지 파일을 새로 크롤링 하고 싶다고 하면, 이를 위해 전체 시스템을 새로 설계하면 안된다.

**개략적 규모 추정**  
아래의 추정치는 많은 가정으로부터 나온 것이다.  
- 매달 10억 개의 웹 페이지를 다운로드
- QPS = 10억/30일/24시간/3600초 = 대략 400 페이지/초
- 최대 QPS = 2 X QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정 
- 10억페이지 x 500k = 500TB/월. 
- 1개월치 데이터: 500TB, 5년간 보관하면 500TB X 12개월 X 5년 = 30PB

### 2단계: 개략적 설계안 제시 및 동의 구하기  

요구사항이 분명해지면 개략적 설계를 진행하자  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/9ec3f9af-558a-4c9b-a19a-3fd7def2268b)

**시작 URL 집합**  
- 웹 크롤러가 크롤링을 시작하는 출발점.
- 예) 어떤 대학 웹사이트로부터 찾아 나갈 수 있는 모든 웹 페이지를 크롤링하는 가장 직관적인 방법은 해당 대학의 도메인 이름이 붙은 모든 페이지의 URL을 시작 URL로 쓰는 것
- 전체 웹을 크롤링해야 하는 경우 시작 URL을 고를 때 좀 더 창의적인 필요가 있다.  
- 일반적으로는 전체 URL 공간을 작은 부분집합으로 나누는 전략을 쓴다. 
  - 지역적인 특색, 나라별로 인기 있는 웹 사이트가 다르다는 점에 착안 하는 것.
- 또다른 방법으로는 주제별로 다른 시작 URL을 사용
  - 예) URL 공간을 쇼핑, 스포츠, 건강 등의 주제별로 세분화하고 그 각각에 다른 시작 URL을 쓰는 것 

**미수집 URL 저장소**  
- 대부분의 현대적 웹 크롤러는 크롤링 상태를 (1) 다운로드할 URL, 그리고 (2) 다운로드된 URL의 두 가지로 나눠 관리한다.
- "다운로드할 URL" 을 저장 관리하는 컴포넌트를 미수집 URL 저장소라고 부른다. (FIFO 큐)

**HTML 다운로더**  
- 인터넷에서 웹 페이지를 다운로드하는 컴포넌트
- 다운로드할 페이지의 URL은 미수집 URL 저장소가 제공

**도메인 이름 변환기**  
- 웹 페이지를 다운받으려면 URL을 IP주소로 변환하는 절차가 필요
- HTML 다운로더는 도메인 이름 변환기를 사용하여 URL에 대응되는 IP주소를 알아낸다.

**콘텐츠 파서**  
- 웹 페이지를 다운로드 하면 파싱과 검증 절차를 거쳐야 한다.
- 이상한 웹 페이지는 문제를 일으킬 수 있고, 저장 공간만 낭비할 수 있다.
- 크롤링 서버 안에 콘텐츠 파서를 구현하면 크롤링 과정이 느려질 수 있어서, 독립된 컴포넌트로 만듬

**중복 컨텐츠인가?**  
- 29% 가량의 웹페이지 콘텐츠는 중복이다.
- 이 문제를 해결하기 위한 자료 구조를 도입하여 데이터 중복을 줄이고 데이터 처리에 소요되는 시간을 줄인다.
- 두 HTML 문서를 비교하는 가장 간단한 방법은 문서를 문자열로 비교하는 것 이지만, 효과적인 방법은 웹 페이지의 해시 값을 비교하는 것 

**콘텐츠 저장소**  
- 콘텐츠 저장소는 HTML 문서를 보관하는 시스템
- 저장소를 구현하는 데 쓰일 기술을 고를 때는 저장할 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려해야 함 
- 본 설계안의 경우에는 디스크와 메모리를 동시에 사용하는 저장소를 택할 것
  - 데이터 양이 너무 많으므로 대부분의 컨텐츠는 디스크에 저장
  - 인기 있는 콘텐츠는 메모리에 두어 접근 지연시간을 줄인다.

**URL 추출기**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/8c1730d6-4363-40b0-b74e-a25ef53d03d0)  
- HTML 페이지를 파싱하여 링크들을 골라내는 역할을 함
- 상대 경로는 전부 https://en.wikipedia.org를 붙여 절대 경로로 변환한다.

**URL 필터**  
- 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할

**이미 방문한 URL**  
- 이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있도록 하는 자료구조를 사용한다.
- 이미 방문한 적이 있는 URL인지 추적하면 같은 URL을 여러 번 처리하는 일을 방지할 수 있으므로 서버 부하를 줄이고, 시스템이 무한 루프에 빠지는 일을 방지할 수 있다.
- 해당 자료 구조로는 블룸 필터나 해시 테이블이 널리 쓰인다.

**URL 저장소**  
- 이미 방문한 URL을 보관하는 저장소 

지금까지 각각의 시스템 컴포넌트가 하는 일을 개략적으로 살펴보았다.  
이제 이 컴포넌트들이 상호 연동하는 과정을 workflow 관점에서 살펴보자.  


![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/963360f7-2711-452b-90f0-f6cbabcccfa9)  
1. 시작 URL들을 미수집 URL 저장소에 저장한다.  
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다.  
3. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고, 해당 IP 주소로 접속하여 웹 페이지를 다운받는다.  
4. 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.
5. 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 개시한다. 
6. 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다 
   - 이미 저장소에 있는 콘텐츠인 경우에는 처리하지 않고 버림
   - 저장소에 없는 콘텐츠인 경우에는 저장소에 저장한 뒤 URL 추출기로 전달
7. URL 추출기는 해당 HTML 페이지에서 링크를 골래낸다
8. 골래난 리읔를 URL 필터로 전달한다
9. 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달한다
10. 이미 처리한 URL인지 확인하기 위하여, URL 저장소에 보관된 URL인지 살핀다. 이미 저장소에 있는 URL은 버린다.  
11. 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소에도 전달한다


### 3단계: 상세 설계  
지금부터는 가장 중요한 컴포넌트와 그 구현 기술을 심도 있게 살펴보자.  
- DFS vs BFS
- 미수집 URL 저장소 
- HTML 다운로더 
- 안정성 확보 전략
- 확장성 확보 전략
- 문제 있는 콘텐츠 감지 및 회피 전략 

**DFS vs BFS**  
- 웹은 유향 그래프와 같다.  
  - 페이지: 노드, 하이퍼링크(URL): 에지(edge)
- 크롤링 프로세스는 이 유향 그래프를 에지를 따라 탐색하는 과정
- 하지만 그래프 크기가 클 경우 어느 정도로 깊숙이 가게 될지 가늠하기 어렵기 때문에, DFS는 좋은 선택이 아닐 가능성이 높다
- 따라서 웹 크롤러는 보통 BFS를 사용한다. 
  - 이 큐의 한쪽으로 탐색할 URL을 집어넣고, 다른 한쪽으로 꺼낸다.
- 하지만 이 구현 방법은 두가지 문제가 있다.  
  - 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다.
    - 결국 크롤러는 같은 호스트에 속한 많은 링크를 다운받느라 바빠지게 되는데, 이때 링크들을 병렬로 처리하게 된다면 크롤링 되는 서버에 수많은 요청으로 과부하에 걸린다. (impolite(예의없는) 크롤러)
  - 표준적 BFS는 URL 간에 우선순위를 두지 않는다.
    - 즉, 처리 순서에 우선순위가 없는다. 
    - 모든 웹 페이지가 같은 수준의 품질, 같은 수준의 중요성을 갖지 않다.
    - 페이지 순위, 사용자 트래픽의 양, 업데이트 빈도 등 여러 가지 척도로 우선순위를 구별하는것이 좋다.

**미수집 URL 저장소**  
- 미수집 URL 저장소를 활용하면 위 문제를 쉽게 해결할 수 있다.  
- 이 저장소를 잘 구현하면 예의(politeness)를 갖춘 크롤러, URL 사이의 우선순위와 신선도를 구별하는 크롤러를 구현할 수 있다.


미수집 URL 저장소의 구현방법에 대한 논문도 다수 나와 있는데, 중요한것을 요약하면 아래와 같다.  

예의   
- 웹 크롤러 수집 대상 서버로 너무 많은 요청을 보내는 것은 무례한(impolite) 일이며, 때로는 DOS 공격으로 간주되기도 한다.  
- 예의 바른 크롤러를 만드는 데 있어서 지켜야 할 한 가지 원칙은, 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청한다는 것이다.  
  - 같은 웹 사이트의 페이지를 다운받는 태스크는 시간차를 두고 실행
  - 웹사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지
  - 즉, 다운로드 스레드는 별도 FIFO 큐를 갖고 있어서, 해당 큐에서 꺼낸 URL만 다운로드 한다.

<img width="794" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/414944e8-753f-4f4b-a92a-51e12b1232e2">    

- 큐 라우터: 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장하는 역할 
- 매핑 테이블: 호스트 이름과 큐 사이의 관계를 보관하는 테이블
- FIFO 큐: 같은 호스트에 속한 URL은 언제나 같은 큐에 보관된다
- 큐 선택기: 큐들을 순회하면서 큐에서 URL을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달하는 역할 
- 작업 스레드: 전달된 URL을 다운로드하는 작업을 수행. 전달된 URL은 순차적으로 처리되며, 작업들 사이에는 일정한 delay를 둘 수 있다.

우선순위  
- 애플 제품에 대한 사용자 의견이 올라오는 포럼의 페이지가 애플 홈페이지와 같은 중요도를 갖는다고 보기 힘들다.  
- 크롤러 입장에서는 중요한 페이지인 애플 홈페이지를 먼저 수집하도록 하는 것이 바람직하다.  
- 유용성에 따라 URL의 우선순위를 나눌 때는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도를 사용할 수 있다.  

<img width="812" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/ce56236c-c0e7-4bd4-8b3d-6456072a9c7b">   

- 순위결정장치: URL 우선순위를 정하는 컴포넌트, URL을 입력으로 받고 우선순위를 계산
- 큐: 우선순위별로 큐가 하나씩 할당. 우선순위가 높으면 선택될 확률도 올라감
- 큐 선택기: 임의 큐에서 처리할 URL을 꺼내는 역할을 담당. 순위가 높은 큐에서 더 자주 꺼내도록 프로그램되어 있다.

<img width="812" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/eecced00-3b39-48d1-a624-ef076077e041">    

- 처리할 URL위에 후면 큐 선택기가 아니라, 전면 큐 선택기임. (그림 잘못됨)
- 전면 큐: 우선순위 결정 과정을 처리
- 후면 큐: 크롤러가 예의 바르게 동작하도록 보증

신선도  
- 웹 페이지는 데이터의 신선함을 유지하기 위해서 이미 다운로드한 페이지라고 해도 주기적으로 재수집할 필요가 있다.  
- 그러나 모든 URL을 재수집하는 것은 많은 시간과 자원이 필요하다.  
- 이 작업을 최적화하기 위한 전략으로는 다음과 같은 것이 있다.  
  - 웹 페이지의 변경 이력 활용
  - 우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집 

미수집 URL 저장소를 위한 지속성 저장장치  
- 검색 엔진을 위한 크롤러의 경우, 처리해야 하는 URL 수는 수억 개에 달한다.  
- 그 모두를 메모리에 보관하는 것은 안정성, 규모확장성 측면에서 바람직하지 않다.  
- 느려서 쉽게 성능 병목지점이 되기 때문에, 전부 디스크에 저장하는것도 좋지 않다.  
- 따라서 본 설계안은 대부분의 URL은 디스크에 두지만 IO 비용을 줄이기 위해 메모리 버퍼에 큐를 두고, 버퍼에 있는 데이터는 주기적으로 디스크에 기록한다.

**HTML 다운로더**  
- HTTP 프로토콜을 통해 웹 페이지를 내려 받는다.  
- 우선, 로봇 제외 프로토콜부터 살펴보자.  


Robots.txt  
- 로봇 제외 프로토콜이라고 부르기도 한다.  
- 이 파일에는 크롤러가 수집해도 되는 페이지 목록이 들어있다.  
- Robots.txt 파일을 반복 다운로드하는 것을 피하기 위해, 주기적으로 다운로드해서 캐시에 보관한다.  
- Robots.txt 도 중요하지만 HTML 다운로더를 설계할 때는 성능최적화도 아주 중요하다.  
- 특정 사이트의 robots.txt 파일을 다운로드 받으려면 .../robots.txt 로 가보면 된다. 

**성능 최적화**  
HTML 다운로더에 사용할 수 있는 성능 최적화 기법들을 살펴보자.  

1. 분산 크롤링  

<img width="391" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/52d034da-b48c-4fce-b71d-58ae649f8a9f">  

- 크롤링 작업을 여러 서버에 분산하는 방법.  
- 각 서버는 여러 스레드를 돌려 다운로드 작업을 처리
- 이 구성을 위해서는 URL 공간을 작은 단위로 분할하고, 각 서버가 그중 일부의 다운로드를 담당하도록 해야 한다.  

2. 도메인 이름 변환 결과 캐시   

- 도메인 이름 변환기는 크롤러 성능의 병목 중 하나, DNS 요청을 보내고 결과를 받는 작업의 동기적 특성 때문 (보통 10ms~200ms 소요)
- 크롤러 스레드 가운데 어느 하나라도 이 작업을 하고 있으면 다른 스레드의 DNS의 요청은 전부 블록된다.  
- 따라서 DNS 조회 결과로 얻어진 도메인 이름과 IP 주소를 캐시에 보관해놓고 크론잡 등을 돌려 주기적으로 갱신하도록 하면 성능을 효과적으로 높일 수 있다.  


3. 지역성  

- 크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법.  
- 크롤링 서버가 크롤링 대상 서버와 지역적으로 가까우면 페이지 다운로드 시간이 줄어들 것이다.  
- 크롤 서버, 캐시, 큐, 저장소 등 대부분의 컴포넌트에 적용 가능 


4. 짧은 타임아웃  

- 어떤 웹 서버는 응답이 느리거나 응답하지 않을 수 있다.  
- 특정 시간 동안 서버가 응답하지 않으면 크롤러는 해당 페이지 다운로드를 중단하고 다음 페이지로 넘어간다.


**안정성**   
최적화된 성능뿐 아니라 안정성도 다운로더 설계 시 중요하게 고려해야 할 부분이다.  
시스템 안정성을 향상시키기 위한 접근법 중 중요한 몇 가지는 아래와 같다.  
- 안정해시: 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술.
- 크롤링 상태 및 수집 데이터 저장: 장애가 발생한 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적으로 저장장치에 기록해두는 것이 좋다. -> 중단되었던 크롤링을 쉽게 재시작할 수 있다.  
- 예외처리: 예외가 발생해도 전체 시스템이 중단되는 일 없이 그 작업을 우아하게 이어나갈 수 있어야 한다.  
- 데이터 검증

**확장성**  
<img width="651" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/f6e98514-0729-421b-859e-1ae94e67c240">  

- 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 신경 써야 한다.
- 본 예제에서는 새로운 모듈을 끼워 넣음으로써 새로운 형태의 컨텐츠를 지원할 수 있도록 설계했다.  
- PNG 다운로더: PNG파일을 다운로드하는 플러그인 모듈 
- 웹 모니터: 웹을 모닡러이하여 저작권이나 상표권이 침해되는 일을 막는 모듈 

**문제 있는 콘텐츠 감지 및 회피**  
중복이거나 의미 없는, 또는 유해한 콘텐츠를 어떻게 감지하고 시스템으로부터 차단할지 살펴보자.  

1. 중복 콘텐츠  

- 해시나 체크섬을 사용해서 탐지

2. 거미 덫  
 
- 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지
- 덫을 자동으로 피해가는 알고리즘을 만들어내는 것은 까다롭다.  
- 한가지 방법은 사람이 수작업으로 덫을 확인하고 찾아낸후에 덫이 있는 사이트를 크롤러 탐색 대상에서 제외하거나 URL 필터 목록에 걸어두는것이다.

3. 데이터 노이즈  

- 어떤 콘텐츠는 거의 가치가 없다.  
- 광고, 스크립트 코드, 스팸 URL 등 
- 크롤러에 도움이 안되므올 가능하다면 제외하자.


### 4단계: 마무리  
좋은 크롤러가 갖추어야하는 특성: 규모 확장성, 예의, 확장성, 안정성  
크롤러 설계안을 제시하고, 핵심 컴포넌트에 쓰이는 기술들을 살펴보았다.   

시간이 허락한다면 면접관과 다음과 같은 것을 추가로 논의해보자.  
- 서버 측 렌더링
  - 많은 웹사이트가 js, ajax 등의 기술을 사용해서 링크를 즉성에서 만들어 낸다.  
  - 웹 페이지를 그냥 있는 그대로 다운받아서 파싱해보면 동적으로 생성되는 링크는 발견할 수 없을 것이다.  
  - 이 문제는 페이지를 파싱하기 전에 서버측 렌더링(동적 렌더링)을 적용하면 해결할 수 있다. 
- 원치 않는 페이지 필터링: 스팸 방지 컴포넌트를 두어 품질이 조악하거나 스팸성인 페이지를 걸러내자.
- 데이터베이스 다중화 및 샤딩: 데이터 계층의 가용성, 규모 확장성, 안정성 향상 
- 수평적 규모 확장성
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션

### 번외: 동적 크롤링 예시 
```python3
import csv
import re
import time

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

csv_file = open('restaurants.csv', mode='w', newline='', encoding='utf-8')
csv_writer = csv.writer(csv_file)
csv_writer.writerow(['name', 'category', 'review_count', 'address', 'rating', 'number', 'image_url'])

options = Options()
options.add_argument("--disable-blink-features=AutomationControlled")

user_agent = 'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
options.add_argument(user_agent)

options.add_experimental_option('excludeSwitches', ['enable-logging', 'enable-automation'])
options.add_experimental_option("useAutomationExtension", False)
options.add_argument("--remote-allow-origins=*")

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

url = "https://map.naver.com/p/search/%EC%84%B1%EA%B7%A0%EA%B4%80%EB%8C%80%EC%97%AD%20%EC%9D%8C%EC%8B%9D%EC%A0%90?c=15.00,0,0,0,dh"
driver.get(url)
time.sleep(12)

searchIFrame = driver.find_element(By.CSS_SELECTOR, "iframe#searchIframe")
driver.switch_to.frame(searchIFrame)
time.sleep(1)

for _ in range(5):  # 6페이지까지
    scrollable_div = driver.find_element(By.CSS_SELECTOR, "div.mFg6p")

    scroll_div = driver.find_element(By.XPATH, "/html/body/div[3]/div/div[2]/div[1]")
    for _ in range(10):
        driver.execute_script("arguments[0].scrollBy(0,2000);", scroll_div)
        time.sleep(2)

    restaurant_names = driver.find_elements(By.XPATH, "//ul/li/div[1]/a[1]/div/div/span[1]")
    restaurant_categories = driver.find_elements(By.XPATH,
                                                 "//div[contains(@class, 'place_bluelink')]//span[@class='KCMnt']")
    reviews = driver.find_elements(By.XPATH, "//div[contains(@class, 'Dr_06')]//span[@class='h69bs']")

    print(len(restaurant_names))
```

출처: https://github.com/meokgu-skku/crawling/blob/main/naver_map_crawling.py


## 10. 알림 시스템 설계  

- 알림 시스템은 최근 많은 프로그램이 채택한 인기 있는 기능
- 최신 뉴스, 제품 업데이트, 이벤트, 선물 등 고객에게 중요할 만한 정보를 비동기적으로 제공한다.  
- 알림 시스템은 단순히 모바일 푸시 알림에 한정되지 않는다.  
- 모바일 푸시 알림, SMS 메시지, 그리고 이메일의 세 가지로 분류할 수 있다.  


### 1단계: 문제 이해 및 설계 범위 확정   

지원자: 이 시스템은 어떤 종류의 알림을 지원해야 하나요?    
면접관: 푸시 알림, SMS 메시지, 그리고 이메일입니다.  
지원자: 실시간 시스템이어야 하나요?  
면접관: 연성 실시간(soft real-time) 시스템이라고 가정합니다. 알림은 가능한한 빨리 전달되어야 하지만 시스템에 높은 부하가 걸렸을 때 약간의 지연은 무방합니다.  
지원자: 어떤 종류의 단말을 지원해야 하나요?  
면접관: IOS 단말, 안드로이드 단말, 그리고 랩톱/데스크톱을 지원해야 합니다.  
지원자: 사용자에게 보낼 알림은 누가 만들 수 있나요?  
면접관: 클라이언트 애플리케이션 프로그램이 만들 수도 있구요. 서버 측에서 스케줄링 할 수도 있습니다.  
지원자: 사용자가 알림을 받지 않도록 설정할 수도 있어야 하나요?  
면접관: 네. 해당 설정을 마친 사용자는 더 이상 알림을 받지 않습니다.  
지원자: 하루에 몇 건의 알림을 보낼 수 있어야 하나요?  
면접관: 천만 건의 모바일 푸시 알림, 백만 건의 SMS 메시지, 5백만 건의 이메일을 보낼 수 있어야 합니다.  


### 2단계: 개략적 설계안 제시 및 동의 구하기  
여기에서는 다음과 같은 내용을 다룬다.  
- 알림 유형별 지원 방안
- 연락처 정보 수집 절차  
- 알림 전송 및 수신 절차

**알림 유형별 지원 방안**  
각각의 알림 메커니즘이 어떻게 동작하는지부터 알아보자.  

<br>
<br>

IOS 푸시 알림  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/8038b2bd-2f8e-46f7-ba8c-029932ac97cc)
- IOS에서 푸시 알림을 보내기 위해서는 세 가지 컴포넌트가 필요하다.  
- 알림 제공자(provider): 알림 요청을 만들어 애플 푸시 알림 서비스(APNS: Apple Push Notification Service)로 보내는 주체. 알림 요청을 만드려면 다음과 같은 데이터가 필요하다.  
  - 단말 토큰: 알림 요청을 보내는 데 필요한 고유 식별자
  - 페이로드: 알림 내용을 담은 JSON 딕셔너리
- APNS: 애플리 제공하는 원격 서비스. 푸시 알림을 IOS 장치로 보내는 역할을 담당
- IOS 단말: 푸시 알림을 수신하는 사용자 단말 

안드로이드 푸시 알림  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/2495b5e5-1d9f-4fc3-85b6-7fa7045f637e)
- 안드로이드 푸시 알림도 비슷한 절차로 전송된다.  
- APNS 대신 FCM을 사용한다는 점만 다르다.  

SMS 메시지  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/b989ace0-cf44-416b-bc06-b0c82bb8f0d1)  
- SMS 메시지를 보낼 때는 보통 트윌리오, 넥스모 같은 제3 사업자의 서비스를 많이 이용한다.  
- 이런 서비스는 대부분 상용 서비스라서 이용요금을 내야 한다.

이메일  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/3185f463-3560-4975-b0aa-1dbb44860965)  
- 대부분의 회사는 고유 이메일 서버를 구축할 역량은 갖추고 있다.  
- 그럼에도 많은 회사가 상용 이메일 서비스를 이용한다.  
- 그중 유명한 서비스로 센드그리드, 메일침프가 있다.  
- 전송 성공률도 높고, 데이터 분석 서비스도 제공한다.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/ad79e795-a089-41b1-9e09-122c0e58a850)  
- 지금까지 살펴본 알림 유형 전부를 한 시스템으로 묶은 결과다.


**연락처 정보 수집 절차**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/f8ac4ba1-a48f-49c6-8af4-f40c80aba5d4)  
- 알림을 보내려면 모바일 단말 토큰, 전화번호, 이메일 주소 등의 정보가 필요하다.  
- 위 그림과 같이 사용자가 우리 앱을 설치하거나 처음으로 계정을 등록하면 API 서버는 해당 사용자의 정보를 수집하여 데이터베이스에 저장한다.  

![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/b79c9791-79ff-4a81-9b6a-85f7c2c336a4)  
- 이 데이터베이스에 연락처 정보를 저장할 테이블 구조가 위 그림이다.  
- 필수 정보만 담은 개략적인 설계안이고, 이메일 주소와 전화번호는 user 테이블에 저장하고, 단말 토큰은 device 테이블에 저장한다.  
- 한 사용자가 여러 단말을 가질 수 있고, 알림은 모든 단말에 전송되어야 한다는 점을 고려한 것이다.

<br>
<br>

**알림 전송 및 수신 절차**  
우선 개략적인 설계안부터 살펴보고, 점차로 최적화해 나가도록 하자.  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/3de1e27a-4fa5-4a86-abce-edcb0389fa8e)  
- 1부터 N까지의 서비스
  - 이 서비스 각각은 마이크로서비스, 크론잡, 분산 시스템 컴포넌트들이 될 수 있다.  
  - 예) 사용자에게 납기일을 알리고자 하는 과금 서비스, 배송 알림을 보내려는 쇼핑몰 웹사이트 등
- 알림 시스템
  - 알림 전송/수신 처리의 핵심
  - 우선 1개 서버만 사용하는 시스템이라고 가정하면
  - 서비스 1~N에 알림 전송을 위한 API를 제공하고, 제3자 서비스에 전달할 알림 페이로드를 만들어 내야 한다.
- 제3자 서비스
  - 사용자에게 알림을 실제로 전달하는 역할
  - 제3자 서비스와의 통합을 진행할 때 유의할 것은 확장성이다. (새로운 서비스를 통합하거나 기존 서비스를 제거할 수 있어야 한다)
  - 또 하나 고려해야 할 것은, 어떤 서비스는 다른 시장에서는 사용할 수 없을 수 있다. (FCM은 중국에서 사용할 수 없어서 Jpush, PushY 같은 서비스를 써야 함)
- IOS, 안드로이드, SMS, 이메일 단말: 사용자는 자기 단말에서 알림을 수신한다.

위 설계를 몇 가지 문제가 있다.  

- SPOF: 알림 서비스에 서버가 하나밖에 없어서, 그 서버에 장애가 생기면 전체 서비스의 장애로 이어진다.  
- 규모 확장성: 한 대 서비스로 푸시 알림에 관계된 모든 것을 처리해서, 데이터베이스나 캐시 등 중요 컴포넌트의 규모를 개별적으로 늘릴 방법이 없다.  
- 성능 병목
  - 알림을 처리하고 보내는 것은 자원을 많이 필요로 하는 작업일 수 있다.
  - 예) HTML 페이지를 만들고 제3자 서비스의 응답을 기다리는 일은 시간이 많이 걸릴 가능성이 있는 작업이다.  
  - 따라서 모든 것을 한 서버로 처리하면 사용자 트래픽이 많이 몰리는 시간에는 시스템이 과부하 상태에 빠질 수 있다. 


<br>
<br>
<br>

개략적 설계안 (개선된 버전)  
다음과 같은 방향으로 개선해 보자.  
- 데이터베이스와 캐시를 알림 시스템의 주 서버에서 분리한다
- 알림 서버를 증설하고 자동으로 수평적 규모 확장이 이루어질 수 있도록 한다.  
- 메시지 큐를 이용해 시스템 컴포넌트 사이의 강한 결합을 끊는다.  

<img width="805" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/ed7e8d8b-0cd1-42e6-8907-2cfc43118d6f">   
- 1부터 N까지의 서비스: 알림 시스템 서버의 API를 통해 알림을 보낼 서비스들
- 알림 서버
  - 알림 전송 API: 스팸 방지를 위해 보통 사내 서비스 또는 인증 클라이언트만 이용 가능하다.
  - 알림 검증: 이메일 주소, 전화번호 등에 대한 기본적 검증을 수행한다.  
  - 데이터베이스 또는 캐시 질의: 알림에 포함시킬 데이터를 가져오는 기능이다.  
  - 알림 전송: 알림 데이터를 메시지 큐에 넣는다. 본 설계안의 경우 하나 이상의 메시지 큐를 사용하므로 알림을 병렬적으로 처리할 수 있다. 
- 캐시: 사용자 정보, 단말 정보, 알림 템플릿 등을 캐시한다.  
- 데이터베이스: 사용자, 알림, 설정 등 다양한 정보를 저장한다  
- 메시지 큐
  - 시스템 컴포넌트 간 의존성을 제거하기 위해 사용
  - 다양한 알림이 전송되어야 하는 경우를 대비한 버퍼 역할도 한다.
  - 알림의 종류별로 별도 메시지 큐를 사용했기 때문에, 제3자 서비스 가운데 하나에 장애가 발생해도 다른 종류의 알림은 정상 동작한다.
  - 작업 서버: 메시지 큐에서 전송할 알림을 꺼내서 제3자 서비스로 전달하는 역할을 담당한다.  
  - 제3자 서비스, IOS AOS SMS EMAIL: 설계 초안이랑 동일하다.  

<br><br><br>

이제 이 컴포넌트들이 어떠헥 협력하여 알림을 전송하게 되는지 살펴보자  

1. API를 호출하여 알림 서버로 알림을 보낸다.
2. 알림 서버는 사용자 정보, 단말 토큰, 알림 설정 같은 메타데이터를 캐시나 데이터베이스에서 가져온다.  
3. 알림 서버는 전송할 알림에 맞는 이벤트를 만들어서 해당 이벤트를 위한 큐에 넣는다. 가령 IOS 푸시 알림 이벤트는 IOS 푸시 알림 큐에 넣어야 한다.  
4. 작업 서버는 메시지 큐에서 알림 이벤트를 꺼낸다.  
5. 작업 서버는 알림을 제3자 서비스로 보낸다  
6. 제3자 서비스는 사용자 단말로 알림을 전송한다.

### 3단계: 상세 설계   
다음 내용을 좀 더 자세히 알아보자.  
- 안정성
- 추가로 필요한 컴포넌트 및 고려사항: 알림 템플릿, 알림 설정, rate limiting, retry mechanism, 보안, 큐에 보관된 알림에 대한 모니터링과 이벤트 추적
- 개선된 설계안 

**안정성**  
분산 환경에서 운영될 알림 시스템을 설계할 때는 안정성을 확보하기 위한 사항 몇 가지를 반드시 고려해야 한다.  

<br><br>

1. 데이터 손실 방지  

<img width="564" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/84e390c8-25c1-4d54-9718-1fa9d7367306">   
- 알림 전송 시스템의 가장 중요한 요구사항 가운데 하나는 어떤 상황에서도 알림이 소실되면 안 된다는 것이다.  
- 알림이 지연되거나 순서가 틀려도 괜찮지만, 사라지면 곤란하다.  
- 이 요구사항을 만족하려면 알림 시스템은 알림 데이터를 데이터베이스에 보관하고 재시도 메커니즘을 구현해야 한다.  
- 위 그림과 같이 알림로그 데이터베이스를 유지하는 것이 한 가지 방법이다.  

2. 알림 중복 전송 방지  

- 같은 알림이 여러 번 반복되는 것을 완전히 막는 것은 가능하지 않다.  
- 대부분의 경우 알림은 딱 한 번만 전송되겠지만, 분산 시스템의 특성상 가끔 같은 알림이 중복되어 전송되기도 할 것이다.  
- 그 빈도를 줄이려면 중복을 탐지하는 메커니즘을 도입하고, 오류를 신중하게 처리해야 한다.  
- 다음은 간단한 로직 방지 로직 사례다.
  - 보내야 할 알림이 도착하면 그 이벤트 ID를 검사하여 이전에 본 적이 있는 이벤트인지 살핀다.  
  - 중복된 이벤트면 버리고, 그렇지 않으면 알림을 발송한다.

<br><br><br>

**추가로 필요한 컴포넌트 및 고려사항**  
- 지금까지 사용자 연락처 정보를 어떻게 수집하고, 알림은 어떻게 보내고 받을 것인지 살펴보았다.  
- 그러나 알림 시스템은 사실 이보다 훨씬 복잡하다.  
- 지금부터는 알림 템플릿, 알림 설정, 이벤트 추적, 시스템 모니터링, 처리율 제한 등 알림 시스템 구현을 위해 필요한 추가 컴포넌트들에 대해 알아보자.

1. 알림 템플릿  
- 사용자는 이미 너무 많은 알림을 받고 있어서 쉽게 피곤함을 느낀다.  
- 따라서 많은 웹사이트와 앱에서는 사용자가 알림 설정을 상세히 조정할 수 있도록 하고 있다.  
- 이 정보는 알림 설정 테이블에 보관되며, 이 테이블에는 다음과 같은 필드들이 필요하다. 
  - user_id, channel (푸시 알림, 이메일, SMS 등), opt_in (알림 여부)
- 이와 같은 설정을 도입한 뒤에는 특정 종류의 알ㄹㄹ미을 보내기 전에 반드시 해당 사용자가 해당 알림을 켜 두었는지 확인해야 한다. 

2. 전송률 제한  
- 사용자에게 너무 많은 알림을 보내지 않도록 한 가지 방법은, 한 사용자가 받을 수 있는 알림의 빈도를 제한하는 것이다.  
- 이것이 중요한 이유는, 알림을 너무 많이 보내면 사용자가 알림 기능을 아예 꺼 버릴 수도 있기 때문이다.

3. 재시도 방법  
- 제3자 서비스가 알림 전송에 실패하면, 해당 알림을 재시도 전용 큐에 넣는다.  
- 같은 문제가 계속해서 발생하면 개발자에게 통지한다.  

4. 푸시 알림과 보안
- IOS와 안드로이드 앱의 경우, 알림 전송 API는 appKey와 appSecret을 사용하여 보안을 유지한다.  
- 따라서 인증된, 혹은 승인된 클라이언트만 해당 API를 사용하여 알림을 보낼 수 있다.  

5. 큐 모니터링
- 알림 시스템을 모니터링 할 때 중요한 메트릭 하나는 큐에 쌓인 알림의 개수이다.  
- 이 수가 너무 크면 작업 서버들이 이벤트를 빠르게 처리하고 있지 못하다는 뜻이다.  
- 그런 경우에는 작업 서버를 증설하는 게 바람직하다.

6. 이벤트 추적  
<img width="564" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/2f59bf59-4f69-42ba-8043-a363deb828b6">  
- 알림 확인율, 클릭율, 시렞 앱 사용으로 이어지는 비율 같은 메트릭은 사용자를 이해하는데 중요하다.  
- 데이터 분석 서비스는 보통 이벤트 추적 기능도 제공한다.  
- 따라서 보통 알림 시스템을 만들면 데이터 분석 서비스와도 통합해야 한다.  
- 위의 그림은 데이터 분석 서비스를 통해 추적하게 될 알림 시스템 이벤트의 사례다.

<br><br><br><br>

**수정된 설계안**  
<img width="759" alt="image" src="https://github.com/sinkyoungdeok/TIL/assets/28394879/2845e5fd-b60e-49d3-8585-03b63aca0748">   
종전 설계안에 없는 많은 컴포넌트가 추가되었다.  
- 알림 서버에 인증과 전송률 제한기능 추가
- 전송 실패에 대응하기 위한 재시도 기능이 추가 -> 전송에 실패한 알림은 다시 큐에 넣고 지정된 횟수만큼 재시도한다.  
- 전송 템플릿을 사용하여 알림 생성 과정을 단순화하고 알림 내용의 일관성을 유지한다.  
- 모니터링과 추적 시스템을 추가하여 시스템 상태를 확인하고 추후 시스템을 개선하기 쉽도록 했다.

<br><br>

### 4단계: 마무리  
- 규모 확장이 쉬울 뿐 아니라 푸시 알림, SMS 메시지, 이메일 등 다양한 정보 전달 방식을 지원하는 알림 시스템을 만들어 보았다.  
- 시스템 컴포넌트 사이의 결합도를 낮추기 위해 메시지 큐를 적극적으로 사용하였다.  
- 개략적 설계안과 더불어 각 컴포넌트의 구현 방법과 최적화 기법에 대해서 심도있게 알아보았다. 특히 아래 주제에 집중하였다. 
  - 안정성: 메시지 전송 실패율을 낮추기 위해 안정적인 재시도 메커니즘을 도입
  - 보안: 인증된 클라이언트만이 알림을 보낼 수 있도록 appKey, appSecret 등의 메커니즘을 이용
  - 이벤트 추적 및 모니터링: 알림이 만들어진 후 성공적으로 전송되기까지의 과정을 추적하고 시스템 상태를 모니터링하기 위해 알림 전송의 각 단계마다 이벤트를 추적하고 모니터링할 수 있는 시스템을 통합하였다.  
  - 사용자 설정: 사용자가 알림 수신 설정을 조정할 수 있도록 하였다. 
  - 전송률 제한: 사용자가 알림을 보내는 빈도를 제한할 수 있도록 하였다. 


## 11. 뉴스 피드 시스템 설계    

- 뉴스피드는 홈 페이지 중앙에 지속적으로 업데이트되는 스토리들
- 사용자 상태 정보 업데이트, 사진, 비디오, 링크, 앱 활동, 팔로하는 사람들, 페이지, 또는 그룹으로부터 나오는 좋아요 등을 포함한다.  
- 뉴스피드 시스템 설계는 아주 유명한 면접 문제고, 비슷한 유형의 문제는 "페이스북 뉴스 피슫 설계", "인스타그램 피드 설계", "트위터 타임라인 설계" 등이 있다.



### 1단계: 문제 이해 및 설계 범위 확정  

뉴스 피드 시스템을 설계하라고 했을 때 면접관의 의도는 무엇이었는지 질문을 통해 알아가야 한다.  
최소한 어떤 기능을 지원해야 할지는 반드시 파악해야 한다.  


지원자: 모바일 앱을 위한 시스템인가요? 아니면 웹? 둘다 지원해야 합니까?  
면접관: 둘 다 지원해야 합니다.  
지원자: 중요한 기능으로는 어떤 것이 있을까요?  
면접관: 사용자는 뉴스 피드 페이지에 새로운 스토리를 올릴 수 있어야 하고, 친구들이 올리는 스토리를 볼 수도 있어야 합니다.  
지원자: 뉴스 피드에는 어떤 순서로 스토리가 표시되어야 하나요? 최신 포스트가 위에 오도록 해야 하나요? 아니면 토픽 점수 같은 다른 기준이 있습니까? 예를들어, 가까운 친구의 포스트는 좀 더 위에 배치 되어야 한다는가 하는.  
면접관: 그냥 단순히 시간 흐름 역순으로 표시된다고 가정합시다.  
지원자: 한 명의 사용자는 최대 몇 명의 친구를 가질 수 있습니까?  
면접관: 5,000 명입니다.  
지원자: 트래픽 규모는 어느 정도입니까?  
면접관: 매일 천만 명이 방문한다고 가정합시다. (10million DAU)
지원자: 피드에 이미지나 비디오 스토리도 올라올 수 있습니까?  
면접관: 스토리에는 이미지나 비디오 등의 미디어 파일이 포함될 수 있습니다.  


### 2단계: 개략적 설계안 제시 및 동의 구하기 

(1) 피드 발행 과 (2) 뉴스 피드 생성 두 가지 부분으로 나뉘어서 보자.  
- 피드 발행: 사용자가 스토리를 포스팅하면 해당 데이터를 캐시와 데이터베이스에 기록한다. 새 포스팅은 친구의 뉴스 피드에도 전송된다.  
- 뉴스 피드 생성: 지면 관계쌍 뉴스 피드는 모든 친구의 포스팅을 시간 흐름 역순으로 모아서 만든다고 가정하자.  

**뉴스 피드 API**  
- 클라이언트가 서버와 통신하기 위해 사용하는 수단  
- HTTP 프로토콜 기반이고, 상태 정보를 업데이트하거나, 뉴스 피드를 가져오거나, 친구를 추가하는 등의 다양한 작업을 수행하는데 사용한다.  
- 피드 발행 API와 피드 읽기 API를 살펴보자  

<br><br>


피드 발행 API -> POST /v1/me/feed

피드 읽기 API -> GET /v1/me/feed


**피드 발행**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/10e0873a-9148-455d-b5da-42c2d8313f4c)  
- 사용자: 모바일 앱이나 브라우저에서 새 포스팅을 올리는 주체. 피드 발행 API를 사용
- 로드밸런서: 트래픽을 웹 서버들로 분산한다 
- 웹 서버: HTTP 요청을 내부 서비스로 중계하는 역할을 담당 
- 포스팅 저장 서비스: 새 포스팅을 데이터베이스와 캐시에 저장한다  
- 포스팅 전송 서비스: 새 포스팅을 친구의 뉴스 피드에 푸시한다. 뉴스 피드 데이터는 캐시에 보관하여 빠르게 읽어갈 수 있도록 한다.  
- 알림 서비스: 친구들에게 새 포스팅이 올라왔음을 알리거나, 푸시 알림을 보내는 역할을 담당한다

**뉴스 피드 생성**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/1eeb06e6-a449-4824-b5c0-ff0cfe817e64)  
- 사용자: 뉴스 피드를 읽는 주체. 피드 읽기 API를 사용
- 로드밸런서: 트래픽을 웹 서버들로 분산한다 
- 웹 서버: 트래픽을 뉴스 피드 서비스로 보낸다
- 뉴스 피드 서비스: 캐시에서 뉴스 피드를 가져오는 서비스
- 뉴스 피드 캐시: 뉴스 피드를 렌더링할 때 필요한 피드 ID를 보관한다.  

<br><br><bvr><br><br>

### 3단계: 상세 설계  
피드 발행과 생성의 두가지 흐름이 포함된 설계를 보다 상세히 살펴보자.  

**피드 발행 흐름 상세 설계**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/cca943e7-5841-4ee1-ba99-4a9a134d869f)  
- 위 그림은 피드 발행 흐름의 상세 설계안이다.  
- 대부분의 컴포넌트는 개략적 설계안에서 다룬 정도로 충분하고
- 웹 서버와 포스팅 전송 서비스에 초점을 맞추어 보자.

웹 서버  
- 클라이언트와 통신 + 인증이나 처리율 제한 등의 기능도 수행
- 올바른 인증 토큰을 Authorization 헤더에 넣고 API를 호출하는 사용자만 포스팅을 할 수 있어야 한다.  
- 스팸을 막고 유해한 콘텐츠가 자주 올라오는 것을 방지하기 위해서 특정 기간 동안 한 사용자가 올릴 수 있는 포스팅의 수에 제한을 두어야 한다.  

포스팅 전송(팬아웃) 서비스  
- 어떤 사용자의 새 포스팅을 그 사용자와 친구 관계에 있는 모든 사용자에게 전달하는 과정
- 쓰기 시점에 팬아웃하는 모델(푸시 모델), 읽기 시점에 팬아웃하느 모델(풀 모델)
- 이 각각의 동작 원리를 좀 더 자세히 살펴본 후, 우리 시스템에 적합한 모델이 무엇인지 살펴보자.  

쓰기 시점에 팬아웃(푸시 모델)  
- 새로운 포스팅을 기록하는 시점에 뉴스 피드를 갱신  
- 포스팅이 완료되면 바로 해당 사용자의 캐시에 해당 포스팅을 기록한다.  
- 장점
  - 뉴스 피드가 실시간으로 갱신되며 친구 목록에 있는 사용자에게 즉시 전송한다.  
  - 새 포스팅이 기록되는 순간에 뉴스 피드가 이미 갱신되므로 뉴스 피드를 읽는데 드는 시간이 짧아진다.  
- 단점
  - 친구가 많은 사용자의 경우 친구 목록을 가져오고 그 목록에 있는 사용자 모두의 뉴스 피드를 갱신하는데 많은 시간이 소요될 수 있다. 핫키 문제
  - 서비스를 자주 이용하지 않는 사용자의 피드까지 갱신해야 하므로 컴퓨팅 자원이 낭비된다.  

읽기 시점에 팬아웃(풀 모델)  
- 피드를 읽어야 하는 시점에 뉴스 피드를 갱신  
- 사용자가 본인 홈페이지나 타임라인을 로딩하는 시점에 새로운 포스트를 가져온다.  
- 장점
  - 비활성화된 사용자, 또는 서비스에 거의 로그인하지 않는 사용자의 경우에는 이 모델이 유리
  - 데이터를 친구 각각에 푸시하는 작업이 필요 없으므로 핫키 문제가 없다.  
- 단점
  - 뉴스 피드를 읽는데 많은 시간이 소요될 수 있다. 

<br><Br>
- 본 설계안의 경우, 위 두가지 방법을 결합하여 장점은 취하고 단점은 버리는 전략을 취할 것이다.  
- 뉴스 피드를 빠르게 가져올 수 있도록 하는 것은 아주 중요하므로 대부분의 사용자에 대해서는 푸시 모델을 사용한다.  
- 친구나 팔로어가 아주 많은 사용자의 경우에는 팔로어로 하여금 해당 사용자의 포스팅을 필요할 때 가져가도록 하는 풀 모델을 사용하여 시스템 과부하를 방지한다.  
- 아울러 안정 해시를 통해 요청과 데이터를 보다 고르게 분산하여 핫키 문제를 줄여볼 것이다.  
- 바로 전 위 그림에 제시한 설계안 가운데 팬아웃 서비스에 관한 부분만 따로 떼어서 아래그림으로 옮겼다.  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/efdcd97f-61cd-4550-a811-73fb97d5d9fe)  

팬아웃 서비스는 다음과 같이 동작한다.  
1. 그래프 데이터베이스에서 친구 ID 목록을 가져온다. 그래프 데이터베이스는 친구 관계나 친구 추천을 관리하기 적합하다.  
2. 사용자 정보 캐시에서 친구들의 정보를 가져온다. 그런 후에 사용자 설정에 따라 친구 가운데 일부를 걸러낸다. 예를 들어 친구 중 누군가의 피드 업데이트를 무시하기로 설정했다면 친구 관계는 유지되지만 새 스토리는 보이지 않아야 한다.  
3. 친구 목록과 새 스토리의 포스팅 ID를 메시지 큐에 넣는다  
4. 팬아웃 작업 서버가 메시지 큐에서 데이터를 꺼내서 뉴스 피드 데이터를 뉴스 피드 캐시에 넣는다.   
   1. 뉴스 피드 캐시는 <포스팅 ID, 사용자 ID>의 순서쌍을 보관하는 매핑 테이블이라고 볼 수 있다.
   2. 사용자 정보와 포스팅 정보 전부를 이 테이블에 저장하지 않는 이유는, 메모리 요구량이 지나치게 높아질 수 있기 때문이다.  
   3. 메모리 크기를 적정 수준으로 유지하기 위해서, 이 캐시의 크기에 제한을 두며, 해당 값은 조정이 가능하도록 한다. 어떤 사용자가 뉴스 피드에 올라온 수 천 개의 스토리를 전부 훑어보는 일이 벌어질 확률은 지극히 낮고 대부분의 사용자가 보고자 하는것은 최신 스토리다. 따라서 캐시 미스가 일어날 확률은 낮다. 


<br><br><br>  
**피드 읽기 흐름 상세 설계**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/5c40b1d3-9a00-42c2-b374-af82b7618581)   
이미지나 비디오와 같은 미디어 콘텐츠는 CDN에 저장하여 빨리 읽어갈 수 있도록 했다.  
1. 사용자가 뉴스 피드를 읽으려는 요청을 보낸다. (/v1/me/feed)
2. 로드밸런서가 요청을 웹 서버 가운데 하나로 보낸다.  
3. 웹 서버는 피드를 가져오기 위해 뉴스 피드 서비스를 호출한다  
4. 뉴스 피드 서비스는 뉴스 피드 캐시에서 포스팅 ID 목록을 가져온다.  
5. 뉴스 피드에 표시할 사용자 이름, 사용자 사진, 포스팅 콘텐츠, 이미지 등을 사요자 캐시와 포스팅 캐시에서 가져와 완전한 뉴스 피드를 만든다.  
6. 생성된 뉴스 피드를 JSON 형태로 클라이언트에게 보낸다. 클라이언트는 해당 피드를 렌더링한다. 


<br><Br><br>  
**캐시 구조**  
![image](https://github.com/sinkyoungdeok/TIL/assets/28394879/315cb88a-5368-4301-bbb9-b2c6716ee84b)  
캐시는 뉴스 피드 시스템의 핵심 컴포넌트다.  
본 설계안의 경우 위의 그림과 같이 캐시를 다섯 계층으로 나눈다.  
- 뉴스 피드: 뉴스 피드의 ID를 보관
- 콘텐츠: 포스팅 데이터를 보관한다. 인기 콘텐츠는 따로 보관한다.  
- 소셜 그래프: 사용자 간 관계 정보를 보관
- 행동: 포스팅에 대한 사용자의 행위에 관한 정보를 보관. 포스팅에 대한 좋아요, 답글 등등이 해당된다.  
- 횟수: 좋아요 횟수, 응답 수, 팔로어 수, 팔로잉 수 등의 정보를 보관

### 4단계: 마무리  
- 뉴스 피드 시스템에서 이번 설계안은 뉴스 피드 발행과 생성의 두부분으로 구성되어 있다.  
- 설계를 마친후에도 시간이 좀 남는다면 면접관과 규모 확장성 이슈를 논의하는 것도 좋다.  
- 다루면 좋을 만한 주제를 나열해보았다.  

데이터베이스 규모 확장  
- 수직 vs 수평
- SQL vs NoSQL
- 주-부(master-slave) 다중화
- 복제본에 대한 읽기 연산 
- 일관성 모델
- 데이터베이스 샤딩

이외에도 논의해 보면 좋을 만한 주제  
- 웹 계층을 무상태로 운영
- 가능한 한 많은 데이터를 캐시 하는 방법
- 여러 데이터 센터를 지원할 방법 
- 메시지 큐를 사용하여 컴포넌트 사이의 결합도 낮추기  
- 핵심 메트릭에 대한 모니터링. 예) 트래픽이 몰리는 시간대의 QPS, 사용자가 뉴스 피드를 새로고침 할 때의 지연시간 등 


## 12. 채팅 시스템 설계  

### 1단계: 문제 이해 및 설계 범위 확정  

- 1:1 채팅 집중 앱: 페이스북 메신저, 위챗, 왓츠앱
- 그룹 채팅 집중 앱: 슬랙, 디스코드 (대규모 그룹 소통 + 응답지연이 낮은 음성 채팅)
- 적어도 설계 대상이 1:1 채팅 앱인지 아니면 그룹 채팅 앱인지 정도는 물어봐야한다.  

지원자: 어떤 앱을 설계해야 하나요? 1:1 채팅 앱입니까 아니면 그룹 채팅 앱입니까?   
면접관: 둘 다 지원할 수 있어야 합니다.   
지원자: 모바일 앱인가요 아니면 웹 앱인가요?   
면접관: 둘 다입니다.   
지원자: 처리해야 하는 트래픽 규모는 어느 정도입니까?   
면접관: 일별 능동 사용자 수(DAU) 기준으로 5천만 명을 처리할 수 있어야 합니다.   
지원자: 그룹 채팅의 경우에 인원 제한이 있습니까?   
면접관: 최대 100명까지 참가할 수 있습니다.   
지원자: 중요 기능으로는 어떤 것이 있을까요? 가령, 첨부파일도 지원할 수 있어야 하나요?   
면접관: 1:1 채팅, 그룹 채팅, 사용자 접속상태 표시를 지원해야 합니다. 텍스트 메시지만 주고받을 수 있습니다.   
지원자: 메시지 길이에 제한이 있나요?   
면접관: 네. 100,000자 이하여야 합니다.  
지원자: 종단 간 암호화를 지원해야 하나요?   
면접관: 현재로서는 필요 없습니다만 시간이 허락한다면 논의해볼 수 있겠습니다.  
지원자: 채팅 이력은 얼마나 오래 보관해야 할까요?  
면접관: 영원히요.  

이번 장에서 페이스북 메신저와 유사한 채팅 앱을 설계해보자.  
이 앱은 다음과 같은 기능을 갖는다.  
- 응답지연이 낮은 일대일 채팅 기능
- 최대 100명까지 참여할 수 있는 그룹 채팅 기능
- 사용자의 접속상태 표시 기능
- 다양한 단말 지원. 하나의 계정으로 여러 단말에 동시 접속 지원  
- 푸시 알림 
- 5천만 DAU

### 2단계: 개략적 설계안 제시 및 동의 구하기 

클라이언트는 서로 직접 통신하지 않고, 채팅 서비스와 통신한다.  
채팅 서비스는 아래 기능을 제공해야 한다.  
- 클라이언트들로부터 메시지 수신  
- 메시지 수신자 결정 및 전달  
- 수신자가 접속 상태가 아닌 경우에는 접속할 때 까지 해당 메시지 보관

![image](https://github.com/user-attachments/assets/3aefd071-63c5-424b-a218-a3b703b90f2d)  
- 채팅을 시작하려는 클라이언트는 네트워크 통신 프로토콜을 사용하여 서비스에 접속한다.  
  - 따라서 채팅 서비스의 경우 어떤 통신 프로토콜을 사용할 것인가도 중요한 문제다.  
- 대부분의 클라이언트/서버 애플리케이션에서 요청을 보내는 것은 클라이언트인데, 채팅 시스템의 경우도 마찬가지다.
  - 메시지 송신 클라이언트(sender)가 이 역할을 한다.  
  - 위 그림에서 송신 클라이언트는 수신 클라이언트에게 전달할 메시지를 채팅 서비스에 보낼 때, 오랜 세월 검증된 HTTP 프로토콜을 사용 한다.  
  - 클라이언트는 채팅 서비스에 HTTP 프로토콜로 연결한 다음 메시지를 보내어 수신자에게 해당 메시지를 전달하라고 알린다.  
  - 채팅 서비스와의 접속에는 keep-alive 헤더를 사용하면 효율적인데, 클라이언트와 서버 사이의 연결을 끊지 않고 계속 유지할 수 있고 TCP 접속 과정에서 발생하는 핸드셰이크 횟수를 줄일 수 있다.
  - HTTP는 메시지 전송 용도로는 괜찮은 선택이고, 페이스북 같은 많은 대중적 채팅 프로그램이 초기에 HTTP를 사용했다.  
- 하지만 메시지 수신 시나리오는 이것보다 복잡하다.  
  - HTTP는 클라이언트가 연결을 만드는 프로토콜이며, 서버에서 클라이언트로 임의 시점에 메시지를 보내는 데는 쉽게 쓰일 수 없다.  
  - 서버가 연결을 만드는 것처럼 동작할 수 있도록 하기 위해 많은 기법이 제안되어 왔는데, 폴링, 롱 폴링, 웹소켓 등이 그런 기술이다.  

<br><br><br><br>  

**폴링**  
![image](https://github.com/user-attachments/assets/621ecfcb-c47b-489f-8336-85752df44ed1)   
- 클라이언트가 주기적으로 서버에게 새 메시지가 있느냐고 물어보는 방법  
- 폴링 비용은 폴링을 자주 할수록 올라간다.  
- 답해줄 메시지가 없는 경우에는 서버 자원이 불필요하게 낭비된다는 문제도 있다.  


**롱 폴링**  
![image](https://github.com/user-attachments/assets/d46b08ba-8e2a-4dc7-910f-8d0c290a6466)  
- 폴링은 여러 가지로 비효율적일 수 있어서 나온 기법이 롱 폴링이다.  
- 롱 폴링의 경우 클라이언트는 새 메시지가 반환되거나 타임아웃 될 때까지 연결을 유지한다.  
- 클라이언트는 새 메시지를 받으면 기존 연결을 종료하고 서버에 새로운 요청을 보내어 모든 절차를 다시 시작한다.  
- 이 방법에는 다음과 같은 약점이 있다.  
  - 메시지를 보내는 클라이언트와 수신하는 클라이언트가 같은 채팅 서버에 접속하게 되지 않을수도 있다. HTTP 서버들은 보통 무상태 서버다. 그러므로 메시지를 받은 서버는 해당 메시지를 수신할 클라이언트와의 롱 폴링 연결을 가지고 있지 않은 서버일 수 있다.
  - 서버 입장에서는 클라이언트가 연결을 해제했는지 아닌지 알 좋은 방법이 없다.  
  - 여전히 비효율적이다. 메시지를 많이 받지 않는 클라이언트도 타임아웃이 일어날 때마다 주기적으로 서버에 다시 접속할 것이다. 

**웹소켓**  
![image](https://github.com/user-attachments/assets/bacd5bb2-cc8f-4b87-b541-37a381f4bb58)  
- 서버가 클라이언트에게 비동기 메시지를 보낼 때 가장 널리 사용하는 기술이다.  
- 웹소켓 연결은 클라이언트가 시작한다. 
  - 한번 맺어진 연결은 항구적이며 양방향이다.  
  - 이 연결은 처음에는 HTTP 연결이지만 특정 핸드셰이크 절차를 거쳐 웹소켓 연결로 업그레이드 된다.  
  - 이 항구적인 연결이 만들어지고 나면 서버는 클라이언트에게 비동기적으로 메시지를 전송할 수 있다.  
  - 웹소켓은 일반적으로 방화벽이 있는 환경에서도 잘 동작한다.  
  - 80이나 443처럼 HTTP 혹은 HTTPS 프로토콜이 사용하는 기본 포트번호를 그대로 쓰기 때문이다.  

![image](https://github.com/user-attachments/assets/574c339a-fbdb-409b-b204-6203eb77cf45)
- HTTP 프로토콜이 메시지를 보내려는 클라이언트에게 괜찮은 프로토콜이지만, 웹소켓은 양방향 메시지 전송까지 가능하게 하므로 웹소켓 대신 HTTP를 굳이 고집할 이유는 없다.  
- 위 그림은 어떻게 웹 소켓이 메시지 전송이나 수신에 쓰일 수 있는지 간략히 제시한다.
- 웹소켓을 이용하면 메시지를 보낼 때나 받을 때 동일한 프로토콜을 사용할 수 있으므로 설계분 아니라 구현도 단순하고 직관적이다.  
- 유의할 것은 웹소켓 연결은 항구적으로 유지되어야 하기 때문에 서버 측에서 연결 관리를 효율적으로 해야 한다는 것이다.  

<br><br><br><br>

**개략적 설계안**  
- 클라이언트와 서버 사이의 주 통신 프로토콜로 웹소켓을 사용하기로 결정했지만, 다른 부분에서는 굳이 웹소켓을 쓸 필요는 없다.  
- 대부분의 기능(회원가입, 로그인, 사용자 프로파일 등)은 일반적인 HTTP상에서 구현해도 된다.  

![image](https://github.com/user-attachments/assets/e0fcf31b-da32-4629-bd98-2b79638b5b33)  
- 이번 장에서 다루는 채팅 시스템은 세 부분으로 나누어 볼 수 있다.  
- 즉 무상태 서비스, 상태유지 서비스, 그리고 제3자 서비스 연동의 세 부분으로 나누어 살펴볼 수 있다.  

<br><br>

무상태 서비스  
- 로그인, 회원가입, 사용자 프로파일 표시 등을 처리하는 전통적인 요청/응답 서비스
- 무상태 서비스는 로드밸런서 뒤에 위치한다.  
  - 로드밸런서가 하는 일은 요청을 그 경로에 맞는 서비스로 정확하게 전달하는 것이다.  
  - 로드밸런서 뒤 서비스는 모놀리틱 서비스 혹은 마이크로서비스다.
- 이 서비스들 가운데 상당수가 시장에 완제품으로 나와 있어서 우리가 직접 구현하지 않아도 쉽게 사서 쓸 수 있다.  
- 나중에 좀 더 자세히 살펴볼 것은 서비스탐색 서비스다.  
- 이 서비스는 클라이언트가 접속할 채팅 서버의 DNS 호스트명을 클라이언트에게 알려주는 역할을 한다.  


상태 유지 서비스  
- 유일하게 상태 유지가 필요한 서비스인 채팅 서비스다.  
- 각 클라이언트가 채팅 서버와 독립적인 네트워크 연결을 유지해야 하기 때문이다.  
- 클라이언트는 보통 서버가 살아 있는 한 다른 서버로 연결을 변경하지 않는다.  
- 탐색 서비스는 채팅 서비스와 긴밀히 협력하여 특정 서버에 부하가 몰리지 않도록 한다.  


제 3자 서비스 연동  
- 채팅 앱에서 가장 중요한 제3자 서비스는 푸시 알림이다.  
- 새 메시지를 받았다면 설사 앱이 실행 중이지 않더라도 알림을 받아야 해서다.  
- 따라서 푸시 알림 서비스와의 통합은 아주 중요하다.  
- 자세한 정보가 필요하면 "알림 시스템 설계"를 참고하자.


규모 확장성  
- 이번 장에서 다루는 시스템의 경우에는 동시 접속자가 1M이라고 가정할 것인데, 접속당 10K의 서버 메모리가 필요하다고 본ㄴ다면 10GB 메모리만 있으면 모든 연결을 다 처리할 수 있다.
- 하지만 모든 것을 서버 한 대에 담은 설계안을 내밀면 면접에서 좋은 점수를 따기 어렵다. 여러 가지 이유가 있지만 SPOF도 그 가운데 하나다.  
- 하지만 서버만 한 대 갖는 설계안에서 출발하여 점차로 다듬어 나가는 것은 괜찮다.
- 지금까지 설명한 모든 것을 하나로 묶으면 아래 그림과 같다. 

![image](https://github.com/user-attachments/assets/f045ca99-8eb7-4474-b737-c248deec4af2)   
위의 그림에서 유의할 것은 실시간으로 메시지를 주고받기 위해 클라이언트는  
채팅 서버와 웹소켓 연결을 끊지 않고 유지한다는 것이다.  
- 채팅 서버는 클라이언트 사이에 메시지를 중계하는 역할을 담당한다.  
- 접속상태 서버는 사용자의 접속 여부를 관리한다  
- API 서버는 로그인, 회원가입, 프로파일 변경 등 그 외 나머지 전부를 처리한다  
- 알림 서버는 푸시 알림을 보낸다 
- 키-값 저장소에는 채팅 이력을 보관한다. 시스템에 접속한 사용자는 이전 채팅 이력을 전부 보게 될 것이다.  


저장소  
- 서버, 제3자 서비스 등의 기술 스택 깊은 곳에 데이터 계층이 있다.  
- 관계형 데이터베이스를 쓸 것인가 아니면 NoSQL을 채택할 것인가? 
  - 이 질문에 대한 올바른 답을 하기 위해 중요하게 따져야 할 것은, 데이터의 유형과 읽기/쓰기 연산의 패턴이다.  
- 채팅 시스템이 다루는 데이터는 보통 두 가지다.  
- 첫번째는 사용자 프로파일, 설정, 친구 목록처럼 일반적인 데이터다.
  - 이런 데이터는 안정성을 보장하는 관계형 데이터베이스에 보관한다. 
  - 다중화, 샤딩은 이런 데이터의 가용성과 규모확장성을 보증하기 위해 보편적으로 사용되는 기술이다. 
- 두번째 유형의 데이터는 채팅 시스템에 고유한 데이터로, 바로 채팅 이력이다. 
- 이 데이터를 어떻게 보관할지 결정하려면 읽기/쓰기 연산 패턴을 이해해야 한다.
  - 채팅 이력 데이터의 양은 엄청나다. 페북 메신저나 왓츠앱은 매일 600억개의 메시지를 처리한다. 
  - 이 데이터 가운데 빈번하게 사용되는 것은 주로 최근에 주고받은 메시지다. 대부분의 사용자는 오래된 메시지는 들여다보지 않는다.
  - 사용자는 대체로 최근에 주고받은 메시지 데이터만 보게 되는 것이 사실이나, 검색 기능을 이용하거나, 특정 사용자가 언급된 메시지를 보거나, 특정 메시지로 점프하거나 하여 무작위적인 데이터 접근을 하게 되는 일도 있다. 데이터 계층은 이런 기능도 지원해야 한다.
  - 1:1 채팅 앱의 경우 읽기:쓰기 비율은 대략 1:1 정도다.
- 이 모두를 지원할 데이터베이스를 고르는 것은 아주 중요한 일이다.  
- 본 설계안의 경우에는 키-값 저장소를 추천할 것인데, 그 이유는 다음과 같다. 
  - 키-값 저장소는 수평적 규모확장이 쉽다.
  - 키-값 저장소는 데이터 접근 지연시간이 낮다.
  - 관계형 데이터베이스는 데이터 가운데 롱 테일에 해당하는 부분을 잘 처리하지 못하는 경향이 있다. 인덱스가 커지면 데이터에 대한 무작위적 접근을 처리하는 비용이 늘어난다.  
  - 이미 많은 안정적인 채팅 시스템이 키-값 저장소를 채택하고 있다. 페이스북 메신저나 디스코드가 그 사례다. 페이스북 메신저는 HBase를 사용하고 있고 디스코드는 카산드라를 이용하고 있다. 

<br><br>

**데이터 모델**  
- 키-값 저장소에 메시지 데이터를 어떻게 보관할지 자세히 보자.  

1:1 채팅을 위한 메시지 테이블
![image](https://github.com/user-attachments/assets/a66ecf9f-b00d-441f-a786-ce4f865173c8)  
- 이 테이블의 기본 키는 message_id로, 메시지 순서를 쉽게 정할 수 있도록 하는 역할도 담당한다.  
- created_at을 사용하여 메시지 순서를 정할 수는 없는데, 서로 다른 두 메시지가 동시에 만들어질 수도 있기 때문이다.  


그룹 채팅을 위한 메시지 테이블  
![image](https://github.com/user-attachments/assets/e138bbff-96e6-4cca-bb77-66a4b8b1f8fa)   
- (channel_id, message_id)의 복합키를 기본키로 사용한다.  
- 여기서 채널은 채팅 그룹과 같은 뜻이다.  
- channel_id는 파티션키로도 사용할 것인데, 그룹 채팅에 적용될 모든 질의는 특정 채널을 대상으로 할 것이기 때문이다.  


**메시지 ID**  
- message_id를 만드는 기법은 자세히 논의할 만한 가치가 있는 흥미로운 주제다.  
- message_id는 메시지들의 순서도 표현할 수 있어야 한다.  
- 그러기 위해서는 다음과 같은 속성을 만족해야 할 것이다.  
  - message_id의 값은 고유해야 한다.  
  - ID 값은 정렬 가능해야 하며 시간 순서와 일치해야 한다. 즉, 새로운 ID는 이전 ID보다 큰 값이어야 한다.  
- 이 두 조건을 만족하려면 RDBMS는 auto_increment가 대안이 되지만 NoSQL은 보통 제공하지 않는다.  
- 두번째 방법은 스노우플레이크같은 전역적 64-bit 순서 번호 생성기를 이용하는 것이다. (제7장)
- 마지막 방법은 지역적 순서 번호 생성기를 이용하는 것이다.  
  - 여기서 지역적이라 함은, ID의 유일성은 같은 그룹 안에서만 보증하면 충분하다
  - 이 방법이 통하는 이유는 메시지 사이의 순서는 같은 채널, 혹은 같은 1:1 채팅 세션 안에서만 유지되면 충분하기 때문이다.  
  - 전역적 ID 생성기에 비해 구현하기 쉬운 접근법이다.  


### 3단계: 상세 설계  
- 개략적 설계안에 포함된 컴포넌트 가운데 몇 가지를 골라 자세히 살펴보는것은 면접장에서 흔히 발생한다.  
- 채팅 시스템의 경우에는 서비스 탐색, 메시지 전달 흐름, 그리고 사용자 접속 상태를 표시하는 방법 정도가 자세히 살펴볼 부분이다.  


**서비스 탐색**   
- 서비스 탐색 기능의 주된 역할은 클라이언트에게 가장 적합한 채팅 서버를 추천하는 것이다.  
- 이 때 사용되는 기준으로는 클라이언트의 위치, 서버의 용량 등이 있다.  
- 서비스 탐색 기능을 구현하는 데 널리 쓰이는 오픈 소스 솔루션으로는 아파치 주키퍼 같은 것이 있다.  
- 사용 가능한 모든 채팅 서버를 여기 등록시켜 두고, 클라이언트가 접속을 시도하면 사전에 정한 기준에 따라 최적의 채팅 서버를 골라 주면 된다.  

![image](https://github.com/user-attachments/assets/f0f5c4c1-de3f-48f6-859d-42a007cf5575)  
위 그림은 주키퍼로 구현한 서비스 탐색 기능이 어떻게 동작하는지를 보여준다.  
1. 사용자 A가 시스템에 로그인을 시도한다.  
2. 로드밸런서가 로그인 요청을 API 서버들 가운데 하나로 보낸다.  
3. API 서버가 사용자 인증을 처리하고 나면 서비스 탐색 기능이 동작하여 해당 사용자를 서비스할 최적의 채팅 서버를 찾는다. 이 예제의 경우에는 채팅 서버 2가 선택되어 사용자 A에게 반환되었다고 하겠다.  
4. 사용자 A는 채팅 서버 2와 웹소켓 연결을 맺는다.  


<br><br><br>


**메시지 흐름**  
- 채팅 시스템에 있어서 종단 간 메시지 흐름을 이해하는 것은 흥미로운 주제다.  
- 이번 절에서는 1:1 채팅 메시지의 처리 흐름과 여러 단말 간 메시지 동기화 과정을 살펴본 다음, 그룹 채팅 메시지의 처리 흐름도 살펴보자.  


<br><br>

1:1 채팅 메시지 처리 흐름  
![image](https://github.com/user-attachments/assets/9ae36414-c6d7-4b86-a7d7-726ef565e0aa)    
- 위 그림은 1:1 채팅에서 사용자 A가 B에게 보낸 메시지가 어떤 경로로 처리되는지를 보여준다.  

1. 사용자 A가 채팅 서버 1로 메시지 전송
2. 채팅 서버 1은 ID 생성기를 사용해 해당 메시지의 ID 결정 
3. 채팅 서버 1은 해당 메시지를 메시지 동기화 큐로 전송
4. 메시지가 키-값 저장소에 보관됨
5. (a) 사용자 B가 접속 중인 경우 메시지는 사용자 B가 접속 중인 채팅 서버로 전송됨 (b) 사용자 B가 접속 중이 아니라면 푸시 알림 메시지를 푸시 알림 서버로 보냄
6. 채팅 서버 2는 메시지를 사용자 B에게 전송. 사용자 B와 채팅 서버 2 사이에는 웹소켓 연결이 있는 상태이므로 그것을 이용 

<br><br>

여러 단말 사이의 메시지 동기화  
![image](https://github.com/user-attachments/assets/dc87838e-6a21-4ecd-9da2-0d64babd85ea)  
- 위 그림에서 사용자 A는 전화기와 랩톱의 두 대 단말을 이용하고 있다.  
- 사용자 A가 전화기에서 채팅 앱에 로그인한 결과로 채팅 서버 1과 해당 단말 사이에 웹소켓 연결이 만들어져 있고, 랩톱에서 로그인한 결과로 역시 별도 웹소켓이 채팅 서버 1에 연결되어 있는 상황이다.  
- 각 단말은 cur_max_message_id라는 변수를 유지하는데, 해당 단말에서 관측된 가장 최신 메시지의 ID를 추적하는 용도다. 아래 두조건을 만족하는 메시지는 새 메시지로 간주한다.  
  - 수신자 ID가 현재 로그인한 사용자 ID와 같다
  - 키-값 저장소에 뵤관된 메시지로서, 그 ID가 cur_max_message_id보다 크다. 
- cur_max_message_id는 단말마다 별도로 유지 관리하면 되는 값이라 키-값 저장소에서 새 메시지를 가져오는 동기화 작업도 쉽게 구현할 수 있다.  

<br><br>

소규모 그룹 채팅에서의 메시지 흐름  
![image](https://github.com/user-attachments/assets/ea4e289d-3cab-4cfa-bcd7-0aa59fef7071)   
- 1:1 채팅에 비해 그룹 채팅에서의 메시지 흐름은 조금 더 복잡하다.  
- 위 그림은 사용자 A가 그룹 채팅 방에서 메시지를 보냈을 때 어떤 일이 벌어지는지를 보여준다.  
- 해당 그룹에 3명의 사용자가 있다고 하자(사용자 A,B,C).
- 우선 사용자 A가 보낸 메시지가 사용자 B, C의 메시지 동기화 큐에 복사된다.  
- 이 큐를 사용자 각각에 할당된 메시지 수신함 같은 것으로 생각해도 무방하다.  
- 이 설계안은 소규모 그룹 채팅에 적합한데, 이유는 다음과 같다.  
  - 새로운 메시지가 왔는지 확인하려면 자기 큐만 보면 되니까 메시지 동기화 플로가 단순하다.
  - 그룹이 크지 않으면 메시지를 수신자별로 복사해서 큐에 넣는 작업의 비용이 문제가 되지 않는다.  
- 위챗이 이런 접근법을 쓰고 있으며, 그룹의 크기는 500명으로 제한하고 있다.  
- 하지만 많은 사용자를 지원해야 하는 경우라면 똑같은 메시지를 모든 사용자의 큐에 복사하는게 바람직하지 않을 것이다.  

![image](https://github.com/user-attachments/assets/043afd43-9a4e-4a22-aebd-b0eed77d956a)
- 수신자 관점에서 살펴보면, 한 수신자는 여러 사용자로부터 오는 메시지를 수신할 수 있어야 한다.  
- 따라서 각 사용자의 수신함, 즉 메시지 동기화 큐는 위 그림과 같이 여러 사용자로부터 오는 메시지를 받을 수 있어야 한다.  

<br><br><br><br><br><br>

**접속상태 표시**  
- 사용자의 접속 상태를 표시하는 것은 상당수 채팅 애플리케이션의 핵심 기능이다.  
- 사용자의 프로파일 이미지나 대화명 옆에 녹색 점이 붙어 있는 것을 보게 되는데, 이것이 사용자가 접속중임을 나타낸다.  
- 개략적 설계안에서는 접속상태 서버를 통해 사용자의 상태를 관리한다고 했었다.  
- 접속상태 서버는 클라이언트와 웹소켓으로 통신하는 실시간 서비스의 일부라는 점에 유의하자.  
- 사용자의 상태가 바뀌는 시나리오는 몇가지가 있는데, 지금부터 하나씩 살펴보자.

<br><br>

사용자 로그인  
![image](https://github.com/user-attachments/assets/5d98d9ea-ca8d-4fb6-9346-c722da8ef466)  
- 사용자 로그인 절차에 대해서는 "서비스 탐색"절에서 설명한 바 있다.  
- 클라이언트와 실시간 서비스 사이에 웹소켓 연결이 맺어지고 나면 접속상태 서버는 A의 상태와 last_active_at 타임스탬프 값을 키-값 저장소에 보관한다.  
- 이 절차가 끝나고 나면 해당 사용자는 접속 중인 것으로 표시된다.  

<br><br> 

로그아웃  
![image](https://github.com/user-attachments/assets/4fe23ec0-11bb-4f63-a8ee-ee0ae16f6120)  
- 키-값 저장소에 보관된 사용자 상태가 online에서 offline으로 바뀐다.
- 끝나고 나면 UI상에서 사용자의 상태는 접속 중이 아닌것으로 표시된다.  

<br><br> 

접속 장애  
- 인터넷을 통한 연결이 항상 안정적이지는 않기 때문에 문제다.  
- 그러니 그런 상황에 대응할 수 있는 설계를 준비해야 한다.  
  - 사용자의 인터넷 연결이 끊어지면 클라이언트와 서버 사이에 맺어진 웹소켓 같은 지속성 연결도 끊어진다.  
  - 이런 장애에 대응하는 간단한 방법은 사용자를 오프라인 상태로 표시하고 연결이 복구되면 온라인 상태로 변경하는 것이다.  
- 하지만 이 방법에는 심각한 문제가 있다.
  - 짧은 시간 동안 인터넷이 연결이 끊어졌다 복구되는 일이 흔하다.
  - 이런일이 벌어질 때마다 사용자의 접속 상태를 변경한다면 그것은 지나친 일이고, 사용자 경험 측면에서도 좋지 않다.  
- 본 설계안에서는 heartbeat 검사를 통해 이 문제를 해결할 것이다. 
  - 온라인 상태의 클라이언트로 하여금 주기적으로 박동 이벤트를 접속상태 서버로 보내도록 하고, 마지막 이벤트를 받은 지 x초 이내에 또 다른 박동 이벤트 메시지를 받으면 해당 사용자의 접속상태를 계속 온라인으로 유지한다. 그렇지 않을 경우에만 오프라인으로 한다.

![image](https://github.com/user-attachments/assets/60803a3f-7e2f-47be-aa83-15a2f9dac3a8)  
- 위의 그림 예제에 등장하는 클라이언트는 박동 이벤트를 매 5초마다 서버로 보내고 있다. 
- 그런데 이벤트를 3번 보낸 후, x = 30초 동안 아무런 메시지를 보내지 않아서 오프라인 상태로 변경되었다. 

<br><br>

상태 정보의 전송  
![image](https://github.com/user-attachments/assets/b47a4a11-3b3b-415e-9f59-21beda041215)  
- 사용자 A와 친구 관계에 있는 사용자들이 어떻게 해당 사용자의 상태 변화를 알까? 위의 그림이 원리를 보여준다.  
- 상태정보 서버는 발행-구독 모델을 사용하는데, 즉 각각의 친구관계마다 채널을 하나씩 두는 것이다.  
- 사용자 A의 접속상태가 변경되면, 세개의 채널 A-B, A-C, A-D에 쓰는 것이다. 
- 클라이언트와 서버 사이의 통신에는 실시간 웹소켓을 사용한다.  
- 이 방안은 그룹 크기가 작을 때는 효과적이다.
  - 위챗은 그룹 크기 상한을 500으로 제한하고 있어서 이와 유사한 접근법을 사용할 수 있었다. 
- 그룹 크기가 더 커지면 이런식으로 접속상태 변화를 알려서는 비용이나 시간이 많이 들게 되므로 좋지 않다.
  - 예로 그룹 하나가 100,000 사용자가 있다면 상태 변화 1건당 100,000개의 이벤트 메시지가 발생한다.
- 이런 성능 문제를 해소하는 한가지 방법은 상요자가 그룹 채팅에 입장하는 순간에만 상태 정보를 읽어가게 하거나, 친구 리스트에 있는 사용자의 접속상태를 갱신하고 싶으면 수동하도록 유도하는 것이다. 

### 4단계: 마무리 

클라이언트와 서버 사이의 실시간 통신을 가능하도록 하기 위해 웹소켓을 사용하고  
실시간 메시징을 지원하는 채팅 서버, 접속 상태 서버, 푸시 알림 서버, 채팅 이력을   
보관할 키-값 젖아소, 그리고 이를 제외한 나머지 기능을 구현하는 데 쓰일 API 서버 등이 그 주요 컴포넌트다.    
면접 말미에 시간이 남으면 다음 내용을 논의해도 좋다.
- 채팅앱을확장하여사진이나비디오등의미디어를지원하도록하는방법: 미디어파일은텍스트에비해크기가크다. 그와관련하여압축방식, 클라
우드저장소, 섬네일(thumbnail)생성 등 을 논의해보면 재미있을것이다.
- 종단간암호화: 왓츠앱은메시지전송에있어종단간암호화를지원한다. 메시지발신인과 수신자이 외 에는 아무도 메시지 내용을 볼 수 없다는 뜻이다. 
- 캐시: 클라이언트에 이미 읽은 메시지를 캐시해 두면 서버와 주고받는 데이터 양을 줄일 수 있다.
- 로딩 속도 개선: 슬랙은 사용자의 데이터, 채널 등을 지역적으로 분산하는 네트워크를 구축하여 앱 로딩 속도를 개선하였다.  
- 오류처리
  - 채팅 서버 오류: 채팅 서버 하나에 수십한 사용자가 접속해 있는 상황이면, 그런 서버 하나가 죽으면 서비스 탐색 기능(주키퍼)이 동작하여 클라이언트에게 새로운 서버를 배정하고 다시 접속할 수 있도록 해야 한다.
  - 메시지 재전송: 재시도나 큐는 메시지의 안정적 전송을 보장하기 위해 흔히 사용되는 기법이다. 

## 13. 검색어 자동완성 시스템  

### 1단계: 문제 이해 및 설계 범위 확정  
지원자: 사용자가 입력하는 단어는 자동완성될 검색어의 첫 부분이어야 하나요? 아니면 중간 부분이 될 수도 있습니까?  
면접관: 첫 부분으로 한정하겠습니다.  
지원자: 몇 개의 자동완성 검색어가 표시되어야 합니까?  
면접관: 5개입니다.  
지원자: 자동완성 검색어 5개를 고르는 기준은 무엇입니까?  
면접관: 질의 빈도에 따라 정해지는 검색어 인기 순위를 기준으로 삼겠습니다.  
지원자: 맞춤법 검사 기능도 제공해야 합니까?  
면접관: 아뇨. 맞춤법 검사나 자동수정은 지원하지 않습니다.  
지원자: 질의는 영어입니까?  
면접관: 네. 하지만 시간이 허락한다면 다국어 지원을 생각해도 좋습니다.  
지원자: 대문자나 특수 문자 처리도 해야 합니까?  
면접관: 아뇨. 모든 질의는 영어 소문자로 이루어진다고 가정하겠습니다.  
지원자: 얼마나 많은 사용자를 지원해야 합니까?  
면접관: 일간 능동 사용자(DAU) 기준으로 천만 명입니다.  


**요구사항**  
- 빠른 응답 속도: 페북 검색어 자동완성 시스템에 관한 문서를 보면 시스템 응답속도는 100밀리초 이내여야 한다. 그렇지 않으면 시스템 이용이 불편해진다.
- 연관성: 자동완성되어 출력되는 검색어는 사용자가 입력한 단어와 연관된 것이어야 한다.  
- 정렬: 시스템의 계산 결과는 인기도 등의 순위 모델에 의해 정렬되어 있어야 한다.  
- 규모 확장성: 시스템은 많은 트래픽을 감당할 수 있도록 확장 가능해야 한다  
- 고가용성: 시스템의 일부에 장애가 발생하거나, 느려지거나, 예상치 못한 네트워크 문제가 생겨도 시스템은 계속 사용 가능해야 한다.  

**개략적 규모 추정**  
- DAU: 천만명
- 평균 사용자의 검색 질의: 매일 10건
- 질의할 때 마다 평균적으로 20바이트의 데이터를 입력한다고 가정
  - 문자 인코딩 방법으로 ASCII를 사용하면, 1문자 = 1바이트
  - 질의문은 평균적으로 4개의 단어로 이루어진다고 가정, 각 단어는 평균적으로 다섯 글자로 구성된다고 가정
  - 따라서 질의당 평균 4 X 5 = 20 바이트
- 검색창에 글자를 입력할 때마다 클라이언트는 검색어 자동완성 백엔드에 요청을 보낸다. 따라서 평균적으로 1회 검색당 20건의 요청이 백엔드로 전달된다.
  - 예를들어 dinner라고 검색하면 
  - search?q=d, search?q=di, search?q=din, ... 
  - 위와 같이 백엔드에 전송된다.
- QPS(질의): 대략 초당 24,000 건 ( = 10,000,000 사용자 X 10질의 / 일 X 20자 / 24시간 / 3600초) 
- 최대 QPS = QPS X 2 = 대략 48,000 건 
- 질의 가운데 20% 정도는 신규 검색어라고 가정
  - 따라서 대략 0.4GB 정도 (= 10,000,000 사용자 X 10질의/일X20자X20%)
  - 매일 0.4GB의 신규 데이터가 시스템에 추가된다는 뜻이다.

### 2단계: 개략적 설계안 제시 및 동의 구하기
개략적으로 보면 시스템은 두 부분으로 나뉜다
- 데이터 수집 서비스
  - 사용자가 입력한 질의를 실시간으로 수집하는 시스템
  - 데이터가 많은 애플리케이션에 실시간 시스템은 그다지 바람직하지 않지만 설계안을 만드는 출발점으로는 괜찮다. 상세 설계안을 준비할 때 보다 현실적인 안으로 교체해보자.
- 질의 서비스
  - 주어진 질의에 다섯 개의 인기 검색어를 정렬해 내놓는 서비스

**데이터 수집 서비스**  
![image](https://github.com/user-attachments/assets/942c1e43-2b5b-4aa9-8cd7-f3674bc02e9c)  
- 위 그림은 데이터 수집 서비스가 어떻게 동작하는지의 간단한 예다.
- 질의문과 사용빈도를 저장하는 빈도 테이블이 있다고 가정
- 처음에 이 테이블은 비어 있는데, 사용자가 twitch, twitter, twitter, twillo 를 순서대로 검색하면 그 상태가 다음과 같이 바뀌어 나간다.

<br>

**질의 서비스**  
![image](https://github.com/user-attachments/assets/b28037d4-be44-4f39-a181-e1a4aa75ce5d)  
- 위의그림처럼 빈도 테이블이 있는 상태라고 가정.
- 두개의 필드가 있음을 볼 수 있다.
  - query: 질의문
  - frequency: 질의문이 사용된 빈도

![image](https://github.com/user-attachments/assets/f51024ad-363f-49b2-aa59-94c47cd9fe6f)  
- 위 그림은 사용자가 "tw"를 검색창에 입력했을때의 그림이다.  

```sql
SELECT * FROM frequency_table 
WHERE query LIKE `prefix%`
ORDER BY frequency DESC
LIMIT 5
``` 
- 가장 많이 사용된 5개 검색어에 대한 SQL이다.  
- 데이터 양이 적을 때는 나쁘지 않은 설계안이다.  
- 하지만 데이터가 아주 많아지면 데이터베이스가 병목이 될 수 있다.  
- 상세 설계안에서 이 문제를 해결할 방법을 알아보자.


